#!/usr/bin/env python3
"""
YC Library Backfill Script
æ‰¹é‡æŠ“å– Y Combinator Library çš„å†å²æ–‡ç« å¹¶ç¿»è¯‘

ä½¿ç”¨æ–¹æ³•:
    python scripts/backfill_yc.py --max 10       # æµ‹è¯•ï¼šåªå¤„ç†10ç¯‡
    python scripts/backfill_yc.py --max 100      # å¤„ç†100ç¯‡
    python scripts/backfill_yc.py                 # å¤„ç†å…¨éƒ¨
"""

import os
import sys
import json
import time
import hashlib
import argparse
import requests
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Dict
from xml.etree import ElementTree
from concurrent.futures import ThreadPoolExecutor, as_completed

import yaml
import trafilatura
from openai import OpenAI

# ============================================
# é…ç½®
# ============================================

SITEMAP_URL = "https://www.ycombinator.com/library/sitemap.xml"
SOURCE_NAME = "Y Combinator"
USER_AGENT = "Mozilla/5.0 (compatible; AI-VC-Observer/1.0)"
MAX_WORKERS = 3  # å¹¶è¡Œç¿»è¯‘æ•°
TRANSLATION_MODEL = "deepseek-chat"

# æœ¯è¯­è¡¨
GLOSSARY = {
    'LLM': 'LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰',
    'Large Language Model': 'å¤§è¯­è¨€æ¨¡å‹',
    'GPT': 'GPT',
    'Transformer': 'Transformer',
    'Fine-tuning': 'å¾®è°ƒ',
    'Prompt': 'æç¤ºè¯',
    'RAG': 'RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰',
    'Agent': 'Agentï¼ˆæ™ºèƒ½ä½“ï¼‰',
    'Embedding': 'åµŒå…¥/å‘é‡',
    'Product-Market Fit': 'äº§å“å¸‚åœºå¥‘åˆ',
    'PMF': 'PMFï¼ˆäº§å“å¸‚åœºå¥‘åˆï¼‰',
    'Runway': 'ç°é‡‘è·‘é“',
    'Burn Rate': 'çƒ§é’±é€Ÿåº¦',
    'Series A': 'Aè½®',
    'Series B': 'Bè½®',
    'Seed Round': 'ç§å­è½®',
    'MVP': 'MVPï¼ˆæœ€å°å¯è¡Œäº§å“ï¼‰',
    'YC': 'YCï¼ˆY Combinatorï¼‰',
    'Y Combinator': 'Y Combinator',
}

# ============================================
# è·¯å¾„è®¾ç½®
# ============================================

def find_project_root() -> Path:
    """æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•"""
    current = Path(__file__).resolve().parent
    for _ in range(5):
        if (current / "content" / "posts").exists():
            return current
        if (current / "package.json").exists():
            return current
        current = current.parent
    return Path(__file__).resolve().parent.parent

PROJECT_ROOT = find_project_root()
CONTENT_DIR = PROJECT_ROOT / "content" / "posts"
DATA_DIR = PROJECT_ROOT / "data"

# ============================================
# URL çŠ¶æ€ç®¡ç†
# ============================================

class URLState:
    def __init__(self):
        self.processed_file = DATA_DIR / "processed_urls.json"
        self.processed = set()
        self._load()
    
    def _load(self):
        if self.processed_file.exists():
            with open(self.processed_file, 'r') as f:
                self.processed = set(json.load(f))
    
    def _save(self):
        DATA_DIR.mkdir(parents=True, exist_ok=True)
        with open(self.processed_file, 'w') as f:
            json.dump(list(self.processed), f, indent=2)
    
    def is_processed(self, url: str) -> bool:
        return url in self.processed
    
    def mark_processed(self, url: str):
        self.processed.add(url)
        self._save()

# ============================================
# å†…å®¹æŠ“å–
# ============================================

def fetch_sitemap_urls() -> List[str]:
    """ä» sitemap è·å–æ‰€æœ‰æ–‡ç«  URL"""
    print(f"ğŸ“¡ è·å– sitemap: {SITEMAP_URL}")
    
    response = requests.get(SITEMAP_URL, headers={'User-Agent': USER_AGENT}, timeout=30)
    response.raise_for_status()
    
    root = ElementTree.fromstring(response.content)
    ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
    
    urls = []
    for url_elem in root.findall('.//sm:url', ns):
        loc = url_elem.find('sm:loc', ns)
        if loc is not None and loc.text:
            url = loc.text.strip()
            # åªè¦æ–‡ç« é¡µé¢ï¼Œæ’é™¤åˆ†ç±»é¡µå’Œå…¶ä»–é¡µé¢
            if '/library/' in url and '?' not in url and url.count('/') == 4:
                urls.append(url)
    
    print(f"âœ… æ‰¾åˆ° {len(urls)} ç¯‡æ–‡ç« ")
    return urls

def fetch_article_content(url: str) -> Optional[Dict]:
    """è·å–å•ç¯‡æ–‡ç« å†…å®¹"""
    try:
        response = requests.get(url, headers={'User-Agent': USER_AGENT}, timeout=30)
        response.raise_for_status()
        
        html = response.text
        
        # ä½¿ç”¨ trafilatura æå–æ­£æ–‡
        content = trafilatura.extract(
            html,
            include_comments=False,
            include_tables=True,
            output_format='txt'
        )
        
        if not content or len(content) < 200:
            return None
        
        # æå–å…ƒæ•°æ®
        metadata = trafilatura.extract_metadata(html)
        
        return {
            'url': url,
            'content': content,
            'title': metadata.title if metadata else '',
            'author': metadata.author if metadata else '',
            'date': metadata.date if metadata else '',
        }
    except Exception as e:
        print(f"  âŒ è·å–å¤±è´¥ {url}: {e}")
        return None

# ============================================
# ç¿»è¯‘
# ============================================

class Translator:
    def __init__(self):
        api_key = os.environ.get('DEEPSEEK_API_KEY')
        if not api_key:
            raise ValueError("è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
        
        self.client = OpenAI(
            api_key=api_key,
            base_url=os.environ.get('DEEPSEEK_BASE_URL', 'https://api.deepseek.com')
        )
    
    def _build_glossary_text(self) -> str:
        return "\n".join([f"- {en} â†’ {zh}" for en, zh in GLOSSARY.items()])
    
    def translate(self, content: str) -> str:
        """ç¿»è¯‘æ­£æ–‡"""
        # åˆ†æ®µå¤„ç†é•¿æ–‡
        max_chunk = 8000
        if len(content) <= max_chunk:
            return self._translate_chunk(content)
        
        # åˆ†æ®µ
        paragraphs = content.split('\n\n')
        chunks = []
        current = ""
        
        for para in paragraphs:
            if len(current) + len(para) + 2 <= max_chunk:
                current += para + "\n\n"
            else:
                if current:
                    chunks.append(current.strip())
                current = para + "\n\n"
        if current.strip():
            chunks.append(current.strip())
        
        # ç¿»è¯‘æ¯æ®µ
        translated = []
        for i, chunk in enumerate(chunks):
            print(f"    ç¿»è¯‘ç¬¬ {i+1}/{len(chunks)} æ®µ...")
            translated.append(self._translate_chunk(chunk))
            if i < len(chunks) - 1:
                time.sleep(0.5)
        
        return "\n\n".join(translated)
    
    def _translate_chunk(self, text: str) -> str:
        prompt = f"""è¯·å°†ä»¥ä¸‹è‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ã€‚

ç¿»è¯‘è¦æ±‚ï¼š
1. å‡†ç¡®ä¼ è¾¾åŸæ–‡å«ä¹‰ï¼Œè¯­è¨€è‡ªç„¶æµç•…
2. ä¿æŒåŸæ–‡çš„æ®µè½ç»“æ„
3. ä¸“ä¸šæœ¯è¯­å‚è€ƒä»¥ä¸‹æœ¯è¯­è¡¨ï¼š
{self._build_glossary_text()}

4. ä»£ç å—ã€å…¬å¼ã€URLä¿æŒåŸæ ·ä¸ç¿»è¯‘
5. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯„è®ºï¼Œåªè¾“å‡ºç¿»è¯‘ç»“æœ

åŸæ–‡ï¼š
{text}

ç¿»è¯‘ï¼š"""
        
        try:
            response = self.client.chat.completions.create(
                model=TRANSLATION_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=4000,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"    âš ï¸ ç¿»è¯‘å¤±è´¥: {e}")
            return f"[ç¿»è¯‘å¤±è´¥]\n\n{text}"
    
    def generate_metadata(self, content: str, original_title: str) -> Dict:
        """ç”Ÿæˆå…ƒæ•°æ®"""
        categories = ['AIç ”ç©¶', 'AIäº§å“', 'AIåŸºç¡€è®¾æ–½', 'VCè§‚ç‚¹', 'åˆ›ä¸š', 'æŠ€æœ¯è¶‹åŠ¿', 'æœªåˆ†ç±»']
        
        prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡ç« ç”Ÿæˆå…ƒæ•°æ®ã€‚

æ–‡ç« åŸæ ‡é¢˜ï¼š{original_title}

æ–‡ç« å†…å®¹ï¼ˆå‰2000å­—ï¼‰ï¼š
{content[:2000]}

è¯·ç”Ÿæˆä»¥ä¸‹ä¿¡æ¯ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "title_zh": "ä¸­æ–‡æ ‡é¢˜ï¼ˆç®€æ´æœ‰åŠ›ï¼Œä¸è¶…è¿‡30å­—ï¼‰",
    "summary_zh": "ä¸­æ–‡æ‘˜è¦ï¼ˆ100-150å­—ï¼Œæ¦‚æ‹¬æ–‡ç« æ ¸å¿ƒè§‚ç‚¹ï¼‰",
    "category": "åˆ†ç±»ï¼ˆä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰ä¸€ä¸ªï¼š{', '.join(categories)}ï¼‰",
    "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"]ï¼ˆ3-5ä¸ªç›¸å…³æ ‡ç­¾ï¼‰
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        try:
            response = self.client.chat.completions.create(
                model=TRANSLATION_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=500,
            )
            
            result = response.choices[0].message.content.strip()
            # æ¸…ç† markdown
            if result.startswith('```'):
                result = result.split('\n', 1)[1]
            if result.endswith('```'):
                result = result.rsplit('\n', 1)[0]
            if result.startswith('json'):
                result = result[4:]
            
            return json.loads(result)
        except Exception as e:
            print(f"    âš ï¸ å…ƒæ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
            return {
                'title_zh': original_title,
                'summary_zh': content[:150] + '...',
                'category': 'æœªåˆ†ç±»',
                'tags': []
            }

# ============================================
# æ–‡ç« ç”Ÿæˆ
# ============================================

def generate_article(article: Dict, translated_content: str, metadata: Dict) -> Path:
    """ç”Ÿæˆ Markdown æ–‡ä»¶"""
    CONTENT_DIR.mkdir(parents=True, exist_ok=True)
    
    # æ—¥æœŸ
    date_str = datetime.now().strftime('%Y-%m-%d')
    if article.get('date'):
        try:
            date_str = article['date'][:10]
        except:
            pass
    
    # æ–‡ä»¶å
    url_hash = hashlib.md5(article['url'].encode()).hexdigest()[:8]
    slug = f"{date_str}-ycombinator-{url_hash}"
    filepath = CONTENT_DIR / f"{slug}.md"
    
    # Front matter
    front_matter = {
        'title': metadata.get('title_zh', article.get('title', '')),
        'title_original': article.get('title', ''),
        'date': date_str,
        'source': SOURCE_NAME,
        'source_url': article['url'],
        'author': article.get('author', ''),
        'summary': metadata.get('summary_zh', ''),
        'categories': [metadata.get('category', 'æœªåˆ†ç±»')],
        'tags': metadata.get('tags', []),
        'draft': False,
    }
    
    front_matter_yaml = yaml.safe_dump(
        front_matter,
        allow_unicode=True,
        default_flow_style=False,
        sort_keys=False
    )
    
    content = f"""---
{front_matter_yaml}---

{translated_content}

---

> æœ¬æ–‡ç”±AIè‡ªåŠ¨ç¿»è¯‘ï¼ŒåŸæ–‡é“¾æ¥ï¼š[{article.get('title', 'åŸæ–‡')}]({article['url']})
"""
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    
    return filepath

# ============================================
# å¤„ç†å•ç¯‡æ–‡ç« 
# ============================================

def process_article(url: str, translator: Translator, state: URLState) -> bool:
    """å¤„ç†å•ç¯‡æ–‡ç« ï¼Œè¿”å›æ˜¯å¦æˆåŠŸ"""
    if state.is_processed(url):
        return False
    
    # 1. è·å–å†…å®¹
    article = fetch_article_content(url)
    if not article:
        return False
    
    print(f"  ğŸ“ ç¿»è¯‘: {article['title'][:50]}...")
    
    # 2. ç¿»è¯‘
    translated = translator.translate(article['content'])
    
    # 3. ç”Ÿæˆå…ƒæ•°æ®
    print(f"    ç”Ÿæˆå…ƒæ•°æ®...")
    metadata = translator.generate_metadata(translated, article.get('title', ''))
    
    # 4. ç”Ÿæˆæ–‡ä»¶
    filepath = generate_article(article, translated, metadata)
    print(f"  âœ… å®Œæˆ: {filepath.name}")
    
    # 5. æ ‡è®°å®Œæˆ
    state.mark_processed(url)
    
    return True

# ============================================
# ä¸»å‡½æ•°
# ============================================

def main():
    parser = argparse.ArgumentParser(description='YC Library Backfill')
    parser.add_argument('--max', type=int, default=0, help='æœ€å¤šå¤„ç†å¤šå°‘ç¯‡ï¼ˆ0=å…¨éƒ¨ï¼‰')
    parser.add_argument('--skip', type=int, default=0, help='è·³è¿‡å‰Nç¯‡')
    args = parser.parse_args()
    
    print("=" * 60)
    print("YC Library Backfill")
    print("=" * 60)
    print(f"é¡¹ç›®ç›®å½•: {PROJECT_ROOT}")
    print(f"å†…å®¹ç›®å½•: {CONTENT_DIR}")
    
    # åˆå§‹åŒ–
    state = URLState()
    translator = Translator()
    
    # è·å– URL åˆ—è¡¨
    urls = fetch_sitemap_urls()
    
    # è¿‡æ»¤å·²å¤„ç†çš„
    urls = [u for u in urls if not state.is_processed(u)]
    print(f"ğŸ“Š å¾…å¤„ç†: {len(urls)} ç¯‡")
    
    # åº”ç”¨é™åˆ¶
    if args.skip > 0:
        urls = urls[args.skip:]
    if args.max > 0:
        urls = urls[:args.max]
    
    print(f"ğŸ¯ æœ¬æ¬¡å¤„ç†: {len(urls)} ç¯‡")
    print("=" * 60)
    
    # å¤„ç†
    success_count = 0
    for i, url in enumerate(urls):
        print(f"\n[{i+1}/{len(urls)}] {url}")
        try:
            if process_article(url, translator, state):
                success_count += 1
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
            break
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")
            continue
    
    print("\n" + "=" * 60)
    print(f"âœ… å®Œæˆï¼æˆåŠŸå¤„ç† {success_count} ç¯‡æ–‡ç« ")
    print("=" * 60)

if __name__ == "__main__":
    main()
