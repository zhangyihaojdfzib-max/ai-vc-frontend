#!/usr/bin/env python3
"""
YC Library Backfill Script - é«˜å¹¶å‘ç‰ˆæœ¬
æ‰¹é‡æŠ“å– Y Combinator Library çš„å†å²æ–‡ç« å¹¶ç¿»è¯‘

ç‰¹æ€§ï¼š
- æ”¯æŒ 8-10 ä¸ªå¹¶è¡Œç¿»è¯‘ä»»åŠ¡
- è‡ªåŠ¨é‡è¯•å¤±è´¥ä»»åŠ¡
- è¿›åº¦æ˜¾ç¤º

ä½¿ç”¨æ–¹æ³•:
    python scripts/backfill_yc_parallel.py --max 10 --workers 5   # æµ‹è¯•
    python scripts/backfill_yc_parallel.py --max 100 --workers 8  # 100ç¯‡ï¼Œ8å¹¶å‘
    python scripts/backfill_yc_parallel.py --workers 10           # å…¨éƒ¨ï¼Œ10å¹¶å‘
"""

import os
import sys
import json
import time
import hashlib
import argparse
import requests
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Dict, Tuple
from xml.etree import ElementTree
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

import yaml
import trafilatura
from openai import OpenAI

# ============================================
# é…ç½®
# ============================================

SITEMAP_URL = "https://www.ycombinator.com/library/sitemap.xml"
SOURCE_NAME = "Y Combinator"
USER_AGENT = "Mozilla/5.0 (compatible; AI-VC-Observer/1.0)"
DEFAULT_WORKERS = 8  # é»˜è®¤å¹¶å‘æ•°
TRANSLATION_MODEL = "deepseek-chat"

# æœ¯è¯­è¡¨
GLOSSARY = {
    'LLM': 'LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰',
    'Large Language Model': 'å¤§è¯­è¨€æ¨¡å‹',
    'GPT': 'GPT',
    'Transformer': 'Transformer',
    'Fine-tuning': 'å¾®è°ƒ',
    'Prompt': 'æç¤ºè¯',
    'RAG': 'RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰',
    'Agent': 'Agentï¼ˆæ™ºèƒ½ä½“ï¼‰',
    'Embedding': 'åµŒå…¥/å‘é‡',
    'Product-Market Fit': 'äº§å“å¸‚åœºå¥‘åˆ',
    'PMF': 'PMFï¼ˆäº§å“å¸‚åœºå¥‘åˆï¼‰',
    'Runway': 'ç°é‡‘è·‘é“',
    'Burn Rate': 'çƒ§é’±é€Ÿåº¦',
    'Series A': 'Aè½®',
    'Series B': 'Bè½®',
    'Seed Round': 'ç§å­è½®',
    'MVP': 'MVPï¼ˆæœ€å°å¯è¡Œäº§å“ï¼‰',
    'YC': 'YCï¼ˆY Combinatorï¼‰',
    'Y Combinator': 'Y Combinator',
    'Startup': 'åˆ›ä¸šå…¬å¸',
    'Founder': 'åˆ›å§‹äºº',
    'Co-founder': 'è”åˆåˆ›å§‹äºº',
    'Pitch': 'è·¯æ¼”/æ¨ä»‹',
    'Pivot': 'è½¬å‹',
    'Traction': 'å¢é•¿åŠ¿å¤´',
    'Valuation': 'ä¼°å€¼',
}

# ============================================
# è·¯å¾„è®¾ç½®
# ============================================

def find_project_root() -> Path:
    """æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•"""
    current = Path(__file__).resolve().parent
    for _ in range(5):
        if (current / "content" / "posts").exists():
            return current
        if (current / "package.json").exists():
            return current
        current = current.parent
    return Path(__file__).resolve().parent.parent

PROJECT_ROOT = find_project_root()
CONTENT_DIR = PROJECT_ROOT / "content" / "posts"
DATA_DIR = PROJECT_ROOT / "data"

# ============================================
# çº¿ç¨‹å®‰å…¨çš„ URL çŠ¶æ€ç®¡ç†
# ============================================

class URLState:
    def __init__(self):
        self.processed_file = DATA_DIR / "processed_urls.json"
        self.processed = set()
        self._lock = threading.Lock()
        self._load()
    
    def _load(self):
        if self.processed_file.exists():
            with open(self.processed_file, 'r') as f:
                self.processed = set(json.load(f))
    
    def _save(self):
        DATA_DIR.mkdir(parents=True, exist_ok=True)
        with open(self.processed_file, 'w') as f:
            json.dump(list(self.processed), f, indent=2)
    
    def is_processed(self, url: str) -> bool:
        with self._lock:
            return url in self.processed
    
    def mark_processed(self, url: str):
        with self._lock:
            self.processed.add(url)
            self._save()

# ============================================
# è¿›åº¦è®¡æ•°å™¨
# ============================================

class ProgressCounter:
    def __init__(self, total: int):
        self.total = total
        self.completed = 0
        self.success = 0
        self.failed = 0
        self._lock = threading.Lock()
        self.start_time = time.time()
    
    def increment(self, success: bool = True):
        with self._lock:
            self.completed += 1
            if success:
                self.success += 1
            else:
                self.failed += 1
    
    def get_stats(self) -> str:
        with self._lock:
            elapsed = time.time() - self.start_time
            rate = self.completed / elapsed * 60 if elapsed > 0 else 0
            remaining = (self.total - self.completed) / rate if rate > 0 else 0
            return f"[{self.completed}/{self.total}] âœ…{self.success} âŒ{self.failed} | {rate:.1f}/min | ETA: {remaining:.0f}min"

# ============================================
# å†…å®¹æŠ“å–
# ============================================

def fetch_sitemap_urls() -> List[str]:
    """ä» sitemap è·å–æ‰€æœ‰æ–‡ç«  URL"""
    print(f"ğŸ“¡ è·å– sitemap: {SITEMAP_URL}")
    
    response = requests.get(SITEMAP_URL, headers={'User-Agent': USER_AGENT}, timeout=30)
    response.raise_for_status()
    
    root = ElementTree.fromstring(response.content)
    ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
    
    urls = []
    for url_elem in root.findall('.//sm:url', ns):
        loc = url_elem.find('sm:loc', ns)
        if loc is not None and loc.text:
            url = loc.text.strip()
            # åªè¦æ–‡ç« é¡µé¢ï¼Œæ’é™¤åˆ†ç±»é¡µå’Œå…¶ä»–é¡µé¢
            if '/library/' in url and '?' not in url and url.count('/') == 4:
                urls.append(url)
    
    print(f"âœ… æ‰¾åˆ° {len(urls)} ç¯‡æ–‡ç« ")
    return urls

def fetch_article_content(url: str) -> Optional[Dict]:
    """è·å–å•ç¯‡æ–‡ç« å†…å®¹"""
    try:
        response = requests.get(url, headers={'User-Agent': USER_AGENT}, timeout=30)
        response.raise_for_status()
        
        html = response.text
        
        # ä½¿ç”¨ trafilatura æå–æ­£æ–‡
        content = trafilatura.extract(
            html,
            include_comments=False,
            include_tables=True,
            output_format='txt'
        )
        
        if not content or len(content) < 200:
            return None
        
        # æå–å…ƒæ•°æ®
        metadata = trafilatura.extract_metadata(html)
        
        return {
            'url': url,
            'content': content,
            'title': metadata.title if metadata else '',
            'author': metadata.author if metadata else '',
            'date': metadata.date if metadata else '',
        }
    except Exception as e:
        return None

# ============================================
# ç¿»è¯‘å™¨ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
# ============================================

class Translator:
    def __init__(self):
        api_key = os.environ.get('DEEPSEEK_API_KEY')
        if not api_key:
            raise ValueError("è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
        
        self.client = OpenAI(
            api_key=api_key,
            base_url=os.environ.get('DEEPSEEK_BASE_URL', 'https://api.deepseek.com')
        )
    
    def _build_glossary_text(self) -> str:
        return "\n".join([f"- {en} â†’ {zh}" for en, zh in GLOSSARY.items()])
    
    def translate(self, content: str) -> str:
        """ç¿»è¯‘æ­£æ–‡"""
        max_chunk = 8000
        if len(content) <= max_chunk:
            return self._translate_chunk(content)
        
        # åˆ†æ®µ
        paragraphs = content.split('\n\n')
        chunks = []
        current = ""
        
        for para in paragraphs:
            if len(current) + len(para) + 2 <= max_chunk:
                current += para + "\n\n"
            else:
                if current:
                    chunks.append(current.strip())
                current = para + "\n\n"
        if current.strip():
            chunks.append(current.strip())
        
        # ç¿»è¯‘æ¯æ®µ
        translated = []
        for chunk in chunks:
            translated.append(self._translate_chunk(chunk))
        
        return "\n\n".join(translated)
    
    def _translate_chunk(self, text: str) -> str:
        prompt = f"""è¯·å°†ä»¥ä¸‹è‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ã€‚

ç¿»è¯‘è¦æ±‚ï¼š
1. å‡†ç¡®ä¼ è¾¾åŸæ–‡å«ä¹‰ï¼Œè¯­è¨€è‡ªç„¶æµç•…
2. ä¿æŒåŸæ–‡çš„æ®µè½ç»“æ„
3. ä¸“ä¸šæœ¯è¯­å‚è€ƒä»¥ä¸‹æœ¯è¯­è¡¨ï¼š
{self._build_glossary_text()}

4. ä»£ç å—ã€å…¬å¼ã€URLä¿æŒåŸæ ·ä¸ç¿»è¯‘
5. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯„è®ºï¼Œåªè¾“å‡ºç¿»è¯‘ç»“æœ

åŸæ–‡ï¼š
{text}

ç¿»è¯‘ï¼š"""
        
        try:
            response = self.client.chat.completions.create(
                model=TRANSLATION_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=4000,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"[ç¿»è¯‘å¤±è´¥]\n\n{text}"
    
    def generate_metadata(self, content: str, original_title: str) -> Dict:
        """ç”Ÿæˆå…ƒæ•°æ®"""
        categories = ['AIç ”ç©¶', 'AIäº§å“', 'AIåŸºç¡€è®¾æ–½', 'VCè§‚ç‚¹', 'åˆ›ä¸š', 'æŠ€æœ¯è¶‹åŠ¿', 'æœªåˆ†ç±»']
        
        prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡ç« ç”Ÿæˆå…ƒæ•°æ®ã€‚

æ–‡ç« åŸæ ‡é¢˜ï¼š{original_title}

æ–‡ç« å†…å®¹ï¼ˆå‰2000å­—ï¼‰ï¼š
{content[:2000]}

è¯·ç”Ÿæˆä»¥ä¸‹ä¿¡æ¯ï¼Œä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "title_zh": "ä¸­æ–‡æ ‡é¢˜ï¼ˆç®€æ´æœ‰åŠ›ï¼Œä¸è¶…è¿‡30å­—ï¼‰",
    "summary_zh": "ä¸­æ–‡æ‘˜è¦ï¼ˆ100-150å­—ï¼Œæ¦‚æ‹¬æ–‡ç« æ ¸å¿ƒè§‚ç‚¹ï¼‰",
    "category": "åˆ†ç±»ï¼ˆä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰ä¸€ä¸ªï¼š{', '.join(categories)}ï¼‰",
    "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"]ï¼ˆ3-5ä¸ªç›¸å…³æ ‡ç­¾ï¼‰
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        try:
            response = self.client.chat.completions.create(
                model=TRANSLATION_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=500,
            )
            
            result = response.choices[0].message.content.strip()
            if result.startswith('```'):
                result = result.split('\n', 1)[1]
            if result.endswith('```'):
                result = result.rsplit('\n', 1)[0]
            if result.startswith('json'):
                result = result[4:]
            
            return json.loads(result)
        except Exception as e:
            return {
                'title_zh': original_title,
                'summary_zh': content[:150] + '...',
                'category': 'æœªåˆ†ç±»',
                'tags': []
            }

# ============================================
# æ–‡ç« ç”Ÿæˆï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
# ============================================

_file_lock = threading.Lock()

def generate_article(article: Dict, translated_content: str, metadata: Dict) -> Path:
    """ç”Ÿæˆ Markdown æ–‡ä»¶"""
    with _file_lock:
        CONTENT_DIR.mkdir(parents=True, exist_ok=True)
    
    # æ—¥æœŸ
    date_str = datetime.now().strftime('%Y-%m-%d')
    if article.get('date'):
        try:
            date_str = article['date'][:10]
        except:
            pass
    
    # æ–‡ä»¶å
    url_hash = hashlib.md5(article['url'].encode()).hexdigest()[:8]
    slug = f"{date_str}-ycombinator-{url_hash}"
    filepath = CONTENT_DIR / f"{slug}.md"
    
    # Front matter
    front_matter = {
        'title': metadata.get('title_zh', article.get('title', '')),
        'title_original': article.get('title', ''),
        'date': date_str,
        'source': SOURCE_NAME,
        'source_url': article['url'],
        'author': article.get('author', ''),
        'summary': metadata.get('summary_zh', ''),
        'categories': [metadata.get('category', 'æœªåˆ†ç±»')],
        'tags': metadata.get('tags', []),
        'draft': False,
    }
    
    front_matter_yaml = yaml.safe_dump(
        front_matter,
        allow_unicode=True,
        default_flow_style=False,
        sort_keys=False
    )
    
    content = f"""---
{front_matter_yaml}---

{translated_content}

---

> æœ¬æ–‡ç”±AIè‡ªåŠ¨ç¿»è¯‘ï¼ŒåŸæ–‡é“¾æ¥ï¼š[{article.get('title', 'åŸæ–‡')}]({article['url']})
"""
    
    with _file_lock:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
    
    return filepath

# ============================================
# å¤„ç†å•ç¯‡æ–‡ç« ï¼ˆWorker å‡½æ•°ï¼‰
# ============================================

def process_single_article(args: Tuple[str, Translator, URLState, ProgressCounter]) -> Tuple[str, bool, str]:
    """
    å¤„ç†å•ç¯‡æ–‡ç« 
    è¿”å›: (url, success, message)
    """
    url, translator, state, progress = args
    
    if state.is_processed(url):
        return (url, False, "å·²å¤„ç†")
    
    try:
        # 1. è·å–å†…å®¹
        article = fetch_article_content(url)
        if not article:
            progress.increment(success=False)
            return (url, False, "å†…å®¹è·å–å¤±è´¥")
        
        title = article.get('title', '')[:40]
        
        # 2. ç¿»è¯‘
        translated = translator.translate(article['content'])
        
        # 3. ç”Ÿæˆå…ƒæ•°æ®
        metadata = translator.generate_metadata(translated, article.get('title', ''))
        
        # 4. ç”Ÿæˆæ–‡ä»¶
        filepath = generate_article(article, translated, metadata)
        
        # 5. æ ‡è®°å®Œæˆ
        state.mark_processed(url)
        progress.increment(success=True)
        
        return (url, True, f"âœ… {filepath.name}")
    
    except Exception as e:
        progress.increment(success=False)
        return (url, False, f"âŒ {str(e)[:50]}")

# ============================================
# ä¸»å‡½æ•°
# ============================================

def main():
    parser = argparse.ArgumentParser(description='YC Library Backfill - é«˜å¹¶å‘ç‰ˆæœ¬')
    parser.add_argument('--max', type=int, default=0, help='æœ€å¤šå¤„ç†å¤šå°‘ç¯‡ï¼ˆ0=å…¨éƒ¨ï¼‰')
    parser.add_argument('--skip', type=int, default=0, help='è·³è¿‡å‰Nç¯‡')
    parser.add_argument('--workers', type=int, default=DEFAULT_WORKERS, help=f'å¹¶å‘æ•°ï¼ˆé»˜è®¤{DEFAULT_WORKERS}ï¼‰')
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ YC Library Backfill - é«˜å¹¶å‘ç‰ˆæœ¬")
    print("=" * 60)
    print(f"ğŸ“ é¡¹ç›®ç›®å½•: {PROJECT_ROOT}")
    print(f"ğŸ“ å†…å®¹ç›®å½•: {CONTENT_DIR}")
    print(f"âš¡ å¹¶å‘æ•°: {args.workers}")
    
    # åˆå§‹åŒ–
    state = URLState()
    translator = Translator()
    
    # è·å– URL åˆ—è¡¨
    urls = fetch_sitemap_urls()
    
    # è¿‡æ»¤å·²å¤„ç†çš„
    urls = [u for u in urls if not state.is_processed(u)]
    print(f"ğŸ“Š å¾…å¤„ç†: {len(urls)} ç¯‡")
    
    # åº”ç”¨é™åˆ¶
    if args.skip > 0:
        urls = urls[args.skip:]
    if args.max > 0:
        urls = urls[:args.max]
    
    if not urls:
        print("âœ… æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ç« ï¼")
        return
    
    print(f"ğŸ¯ æœ¬æ¬¡å¤„ç†: {len(urls)} ç¯‡")
    print("=" * 60)
    
    # è¿›åº¦è®¡æ•°å™¨
    progress = ProgressCounter(len(urls))
    
    # å¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=args.workers) as executor:
        # å‡†å¤‡ä»»åŠ¡
        tasks = [(url, translator, state, progress) for url in urls]
        
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {executor.submit(process_single_article, task): task[0] for task in tasks}
        
        # æ”¶é›†ç»“æœ
        try:
            for future in as_completed(futures):
                url = futures[future]
                try:
                    _, success, msg = future.result()
                    print(f"{progress.get_stats()} | {msg}")
                except Exception as e:
                    print(f"{progress.get_stats()} | âŒ {url}: {e}")
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨åœæ­¢...")
            executor.shutdown(wait=False, cancel_futures=True)
    
    # æœ€ç»ˆç»Ÿè®¡
    print("\n" + "=" * 60)
    elapsed = time.time() - progress.start_time
    print(f"ğŸ‰ å®Œæˆï¼")
    print(f"   âœ… æˆåŠŸ: {progress.success} ç¯‡")
    print(f"   âŒ å¤±è´¥: {progress.failed} ç¯‡")
    print(f"   â±ï¸ è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
    print(f"   ğŸ“ˆ é€Ÿåº¦: {progress.success / elapsed * 60:.1f} ç¯‡/åˆ†é’Ÿ")
    print("=" * 60)

if __name__ == "__main__":
    main()
