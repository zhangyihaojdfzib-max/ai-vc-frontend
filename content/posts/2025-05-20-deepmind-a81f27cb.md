---
title: 提升Gemini安全防护：应对间接提示词注入攻击
title_original: Advancing Gemini's security safeguards
date: '2025-05-20'
source: Google DeepMind
source_url: https://deepmind.google/blog/advancing-geminis-security-safeguards/
author: ''
summary: 本文介绍了Google DeepMind团队为提升Gemini 2.5模型安全性所做的努力，重点针对间接提示词注入攻击。文章阐述了通过自动化红队测试评估基线防御策略，并指出静态防御在面对自适应攻击时的局限性。核心解决方案包括对模型进行强化训练，使其能识别并忽略数据中嵌入的恶意指令，同时保持正常任务性能。最终强调需采用包含模型强化、输入输出检查及系统级护栏的多层深度防御策略，以持续保障AI系统的安全与可信。
categories:
- AI研究
tags:
- AI安全
- 提示词注入
- Gemini
- 模型强化
- 红队测试
draft: false
translated_at: '2026-01-29T04:03:47.596242'
---

# 提升Gemini的安全防护能力

Google DeepMind 安全与隐私研究团队

![](/images/posts/0352409b81c4.jpg)

我们发布了一份新的白皮书，阐述了我们如何使Gemini 2.5成为迄今为止我们最安全的模型系列。

想象一下，要求你的AI Agent（智能体）总结你最新的电子邮件——这似乎是一项简单的任务。Gemini和其他大语言模型（LLM）通过访问我们的文档、日历或外部网站等信息，在执行此类任务方面持续改进。但是，如果其中一封电子邮件包含隐藏的恶意指令，旨在诱骗AI共享私人数据或滥用其权限呢？

间接提示词注入提出了一个真实的网络安全挑战，AI模型有时难以区分真实的用户指令和其检索数据中嵌入的操纵性命令。我们的新白皮书《从防御Gemini免受间接提示词注入攻击中汲取的经验》，阐述了我们的战略蓝图，旨在应对间接提示词注入攻击，这些攻击使得由先进大语言模型支持的Agentic AI工具成为此类攻击的目标。

我们致力于构建不仅强大而且安全的AI Agent，这意味着我们持续努力理解Gemini可能如何响应间接提示词注入，并使其对此类攻击更具韧性。

## 评估基线防御策略

间接提示词注入攻击非常复杂，需要持续警惕和多层防御。Google DeepMind的安全与隐私研究团队专门负责保护我们的AI模型免受蓄意的恶意攻击。手动寻找这些漏洞速度慢且效率低，尤其是在模型快速演进的情况下。这就是我们构建自动化系统来持续探测Gemini防御能力的原因之一。

## 利用自动化红队测试提升Gemini安全性

我们安全策略的核心部分是自动化红队测试（ART），我们的内部Gemini团队不断以现实的方式攻击Gemini，以发现模型中潜在的安全弱点。使用这项技术，以及我们白皮书中详述的其他努力，已显著提高了Gemini在使用工具时抵御间接提示词注入攻击的保护率，使Gemini 2.5成为我们迄今为止最安全的模型系列。

我们测试了研究界建议的几种防御策略，以及我们自己的一些想法：

![](/images/posts/c3d0261332cb.jpg)

## 针对自适应攻击定制评估

基线缓解措施在应对基本的、非自适应的攻击方面显示出潜力，显著降低了攻击成功率。然而，恶意行为者越来越多地使用自适应攻击，这些攻击专门设计为随着ART演进和适应，以规避正在测试的防御。

像"聚光"或"自我反思"这样成功的基线防御，在面对学习如何应对和绕过静态防御方法的自适应攻击时，效果大打折扣。

这一发现说明了一个关键点：仅依赖针对静态攻击测试过的防御会带来虚假的安全感。为了获得强大的安全性，评估针对潜在防御而演进的自适应攻击至关重要。

## 通过模型强化构建内在韧性

虽然外部防御和系统级护栏很重要，但增强AI模型识别和忽略数据中嵌入的恶意指令的内在能力也至关重要。我们将此过程称为"模型强化"。

我们在一个包含大量现实场景的数据集上对Gemini进行了微调，其中ART生成了针对敏感信息的有效间接提示词注入。这教会了Gemini忽略恶意的嵌入指令并遵循原始用户请求，从而仅提供其应给出的正确、安全的响应。这使得模型能够从本质上理解如何处理在自适应攻击中随时间演变的受损信息。

这种模型强化显著提升了Gemini识别和忽略注入指令的能力，降低了其攻击成功率。重要的是，这并未显著影响模型在正常任务上的性能。

需要指出的是，即使进行了模型强化，也没有模型能完全免疫。有决心的攻击者仍可能发现新的漏洞。因此，我们的目标是使攻击对对手而言更加困难、成本更高且更复杂。

## 采取整体方法保障模型安全

保护AI模型免受间接提示词注入等攻击需要"深度防御"——使用多层保护，包括模型强化、输入/输出检查（如分类器）和系统级护栏。对抗间接提示词注入是我们实施Agentic安全原则和指南以负责任地开发Agent的关键方式。

保护先进AI系统免受间接提示词注入等特定且不断演变的威胁是一个持续的过程。它需要持续进行自适应评估、改进现有防御并探索新的防御，以及在模型自身中构建内在韧性。通过分层防御和持续学习，我们可以使像Gemini这样的AI助手继续变得极其有用且值得信赖。

要了解更多关于我们为Gemini构建的防御措施，以及我们关于使用更具挑战性的自适应攻击来评估模型鲁棒性的建议，请参阅GDM白皮书《从防御Gemini免受间接提示词注入攻击中汲取的经验》。

---

> 本文由AI自动翻译，原文链接：[Advancing Gemini's security safeguards](https://deepmind.google/blog/advancing-geminis-security-safeguards/)
> 
> 翻译时间：2026-01-29 04:03
