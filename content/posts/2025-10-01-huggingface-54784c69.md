---
title: 推出RTEB：检索评估新标准，解决模型泛化难题
title_original: 'Introducing RTEB: A New Standard for Retrieval Evaluation'
date: '2025-10-01'
source: Hugging Face Blog
source_url: https://huggingface.co/blog/rteb
author: ''
summary: 本文介绍了新推出的检索嵌入基准测试（RTEB），旨在解决现有评估标准在衡量嵌入模型真实检索性能时的不足。RTEB通过混合开放与私有数据集的策略，防止模型对公共数据的过拟合，从而更可靠地评估其在未见数据上的泛化能力。该基准特别关注企业应用场景，涵盖多语言和特定领域数据集，并以NDCG@10作为核心指标，致力于为开发者提供一个公平、透明且以应用为导向的评估标准。
categories:
- AI研究
tags:
- 检索评估
- 嵌入模型
- 基准测试
- RAG
- AI基础设施
draft: false
translated_at: '2026-01-30T04:04:03.386297'
---

# 推出RTEB：检索评估新标准

TL;DR——我们很高兴推出**检索嵌入基准测试（RTEB）**的测试版，这是一个旨在可靠评估嵌入模型在现实应用中检索准确性的新基准。现有基准难以衡量真正的泛化能力，而RTEB通过开放与私有数据集的混合策略解决了这一问题。其目标很简单：建立一个公平、透明且以应用为导向的标准，用于衡量模型在未见数据上的表现。

从RAG和Agent到推荐系统，许多AI应用的性能根本上受限于搜索与检索的质量。因此，准确衡量嵌入模型的检索质量是开发者普遍面临的痛点。你如何**真正**知道模型在真实场景中的表现？

这正是问题棘手之处。当前的评估标准通常依赖于模型在公共基准上的"零样本"性能。然而，这充其量只是模型真实泛化能力的近似值。当模型反复使用相同的公共数据集进行评估时，其报告分数与在新颖、未见数据上的实际性能之间会出现差距。

为解决这些挑战，我们开发了RTEB，这是一个旨在为评估检索模型提供可靠标准的基准。

## 现有基准的不足

尽管底层的评估方法和指标（如NDCG@10）是众所周知且稳健的，但现有基准的完整性常因以下问题而受损：

**泛化差距**。当前的基准生态系统无意中鼓励了"应试教学"。当训练数据源与评估数据集重叠时，模型的分数可能被夸大，从而损害基准的完整性。这种做法，无论有意与否，在多个模型的训练数据集中都很明显。这形成了一个反馈循环，模型因记忆测试数据而非发展出稳健、可泛化的能力而获得高分。

正因如此，零样本分数[1]较低的模型可能在基准测试中表现优异，却无法泛化到新问题。因此，我们通常推荐那些基准性能稍低但零样本分数更高的模型。

![来自Chung等人（2025）](/images/posts/522558abe104.png)

**与当今AI应用脱节**。许多基准与开发者当前构建的企业用例契合度不高。它们通常依赖学术数据集或源自问答数据集的检索任务，这些数据集本身虽有价值，但并非为评估检索而设计，可能无法捕捉现实检索场景中遇到的分布偏差和复杂性。那些没有这些问题的基准往往范围过窄，只专注于代码检索等单一领域，使其不适合评估通用模型。

## 介绍RTEB

今天，我们很高兴推出**检索嵌入基准测试（RTEB）**。其目标是创建一个新的、可靠的、高质量的基准，用于衡量嵌入模型的真实检索准确性。

### 实现真正泛化的混合策略

为防止基准过拟合，RTEB采用了一种混合策略，同时使用开放和私有数据集：

- **开放数据集**：语料库、查询和相关标签完全公开。这确保了透明度，并允许任何用户复现结果。
- **私有数据集**：这些数据集保持私有，评估由MTEB维护者处理以确保公正性。这种设置为衡量模型泛化到未见数据的能力提供了清晰、无偏的指标。为保持透明，我们为每个私有数据集提供了描述性统计、数据集描述以及示例（查询、文档、相关性）三元组。

这种混合方法鼓励开发具有广泛、稳健泛化能力的模型。一个在开放和私有数据集之间性能显著下降的模型将表明存在过拟合，为社区提供明确的信号。这在某些模型上已经显现，它们在RTEB的私有数据集上表现出明显的性能下降。

### 为现实领域构建

RTEB的设计特别强调企业用例。它没有采用复杂的层级结构，而是使用简单的分组以保持清晰。单个数据集可以属于多个组（例如，一个德国法律数据集同时存在于"法律"和"德语"组中）。

- **本质多语言**：基准数据集涵盖20种语言，从英语或日语等常见语言到孟加拉语或芬兰语等较稀有语言。
- **特定领域聚焦**：基准包含来自法律、医疗、代码和金融等关键企业领域的数据集。
- **高效的数据集规模**：数据集足够大以有意义（至少1k个文档和50个查询），但又不会大到使评估耗时且昂贵。
- **检索优先指标**：默认排行榜指标是**NDCG@10**，这是衡量排序搜索结果质量的黄金标准。

完整的数据集列表如下所示。我们计划持续更新开放和封闭部分，增加不同类别的数据集，并积极鼓励社区参与；如果您想建议其他数据集，请在GitHub上的[MTEB仓库](https://github.com/embeddings-benchmark/mteb)提交问题。

#### 封闭数据集

## 启动RTEB：社区共同努力

RTEB今天以测试版形式启动。我们相信构建一个稳健的基准是社区共同努力的结果，我们计划根据开发者和研究人员的反馈来发展RTEB。我们鼓励您分享想法、建议新数据集、发现现有数据集中的问题，并帮助我们为所有人建立一个更可靠的标准。请随时加入讨论或在GitHub上的[MTEB仓库](https://github.com/embeddings-benchmark/mteb)提交问题。

## 局限性与未来工作

为了突出改进领域，我们希望透明地说明RTEB当前的局限性以及未来的计划。

- **基准范围**：RTEB专注于现实的、检索优先的用例。极具挑战性的合成数据集目前不是目标，但未来可能会添加。
- **模态**：基准目前仅评估纯文本检索。我们计划在未来的版本中加入文本-图像和其他多模态检索任务。
- **语言覆盖**：我们正在积极努力扩展语言覆盖范围，特别是中文和阿拉伯语等主要语言，以及更多低资源语言。如果您知道符合这些标准的高质量数据集，请告知我们。
- **问答数据集的重用**：当前约50%的检索数据集是从问答数据集改造而来，这可能导致问题，例如问题与上下文之间存在强烈的词汇重叠，从而偏向依赖关键词匹配而非真正语义理解的模型。
- **私有数据集**：为了测试泛化能力，我们使用了仅对MTEB维护者可访问的私有数据集。为保持公平，所有维护者承诺不发布在这些数据集上训练的模型，并且仅通过公共渠道在这些私有数据集上进行测试，确保没有任何公司或个人获得不公平的优势。

我们的目标是让RTEB成为社区信赖的检索评估标准。

RTEB排行榜现已作为MTEB排行榜新检索部分的一部分在[Hugging Face](https://huggingface.co/spaces/mteb/leaderboard)上线。我们邀请您查看、评估您的模型，并加入我们，为整个AI社区构建一个更好、更可靠的基准。

[1] 零样本分数是指模型提供商明确声明已在其上训练过的评估集比例。这通常仅包括训练集部分。

---

> 本文由AI自动翻译，原文链接：[Introducing RTEB: A New Standard for Retrieval Evaluation](https://huggingface.co/blog/rteb)
> 
> 翻译时间：2026-01-30 04:04
