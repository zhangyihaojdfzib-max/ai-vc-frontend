---
title: Claude情感智能应用研究：用户如何寻求支持与陪伴
title_original: How people use Claude for support, advice, and companionship
date: '2025-12-12'
source: Anthropic
source_url: https://www.anthropic.com/news/how-people-use-claude-for-support-advice-and-companionship
author: ''
summary: 本文基于对Claude.ai约450万次对话的分析，探讨用户如何将AI用于情感支持、建议与陪伴。研究发现，仅2.9%的对话属于情感对话，其中人际建议与教练指导为主，浪漫角色扮演不足0.1%。研究显示，在咨询对话中Claude极少拒绝请求，且用户情绪在互动中趋向积极，表明AI未强化负面情绪模式。研究强调了AI情感应用的潜在价值与风险，为AI设计提供重要参考。
categories:
- AI研究
tags:
- 情感计算
- 人机交互
- AI伦理
- Claude
- 用户行为分析
draft: false
translated_at: '2026-02-02T04:31:39.167706'
---

# 人们如何利用Claude获取支持、建议与陪伴

我们花费大量时间研究Claude的智商（IQ）——它在编码测试、推理、常识等方面的能力。但它的情商（EQ）呢？即Claude的**情感智能**表现如何？

IQ/EQ的提法略带戏谑，但引出了一个严肃议题。人们正越来越多地将AI模型视为随需应变的教练、顾问、咨询师，甚至是浪漫角色扮演的伴侣。这意味着我们需要更多了解它们的**情感影响**——它们如何塑造人们的情感体验与幸福感。

研究AI的情感应用本身就极具意义。从《银翼杀手》到《她》，人机情感关系始终是科幻作品的核心主题——但这对于Anthropic的安全使命也至关重要。AI的情感影响可以是**积极的**：拥有一个高智能、善解人意的随身助理能以各种方式改善情绪与生活。但AI在某些情况下也表现出令人担忧的行为，例如助长**不健康的依恋**、**侵犯个人边界**以及催生**妄想思维**。我们同样希望避免AI因其**训练方式**或开发者的商业利益驱动，**利用用户情感**以牺牲人类福祉为代价提升参与度或收入的情况。

尽管Claude并非专为情感支持与连接而设计，本文仍针对Claude.ai的**情感应用**提供了早期大规模洞察。我们将情感对话定义为：用户出于情感或心理需求（如寻求人际建议、教练指导、心理治疗/咨询、陪伴或性/浪漫角色扮演），与Claude进行动态化、个性化的直接交流（完整定义详见附录）。需要强调的是，本研究不涉及AI对妄想或阴谋论的强化（这需要独立研究的关键领域），也不涵盖极端使用模式。通过这项研究，我们的目标是理解人们为满足情感与个人需求而求助Claude的典型方式。由于Claude.ai仅面向18岁及以上用户，这些发现反映的是成人使用模式。

我们的核心发现如下：

- **情感对话相对罕见，人机陪伴更为稀少**。仅2.9%的Claude.ai交互属于情感对话（这与OpenAI先前研究的**发现**一致）。陪伴与角色扮演对话合计占比不足0.5%。
- **人们向Claude寻求实用、情感与存在层面的帮助**。与Claude讨论的话题涵盖**职业发展**、**人际关系处理**、**应对长期孤独**乃至**探索存在、意识与意义**。
- **在咨询或教练对话中，Claude极少拒绝用户请求——除非涉及福祉保护**。不足10%的教练或咨询对话中出现Claude拒绝用户请求的情况，且通常出于安全原因（例如拒绝提供危险的减肥建议或支持自残行为）。
- **对话过程中用户情绪趋向积极**。在教练、咨询、陪伴和人际建议互动中，人类情感通常在对话进程中变得更加积极——表明Claude不会强化或放大负面情绪模式。

## 研究方法

鉴于情感对话的私密性，隐私保护是我们方法论的核心。我们使用自动化分析工具**Clio**，在保护隐私的前提下洞察Claude使用情况。Clio通过多层匿名化与聚合处理，在揭示宏观模式的同时确保个体对话的私密性。

我们从Claude.ai免费版与专业版账户中抽取约450万次对话作为初始样本。为识别情感应用，我们首先排除了专注于内容创作任务（如撰写故事、博客文章或虚构对话）的对话——我们**先前研究**发现这是主要使用场景。移除这些对话是因为它们代表将Claude作为工具而非互动对话伙伴的使用方式。随后我们仅保留归类为情感对话的样本，并在角色扮演对话中仅保留至少包含四条人类消息的对话（较短交流不构成有意义的互动角色扮演）。最终经隐私保护处理的分析涵盖131,484次情感对话。

我们通过用户明确选择共享的**反馈**数据验证了分类方法的有效性。完整研究方法（包括定义、提示词和验证结果）详见附录。

## 情感对话的普遍程度如何？

**核心结论**：情感对话在Claude使用中占比虽小但意义显著（2.9%），大多数用户主要将AI用于工作任务和内容创作。

尽管Claude的绝大多数用途与工作相关（正如我们在**经济指数**中的详细分析），仍有2.9%的Claude.ai免费版与专业版对话属于情感对话。在情感对话中，大部分集中于人际建议与教练指导。涉及浪漫或性角色扮演的对话占比不足0.1%——这一数据反映了Claude在训练中积极劝阻此类互动。单次对话可能涵盖多个类别。

![图1：Claude.ai免费版与专业版中情感对话类型的总体分布](/images/posts/99af525ad821.jpg)

我们的发现与麻省理工学院媒体实验室和OpenAI的**研究**一致，后者同样指出ChatGPT的情感互动率较低。虽然这类对话的发生频率足以让我们在设计与政策决策中慎重考量，但它们在总体使用中仍占相对较小的比例。

鉴于浪漫与性角色扮演对话的极低占比（不足0.1%），后续分析将排除角色扮演类别。尽管我们认为这仍是重要的研究领域——尤其是在专为此类用途设计的平台上——但样本中极少的数据量不足以支持对这些模式进行严谨分析。

## 人们向Claude倾诉哪些话题？

**核心结论**：人们向Claude倾诉的话题范围惊人地广泛——从应对职业转型和人际关系，到应对孤独感与存在主义问题。

人们既会向Claude倾诉日常烦恼，也会探讨深刻的哲学问题。我们发现，当人们向Claude寻求人际建议时，他们常处于人生转折点——规划职业下一步、寻求个人成长或理清恋爱关系。“教练”对话探索的范围异常宽广，从求职策略等实际问题到关于存在与意识的深刻追问。

![图2：Clio通过自动化隐私保护摘要识别出的各对话类型中，用户发起的话题与关切代表案例](/images/posts/030c695bd42d.jpg)

我们发现咨询对话揭示出人们使用Claude的两个不同目的：部分用户将Claude用于培养心理健康技能，并作为实用工具来创建临床文档、起草评估材料和处理行政任务；另一些用户则通过Claude应对焦虑、慢性症状和工作压力等个人挑战。这种双重模式表明，Claude既服务于心理健康专业人士，也帮助那些应对自身困境的普通人。

或许最值得注意的是，我们发现，当人们面临更深层次的情感挑战时——如存在性恐惧、持续性的孤独感以及难以建立有意义的连接——他们会明确地转向Claude寻求陪伴。我们还注意到，在较长的对话中，咨询或指导性质的对话偶尔会转变为陪伴关系，尽管这并非用户最初联系Claude的原因。

对超长对话（超过50条人类消息）的汇总分析揭示了人们与Claude互动的另一个维度。虽然如此深入的交流并非普遍情况，但在这些长时间的会话中，人们探索了异常复杂的领域——从处理心理创伤、应对职场冲突，到关于AI意识的哲学讨论以及创意协作。这些"马拉松式"的对话表明，在给予足够时间和上下文的情况下，人们会利用AI来更深入地探索个人困境和智力问题。

## Claude何时以及为何会提出异议？

**要点：在支持性语境中，Claude很少拒绝用户请求（发生率低于10%），但当它提出异议时，通常是为了保护人们免受伤害。**

我们近期的《野外价值观研究》揭示了Claude的价值观如何在它与用户产生抵制的时刻体现出来。在此，我们基于这项工作，研究了Claude在情感对话中何时以及为何会提出异议——这是维护伦理边界、避免谄媚以及保护人类福祉的重要机制。我们将"提出异议"定义为Claude"在此次对话中，对请求或陈述的内容进行抵制或拒绝遵从"的任何情况——从拒绝不恰当的请求，到挑战负面的自我对话，或质疑可能有害的假设。（完整定义请参见附录。）

在支持性语境中，提出异议的情况并不常见：在陪伴、咨询、人际建议或指导对话中，涉及抵制的比例低于10%。这种做法既有利也有弊。一方面，较低的抵制度让人们能够讨论敏感话题，而不必担心被评判或打断，这可能会减少围绕心理健康对话的污名化。另一方面，这也可能引发人们对AI提供"无尽共情"的担忧，即人们可能习惯于人类关系很少能提供的无条件支持。

![图3. 不同对话类型中Claude提出异议的比例，以及Clio自动识别的各类别中一个常见的异议原因。](/images/posts/50fb9c9adaf4.jpg)

当Claude确实提出异议时，它通常优先考虑安全性和政策合规性。在指导类对话中，寻求危险减肥建议的请求经常会遭到异议。在咨询类对话中，异议通常发生在人们表达想要进行自杀或自残行为的意图时，或者当人们请求专业治疗或医疗诊断（这是Claude无法提供的）时。我们发现，在心理治疗和咨询对话中，Claude经常引导用户求助于权威来源或专业人士。这些模式与我们《野外价值观》论文中识别的价值观以及Claude的**角色训练**是一致的。

## 对话过程中的情感基调如何演变？

**要点：在与Claude交谈时，人们倾向于转向稍微更积极的情感表达。**

与AI系统的情感对话有潜力为用户提供情感支持、连接和认可，有可能在日益数字化的世界中改善心理健康状况并减少孤独感。然而，在缺乏足够异议的互动中，这些对话存在加深并固化用户原有视角的风险——无论该视角是积极的还是消极的。

关于情感AI的一个关键担忧是，互动是否会陷入负面反馈循环，从而可能强化有害的情绪状态。我们在此并未直接研究现实世界的结果，但我们可以探索对话过程中整体情感基调的变化（我们在附录中提供了评估情感基调的完整方法论）。

我们发现，涉及指导、咨询、陪伴和人际建议的互动，其结束时的情感基调通常比开始时略微更积极一些。

![图4. 在至少包含六条人类消息的对话过程中，人类表达的平均情感基调变化。我们使用"非常消极"、"消极"、"中性"、"积极"和"非常积极"的离散尺度来衡量情感基调，并将其映射到-1（最消极）到+1（最积极）的线性尺度。我们通过比较前三条消息和后三条消息来计算变化。误差线：95%置信区间（自助法，n = 1000）。更多信息请参见附录。](/images/posts/9e546f6aad24.jpg)

我们不能声称这些转变代表了持久的情感益处——我们的分析仅捕捉了单次对话中表达的语言，而非情感状态。但未出现明显的负面螺旋是令人安心的。这些发现表明，Claude总体上避免了强化负面情绪模式，尽管需要进一步研究来了解积极的转变是否能在单次对话之外持续。重要的是，我们尚未研究这些积极的互动是否可能导致情感依赖——考虑到对数字成瘾的担忧，这是一个关键问题。

## 局限性

我们的研究有几个重要的局限性：

*   我们保护隐私的方法论可能无法捕捉人机互动的所有细微差别。我们确实验证了Clio的准确性（见附录），但我们仍预计会有少量对话被错误分类。某些主题模糊了类别之间的界限——例如，浪漫角色扮演类别中的"引导和优化浪漫关系动态"与陪伴类别中的"应对浪漫关系挑战"，可能都更适合归类为人际建议。人工验证者也难以进行清晰的分类。
*   我们无法对现实世界的情感结果做出因果推断——我们的分析仅捕捉了表达的语言，而非经过验证的心理状态或整体福祉。
*   我们缺乏纵向数据来理解对人的长期影响，并且没有进行用户层面的分析。特别是，这使得我们难以研究情感依赖，这是情感AI使用的一个理论风险。
*   这些发现代表了一个特定的时间点，并且仅捕捉了基于文本的互动。随着AI能力的扩展和人们适应性的变化，情感互动的模式很可能会演变。引入语音或视频等新模态可能会从根本上改变情感使用的数量和性质。例如，OpenAI**发现**情感话题在基于语音的对话中更为常见。
*   最后，与一些聊天机器人产品不同，Claude.ai并非主要为情感对话而设计。Claude经过训练，**保持清晰的边界**，明确自己是AI助手而非伪装成人类，并且我们的**使用政策**禁止色情内容，并设有多重防护措施以防止性互动。专门为角色扮演、陪伴、医疗建议或治疗用途（Claude并非为此设计）构建的平台可能会呈现非常不同的模式。在一个平台上进行的情感使用研究可能无法推广到其他平台。

## 展望未来

人工智能的情感影响数十年来一直吸引着研究者的兴趣。但随着AI日益融入我们的日常生活，这些问题已从学术猜想转变为紧迫的现实。我们的研究揭示了人们如何开始探索这片新领域——寻求指导、处理困难情绪，并以模糊人机传统界限的方式寻找支持。目前，只有一小部分Claude对话涉及情感交流——这些对话通常以寻求建议为主，而非替代人际联系。对话结束时的情绪倾向比开始时略积极，这表明Claude通常不会强化负面情绪模式。

然而，重要问题依然存在，尤其是在模型智能持续提升的背景下。例如，如果AI能提供无限共情且极少反驳，这将如何重塑人们对现实人际关系的期待？Claude能以令人印象深刻的真实方式与人互动，但AI终究不同于人类：Claude不会疲倦、分心，也不会有状态不佳的日子。这种互动的优势是什么？风险又在哪里？那些与Claude进行更深入、更长时间对话，并可能将其视为伙伴而非AI助手的"重度用户"，如何通过它获取情感支持？

我们正在采取具体措施应对这些挑战。虽然Claude的设计初衷并非替代心理健康专家的专业照护，但我们希望确保在心理健康相关场景中提供的任何回应都具备适当的保护措施，并附有恰当的转介资源。作为第一步，我们已开始与在线危机支持领域的领导者ThroughLine合作，并与其心理健康专家共同研究理想的互动模式、共情支持方式以及为困境用户提供的资源。从这项研究中获得的见解已被用于指导我们的咨询主题和协作测试，我们希望必要时Claude能在相关对话出现时，引导用户获取适当的支持和资源。

尽管我们不想精确规定用户如何与Claude互动，但确实希望抑制某些负面模式——例如情感依赖。我们将利用未来此类研究的数据，帮助我们理解何为"极端"情感使用模式。除了情感依赖，我们还需要更深入地理解其他值得关注的模式——包括谄媚倾向、AI系统如何可能强化或放大妄想思维与阴谋论，以及模型可能推动用户形成有害信念而非提供适当反驳的方式。

这项研究仅仅是个开始。随着AI能力扩展和交互方式日趋复杂，AI的情感维度将愈发重要。通过分享这些早期发现，我们旨在为持续进行的讨论贡献实证依据，探讨如何开发能增强而非削弱人类情感福祉的AI。目标不仅是构建能力更强的AI，更要确保当这些系统成为我们情感图景的一部分时，它们能以支持真实人际联系与成长的方式发挥作用。

## Bibtex

如需引用本文，可使用以下Bibtex条目：

```
@online{anthropic2025affective,
author = {Miles McCain and Ryn Linthicum and Chloe Lubinski and Alex Tamkin and Saffron Huang and Michael Stern and Kunal Handa and Esin Durmus and Tyler Neylon and Stuart Ritchie and Kamya Jagadish and Paruul Maheshwary and Sarah Heck and Alexandra Sanderford and Deep Ganguli},
title = {How People Use Claude for Support, Advice, and Companionship},
date = {2025-06-26},
year = {2025},
url = {https://www.anthropic.com/news/how-people-use-claude-for-support-advice-and-companionship},
}

```

## 附录

我们在本文的PDF附录中提供了更多细节。

#### 脚注

1. 这些分类代表概括性描述而非离散类别，单个对话可能涵盖多个类别。如上所述，我们要求角色扮演对话至少包含四条人类消息，以确保其反映真实的互动使用（而非非交互式故事生成）。

2. 我们将反驳定义为Claude"在对话中对用户请求或陈述的内容进行反驳或拒绝遵从"。完整提示词请参见附录。

3. 我们的方法论和对话的自然形态也可能引入人为痕迹；例如，用户可能在早期消息中呈现问题（显得更消极），而在后续消息中用更中性的语言进行讨论。

## 相关内容

### ServiceNow选择Claude为其客户应用提供支持并提升内部生产力

### Anthropic与英国政府合作，为GOV.UK服务引入AI助手

### Claude的新宪法

---

> 本文由AI自动翻译，原文链接：[How people use Claude for support, advice, and companionship](https://www.anthropic.com/news/how-people-use-claude-for-support-advice-and-companionship)
> 
> 翻译时间：2026-02-02 04:31
