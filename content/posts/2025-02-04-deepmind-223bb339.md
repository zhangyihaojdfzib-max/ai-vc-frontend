---
title: 前沿安全框架更新：强化AGI道路上的安全协议
title_original: Updating the Frontier Safety Framework
date: '2025-02-04'
source: Google DeepMind
source_url: https://deepmind.google/blog/updating-the-frontier-safety-framework/
author: ''
summary: 本文介绍了前沿安全框架（FSF）的更新版本，旨在应对日益强大的AI模型带来的风险。关键更新包括：为关键能力等级制定安全级别建议，以遏制模型权重泄露风险；实施更一致的部署缓解措施流程；以及提出应对欺骗性对齐风险的行业领先方法。文章强调，确保尖端AI系统的安全是全球共同挑战，需要开发者、学术界和政府协作，建立共同标准并加速研究，以平衡创新与风险管控。
categories:
- 政策监管
tags:
- AI安全
- 前沿安全框架
- 风险缓解
- AGI治理
- 模型部署
draft: false
translated_at: '2026-02-08T04:36:09.523860'
---

# 更新前沿安全框架

![](/images/posts/4222d7d5facd.jpg)

我们FSF的下一次迭代为通往AGI的道路制定了更强的安全协议

AI是一种强大的工具，它正帮助实现新的突破，并在我们这个时代从气候变化到药物发现等一些最大挑战上取得重大进展。但随着其发展，先进能力可能带来新的风险。

这就是为什么我们在去年推出了第一版《前沿安全框架》——一套旨在帮助我们领先于强大前沿AI模型可能带来的严重风险的协议。自那时起，我们与工业界、学术界和政府的专家合作，加深了对风险、用于测试风险的实证评估以及我们可以应用的缓解措施的理解。我们还在评估Gemini 2.0等前沿模型的安全与治理流程中实施了该框架。基于这项工作，今天我们发布了更新版的《前沿安全框架》。

该框架的关键更新包括：

*   为我们的关键能力等级制定安全级别建议，帮助确定在何处最需要付出最大努力来遏制模型权重泄露风险
*   实施更一致的流程来应用部署缓解措施
*   概述了应对欺骗性对齐风险的行业领先方法

## 加强安全的建议

安全缓解措施有助于防止未经授权的行为者窃取模型权重。这一点尤其重要，因为获取模型权重允许移除大多数安全防护措施。考虑到随着我们展望日益强大的AI所涉及的利害关系，处理不当可能会对安全和安保产生严重影响。我们最初的框架认识到需要采用分层安全方法，允许根据风险定制实施不同强度的缓解措施。这种相称的方法也确保我们在缓解风险和促进访问与创新之间取得正确的平衡。

自那时起，我们借鉴了更广泛的研究来发展这些安全缓解级别，并为我们的每个CCL推荐一个级别。* 这些建议反映了我们对前沿AI领域在特定CCL水平上应对此类模型应实施的最低适当安全级别的评估。这个映射过程帮助我们确定在何处最需要强有力的缓解措施来遏制最大风险。在实践中，由于我们强大的整体安全态势，我们安全实践的某些方面可能会超过此处建议的基线水平。

该框架的第二版特别建议对机器学习研发领域内的CCL采用非常高的安全级别。我们认为，对于前沿AI开发者来说，在未来其模型能够显著加速和/或自动化AI开发本身的场景下，拥有强大的安全措施将非常重要。这是因为此类能力不受控制的扩散可能会严重挑战社会仔细管理和适应AI快速发展步伐的能力。

确保尖端AI系统的持续安全是一项全球性的共同挑战，也是所有领先开发者的共同责任。重要的是，正确处理这个问题是一个集体行动问题：如果安全缓解措施未在整个领域广泛应用，任何单一参与者的安全缓解措施的社会价值将大大降低。建立我们认为可能需要的安全能力需要时间——因此，所有前沿AI开发者共同努力加强安全措施并加速推进共同行业标准至关重要。

## 部署缓解措施流程

我们还在框架中概述了部署缓解措施，重点是防止在我们部署的系统中滥用关键能力。我们更新了部署缓解方法，对在滥用风险领域达到CCL的模型应用更严格的安全缓解流程。

更新后的方法包括以下步骤：首先，我们通过迭代一组安全防护措施来准备一套缓解措施。在此过程中，我们还将制定一个安全案例，这是一个可评估的论证，说明如何将与模型CCL相关的严重风险降至可接受水平。然后，相应的公司治理机构审查该安全案例，只有在获得批准后才会进行公开发布部署。最后，我们在部署后继续审查和更新安全防护措施与安全案例。我们做出这一改变是因为我们相信所有关键能力都值得进行这种彻底的缓解流程。

## 应对欺骗性对齐风险的方法

该框架的第一版主要关注滥用风险。在此基础上，我们采取了行业领先的方法，主动应对欺骗性对齐的风险，即自主系统蓄意破坏人类控制的风险。

解决这个问题的一个初步方法侧重于检测模型何时可能发展出基本的工具性推理能力，使其能够在没有安全防护措施的情况下破坏人类控制。为了缓解这种情况，我们探索自动监控以检测工具性推理能力的非法使用。

如果模型达到更强的工具性推理水平，我们不期望自动监控在长期内仍然足够，因此我们正在积极进行——并强烈鼓励——进一步研究，为这些场景开发缓解方法。虽然我们尚不清楚此类能力出现的可能性有多大，但我们认为该领域为此可能性做好准备非常重要。

## 结论

我们将根据我们的AI原则，继续审查和发展该框架，这些原则进一步概述了我们对负责任开发的承诺。

作为我们努力的一部分，我们将继续与社会各界的合作伙伴协作。例如，如果我们评估某个模型已达到一个CCL，对整体公共安全构成未缓解的重大风险，我们旨在与适当的政府当局共享信息，以促进安全AI的发展。此外，最新的框架概述了许多需要进一步研究的潜在领域——我们期待与研究界、其他公司和政府在这些领域合作。

我们相信，开放、迭代和协作的方法将有助于建立评估未来AI模型安全性的共同标准和最佳实践，同时确保其为人类带来益处。《首尔前沿AI安全承诺》标志着朝着这一集体努力迈出了重要一步——我们希望我们更新的《前沿安全框架》能进一步推动这一进展。展望AGI，正确处理将意味着解决非常关键的问题——例如正确的能力阈值和缓解措施——这些问题需要包括政府在内的更广泛社会的参与。

FSF的最新更新由Lewis Ho、Celine Smith、Claudia van der Salm、Joslyn Barnhart和Rohin Shah在Allan Dafoe、Anca Dragan、Andy Song、Demis Hassabis、Four Flynn、Jennifer Beroshi、Helen King、Nicklas Lundblad和Tom Lue的领导下完成。我们感谢Aalok Mehta、Adam Stubblefield、Alex Kaskasoli、Alice Friend、Amy Merrick、Anna Wang、Ben Bariach、Charley Snyder、David Bledin、David Lindner、Dawn Bloxwich、Don Wallace、Eva Lu、Heidi Howard、Iason Gabriel、James Manyika、Joana Iljazi、Kent Walker、Lila Ibrahim、Mary Phuong、Mikel Rodriguez、Peng Ning、Roland S. Zimmermann、Samuel Albanie、Sarah Cogan、Sasha Brown、Seb Farquhar、Sebastien Krier、Shane Legg、Victoria Krakovna、Vijay Bolina、Xerxes Dotiwalla、Ziyue Wang的重要贡献。

脚注

*关键能力定义 - 为了识别模型可能具有的、有潜力造成严重危害的能力，我们研究了模型在高风险领域造成严重危害的路径，然后确定了模型在引发此类危害中发挥作用所必须具备的最低能力水平。我们称之为“关键能力水平”（CCLs），它们指导着我们的评估和缓解方法。

### 引入前沿安全框架

![](/images/posts/2fe7dac67062.jpg)

### 强化我们的前沿安全框架

![](/images/posts/10d724046390.jpg)

---

> 本文由AI自动翻译，原文链接：[Updating the Frontier Safety Framework](https://deepmind.google/blog/updating-the-frontier-safety-framework/)
> 
> 翻译时间：2026-02-08 04:36
