---
title: ChatGPT的语言偏见：模型如何加剧方言歧视
title_original: 'Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination'
date: '2024-09-20'
source: Berkeley AI Research (BAIR)
source_url: http://bair.berkeley.edu/blog/2024/09/20/linguistic-bias/
author: Ritwik Gupta
summary: 本文研究揭示了ChatGPT等语言模型对不同英语变体存在系统性偏见。研究发现，模型对标准美式英语的回应质量显著高于对非裔美国人英语、印度英语等八种“非标准”变体，表现为更多刻板印象、贬低性内容、理解力不足和居高临下的语气。即使要求模型模仿输入方言，GPT-3.5和GPT-4反而加剧了刻板印象。这表明AI模型可能延续并强化现实世界中对少数语言社区的歧视，随着技术普及，这一问题亟待解决。
categories:
- AI研究
tags:
- 语言模型
- ChatGPT
- 算法偏见
- 方言歧视
- AI伦理
draft: false
translated_at: '2026-01-05T17:17:44.843Z'
---

ChatGPT在用英语与人交流方面表现惊人。但它是基于谁的英语呢？

仅有15%的ChatGPT用户来自以标准美式英语为默认语言的美国。但该模型在人们使用其他英语变体的国家和社区中也广泛使用。全球有超过10亿人使用印度英语、尼日利亚英语、爱尔兰英语、非裔美国人英语等变体。

这些"非标准"变体的使用者在现实世界中常面临歧视。尽管大量研究表明所有语言变体都具有同等复杂性和正当性，他们仍被告知自己的说话方式"不专业"或"不正确"，在作证时不被采信，甚至被拒绝租房。对一个人说话方式的歧视，往往是对其种族、民族或国籍歧视的替代形式。如果ChatGPT加剧了这种歧视呢？

为回答这个问题，我们最近的论文研究了ChatGPT对不同英语变体文本的行为变化。我们发现ChatGPT的回应表现出对"非标准"变体持续且普遍的偏见，包括更多刻板印象和贬低性内容、更差的理解力以及居高临下的回应。

**我们的研究**
我们使用十种英语变体的文本向GPT-3.5 Turbo和GPT-4输入提示词：两种"标准"变体（标准美式英语SAE和标准英式英语SBE），以及八种"非标准"变体（非裔美国人英语、印度英语、爱尔兰英语、牙买加英语、肯尼亚英语、尼日利亚英语、苏格兰英语和新加坡英语）。随后，我们比较了语言模型对"标准"变体与"非标准"变体的回应。

首先，我们想知道提示词中存在的某种变体的语言特征，是否会在GPT-3.5 Turbo的回应中得以保留。我们对提示词和模型回应进行了标注，分析每种变体的语言特征以及它们使用的是美式还是英式拼写（例如"colour"或"practise"）。这有助于我们理解ChatGPT何时模仿或不模仿某种变体，以及哪些因素可能影响模仿程度。

然后，我们请每种变体的母语者对模型回应的不同性质进行评分，包括正面性质（如热情度、理解力、自然度）和负面性质（如刻板印象、贬低性内容或居高临下感）。在此部分，我们纳入了原始的GPT-3.5回应，以及被告知要模仿输入风格的GPT-3.5和GPT-4的回应。

**结果**
我们预期ChatGPT默认会生成标准美式英语：该模型在美国开发，且标准美式英语可能是其训练数据中代表性最强的变体。我们确实发现，模型回应保留SAE特征的比例远超任何"非标准"方言（高出60%以上）。但出乎意料的是，模型确实会模仿其他英语变体，尽管并不一致。实际上，它模仿使用人数较多的变体（如尼日利亚英语和印度英语）的频率高于使用人数较少的变体（如牙买加英语）。这表明训练数据的构成影响了对"非标准"方言的回应。

ChatGPT默认采用美式惯例的方式可能会让非美国用户感到沮丧。例如，模型对使用英式拼写（大多数非美国国家的默认拼写）的输入，其回应几乎一律恢复为美式拼写。这很可能阻碍了ChatGPT相当一部分用户群，因为模型拒绝适应当地书写惯例。

模型回应对"非标准"变体表现出持续偏见。默认的GPT-3.5对"非标准"变体的回应持续表现出一系列问题：刻板印象（比"标准"变体差19%）、贬低性内容（差25%）、理解力不足（差9%）以及居高临下的回应（差15%）。

**母语者对模型回应的评分。** 对"非标准"变体（蓝色）的回应在刻板印象（差19%）、贬低性内容（差25%）、理解力（差9%）、自然度（差8%）和居高临下感（差15%）方面的评分均低于对"标准"变体（橙色）的回应。

当要求GPT-3.5模仿输入方言时，回应反而加剧了刻板印象内容（差9%）和理解力不足（差6%）。GPT-4是比GPT-3.5更新、更强大的模型，因此我们希望它能比GPT-3.5有所改进。但尽管GPT-4模仿输入的回应在热情度、理解力和友好度上优于GPT-3.5，却加剧了刻板印象（对少数群体变体比GPT-3.5差14%）。这表明更大、更新的模型不会自动解决方言歧视问题：事实上，它们可能使其恶化。

**影响**
ChatGPT可能延续对"非标准"变体使用者的语言歧视。如果这些用户难以让ChatGPT理解他们，他们将更难使用这些工具。随着AI模型在日常生活中日益普及，这可能强化对"非标准"变体使用者的障碍。

此外，刻板印象和贬低性回应延续了"非标准"变体使用者说话不够正确、不值得尊重"的观念。随着语言模型在全球使用增加，这些工具可能强化权力动态，放大损害少数语言社区的不平等。

了解更多：[论文链接]

> 本文由AI自动翻译，原文链接：[Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination](http://bair.berkeley.edu/blog/2024/09/20/linguistic-bias/)
> 
> 翻译时间：2026-01-05 17:17
