---
title: TensorFlow GNN 1.0发布：专为异构图设计的大规模图神经网络库
title_original: Graph neural networks in TensorFlow
date: '2024-02-06'
source: Google AI Blog
source_url: http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html
author: null
summary: Google正式发布TensorFlow GNN 1.0，这是一个经过生产环境测试、用于构建大规模图神经网络的库。该库专注于处理异构图，支持不同类型的节点和边，通过tfgnn.GraphTensor对象在TensorFlow生态中实现图结构的一等公民支持。文章介绍了GNN通过消息传递机制聚合邻域信息进行预测的原理，并重点展示了TF-GNN在动态子图采样、从交互式研究到分布式海量数据处理等方面的灵活性和可扩展性，旨在弥合图数据与典型深度学习系统之间的鸿沟。
categories:
- AI基础设施
tags:
- 图神经网络
- TensorFlow
- 机器学习库
- 异构图
- 子图采样
draft: false
translated_at: '2026-01-06T14:49:02.034Z'
---

TensorFlow 中的图神经网络
2024年2月6日
发布者：Dustin Zelle，Google Research 软件工程师；Arno Eigenwillig，CoreML 软件工程师
快速链接

对象及其关系在我们周围的世界中无处不在，而关系对于理解一个对象而言，有时与其孤立属性同等重要——例如交通网络、生产网络、知识图谱或社交网络。离散数学和计算机科学有着悠久的历史，将此类网络形式化为图，即由节点通过各种不规则方式由边连接而成。然而，大多数机器学习算法只允许输入对象之间存在规则且统一的关系，例如像素网格、单词序列，或者根本没有关系。

图神经网络，简称 GNN，已成为一种强大的技术，既能利用图的连接性（如较早期的 DeepWalk 和 Node2Vec 算法），也能利用各个节点和边上的输入特征。GNN 可以对整个图（这个分子是否会以某种方式发生反应？）、单个节点（给定其引用，这篇文档的主题是什么？）或潜在的边（这个产品是否可能与那个产品一起被购买？）进行预测。除了对图进行预测外，GNN 还是一个强大的工具，用于弥合与更典型神经网络用例之间的鸿沟。它们以连续的方式对图的离散关系信息进行编码，从而可以自然地将其包含在另一个深度学习系统中。

我们很高兴地宣布发布 TensorFlow GNN 1.0，这是一个经过生产环境测试、用于大规模构建 GNN 的库。它支持在 TensorFlow 中进行建模和训练，以及从海量数据存储中提取输入图。TF-GNN 从一开始就是为异构图构建的，其中对象和关系的类型由不同的节点集和边集表示。现实世界的对象及其关系以不同的类型出现，TF-GNN 对异构性的专注使其能够自然地表示它们。

在 TensorFlow 内部，此类图由 `tfgnn.GraphTensor` 类型的对象表示。这是一种复合张量类型（一个 Python 类中的张量集合），在 `tf.data.Dataset`、`tf.function` 等中被接受为一等公民。它既存储图结构，也存储附加到节点、边和整个图的特征。`GraphTensor` 的可训练变换可以在高级 Keras API 中定义为 Layers 对象，也可以直接使用 `tfgnn.GraphTensor` 原语来定义。

GNN：在上下文中预测对象属性

为了说明，让我们看一个 TF-GNN 的典型应用：预测一个由海量数据库交叉引用表定义的图中，某种特定类型节点的属性。例如，一个计算机科学 arXiv 论文的引用数据库，其中包含一对多的引用和多对一的被引用关系，我们希望预测每篇论文的主题领域。

与大多数神经网络一样，GNN 在大量带标签样本（约数百万）的数据集上进行训练，但每个训练步骤只包含一小批训练样本（例如数百个）。为了扩展到数百万的规模，GNN 在来自底层图的、合理小子图流上进行训练。每个子图包含足够的原始数据，以计算其中心标记节点的 GNN 结果并训练模型。这个过程——通常称为子图采样——对 GNN 训练至关重要。大多数现有工具以批处理方式完成采样，生成用于训练的静态子图。TF-GNN 提供了工具，通过动态和交互式采样来改进这一点。

| 图示为子图采样过程，从较大的图中采样出小型、易处理的子图，以创建 GNN 训练的输入样本。 |

TF-GNN 1.0 推出了一个灵活的 Python API，用于在所有相关规模上配置动态或批量子图采样：在 Colab 笔记本中交互式进行（如此示例），高效采样存储在单个训练主机主内存中的小数据集，或通过 Apache Beam 分布式采样存储在网络文件系统上的海量数据集（高达数亿节点和数十亿边）。详情请分别参阅我们的内存采样和基于 Beam 采样的用户指南。

在这些相同的采样子图上，GNN 的任务是计算根节点的隐藏（或潜在）状态；该隐藏状态聚合并编码了根节点邻域的相关信息。一种经典方法是消息传递神经网络。在每一轮消息传递中，节点沿着入边从其邻居接收消息，并据此更新自己的隐藏状态。经过 n 轮后，根节点的隐藏状态反映了来自 n 条边范围内所有节点的聚合信息（下图以 n = 2 为例）。消息和新的隐藏状态由神经网络的隐藏层计算。在异构图中，为不同类型的节点和边使用分别训练的隐藏层通常是有意义的。

训练设置通过在标记节点的 GNN 隐藏状态之上放置一个输出层、计算损失（以衡量预测误差）以及通过反向传播更新模型权重来完成，这与任何神经网络训练一样。

除了监督训练（即最小化由标签定义的损失），GNN 也可以进行无监督训练（即无需标签）。这使我们能够计算节点及其特征的离散图结构的连续表示（或嵌入）。这些表示随后通常用于其他 ML 系统。通过这种方式，图所编码的离散关系信息可以被包含在更典型的神经网络用例中。TF-GNN 支持对异构图的非监督目标进行细粒度规范。

构建 GNN 架构

TF-GNN 库支持在不同抽象级别上构建和训练 GNN。

在最高级别，用户可以使用库中预定义的、以 Keras 层表达的模型。除了研究文献中的一小部分模型外，TF-GNN 还附带一个高度可配置的模型模板，它提供了一系列经过筛选的建模选择，我们发现这些选择在我们内部的许多问题上提供了强大的基线。模板实现了 GNN 层；用户只需初始化 Keras 层。

在最低级别，用户可以使用在图周围传递数据的原语从头开始编写 GNN 模型，例如将数据从节点广播到其所有出边，或将数据从其所有入边汇集到节点（例如，计算传入消息的总和）。TF-GNN 的图数据模型在处理特征或隐藏状态时，对节点、边和整个输入图一视同仁，使得不仅可以轻松表达像上面讨论的 MPNN 这样以节点为中心的模型，也可以表达更通用的 GraphNets 形式。这可以（但并非必须）使用 Keras 作为核心 TensorFlow 之上的建模框架来完成。有关更多详情和中级建模，请参阅 TF-GNN 用户指南和模型集合。

训练编排

虽然高级用户可以自由进行自定义模型训练，但 TF-GNN Runner 也为常见情况下的 Keras 模型训练编排提供了一种简洁的方式。一个简单的调用可能如下所示：

```python
# 示例代码，保持原样
```

Runner 为 ML 中的痛点（如分布式训练和针对 Cloud TPU 上固定形状的 `tfgnn.GraphTensor` 填充）提供了即用型解决方案。除了在单个任务上训练（如上所示），它还支持对多个（两个或更多）任务进行联合训练。

例如，可以将无监督任务与有监督任务混合，从而为最终的连续表示（或嵌入）注入特定应用的归纳偏置。调用者只需将任务参数替换为任务映射即可：
此外，TF-GNN Runner 还集成了用于模型归因的积分梯度实现。积分梯度输出是一个 GraphTensor，其连接结构与观测到的 GraphTensor 相同，但特征被替换为梯度值，其中较大的值在 GNN 预测中比较小的值贡献更大。用户可以检查梯度值，以了解其 GNN 最常使用哪些特征。

结论
简而言之，我们希望 TF-GNN 能够有助于推动 GNN 在 TensorFlow 中的大规模应用，并促进该领域的进一步创新。如果您想了解更多信息，请尝试我们在流行的 OGBN-MAG 基准测试上的 Colab 演示（在浏览器中直接运行，无需安装），浏览我们的其他用户指南和 Colab 示例，或查阅我们的论文。

致谢
TF-GNN 1.0 版本由以下团队协作开发：Google Research：Sami Abu-El-Haija、Neslihan Bulut、Bahar Fatemi、Johannes Gasteiger、Pedro Gonnet、Jonathan Halcrow、Liangze Jiang、Silvio Lattanzi、Brandon Mayer、Vahab Mirrokni、Bryan Perozzi、Anton Tsitsulin、Dustin Zelle；Google Core ML：Arno Eigenwillig、Oleksandr Ferludin、Parth Kothari、Mihir Paradkar、Jan Pfeifer、Rachael Tamakloe；以及 Google DeepMind：Alvaro Sanchez-Gonzalez 和 Lisa Wang。

---

> 本文由AI自动翻译，原文链接：[Graph neural networks in TensorFlow](http://blog.research.google/2024/02/graph-neural-networks-in-tensorflow.html)
> 
> 翻译时间：2026-01-06 02:09
