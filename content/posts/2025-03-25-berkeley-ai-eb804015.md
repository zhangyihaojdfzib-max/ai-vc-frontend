---
title: 百辆强化学习自动驾驶车部署高速公路，有效平滑交通波并节能
title_original: 'Scaling Up Reinforcement Learning for Traffic Smoothing: A 100-AV
  Highway Deployment'
date: '2025-03-25'
source: Berkeley AI Research (BAIR)
source_url: http://bair.berkeley.edu/blog/2025/03/25/rl-av-smoothing/
author: Ritwik Gupta
summary: 本文介绍了通过部署100辆由强化学习控制的自动驾驶车辆来缓解高速公路拥堵的研究。研究旨在解决“走走停停”的交通波问题，该问题会导致能源浪费和排放增加。团队构建了基于真实交通数据的模拟环境，训练自动驾驶车辆仅利用本地传感器信息，以去中心化的方式学习驾驶策略。通过精心设计的奖励函数平衡平滑交通、节能、安全等多项目标，模拟结果表明，少量自动驾驶车辆通过保持稍大车距即可有效吸收前方减速，显著提升整体交通流效率和所有车辆的燃油经济性。
categories:
- AI研究
tags:
- 强化学习
- 自动驾驶
- 交通优化
- 能源效率
- 智能交通系统
draft: false
translated_at: '2026-01-05T17:18:15.445Z'
---

我们在高峰时段的高速公路交通中部署了100辆由强化学习（RL）控制的车辆，旨在缓解拥堵并降低所有车辆的燃油消耗。我们的目标是解决“走走停停”的交通波，即那些通常无明显原因却导致拥堵和大量能源浪费的恼人减速与加速现象。为了训练高效的交通流平滑控制器，我们构建了快速、数据驱动的模拟环境，RL智能体在其中进行交互学习，以在保持通行效率、安全驾驶于人类驾驶员周围的同时，最大化能源效率。

总体而言，只需一小部分控制良好的自动驾驶车辆（AV）就足以显著改善道路整体交通流和所有驾驶员的燃油效率。此外，经过训练的控制器设计为可部署于大多数现代车辆，以去中心化方式运行，并依赖标准雷达传感器。在我们最新的论文中，我们通过这次100辆车的实验，探讨了将RL控制器从模拟环境大规模部署到实际道路所面临的挑战。

**幽灵拥堵的挑战**

一段在高速公路车流中向后传播的走走停停波。

如果你开车，肯定经历过走走停停波的困扰：这些看似无法解释的交通减速莫名其妙地出现，然后又突然消失。这些波动通常由我们驾驶行为中的微小波动引发，并通过车流被放大。我们自然会根据前车调整速度。如果车距拉大，我们会加速跟上；如果前车刹车，我们也会减速。但由于我们的反应时间不为零，我们可能刹车力度比前车稍大一些。我们后面的驾驶员也会做同样的事，这种效应不断放大。久而久之，最初微不足道的减速会演变成车流后方的完全停滞。这些波在车流中向后传播，导致因频繁加速而能源效率显著下降，同时伴随着二氧化碳排放和事故风险的增加。

这并非孤立现象！当交通密度超过临界阈值时，这些波动在繁忙道路上无处不在。那么，我们如何解决这个问题？传统的匝道控制和可变限速等方法试图管理交通流，但它们通常需要昂贵的基础设施和集中协调。一种更具扩展性的方法是使用自动驾驶车辆，它们可以实时动态调整驾驶行为。然而，仅仅将自动驾驶车辆混入人类驾驶员中是不够的：它们还必须以更智能的方式驾驶，让所有人的交通状况都变得更好，这正是强化学习发挥作用的地方。

交通流基本图。道路上的车辆数量（密度）影响交通向前移动的量（流量）。在低密度时，增加车辆会提高流量，因为更多车辆可以通过。但超过临界阈值后，车辆开始相互阻碍，导致拥堵，此时增加车辆实际上会减慢整体移动速度。

**用于平滑交通波的强化学习自动驾驶车辆**

强化学习是一种强大的控制方法，智能体通过与环境的交互学习最大化奖励信号。智能体通过试错收集经验，从错误中学习，并随时间不断改进。在我们的案例中，环境是一个混合自主性的交通场景，自动驾驶车辆在其中学习驾驶策略，以抑制走走停停波，并降低自身及附近人类驾驶车辆的燃油消耗。

训练这些RL智能体需要能够复现高速公路走走停停行为的、具有真实交通动态的快速模拟。为此，我们利用了在田纳西州纳什维尔附近24号州际公路（I-24）上收集的实验数据，并基于此构建了模拟环境，让车辆重放高速公路轨迹，从而创造出不稳定的交通状况，让跟随其后的自动驾驶车辆学习如何平滑交通。

模拟重放了一段表现出多次走走停停波的高速公路轨迹。

我们设计自动驾驶车辆时考虑了实际部署，确保它们仅需使用关于自身和前车的基本传感器信息即可运行。观测信息包括自动驾驶车辆的速度、前车速度以及两者之间的空间距离。给定这些输入，RL智能体会为自动驾驶车辆指定瞬时加速度或期望速度。仅使用这些本地测量的关键优势在于，RL控制器可以以去中心化的方式部署在大多数现代车辆上，无需额外基础设施。

**奖励设计**

最具挑战性的部分是设计一个奖励函数，当最大化该函数时，能与我们希望自动驾驶车辆实现的不同目标保持一致：
*   **交通波平滑**：减少走走停停的振荡。
*   **能源效率**：降低所有车辆（不仅仅是自动驾驶车辆）的燃油消耗。
*   **安全性**：确保合理的跟车距离，避免急刹车。
*   **驾驶舒适性**：避免激进的加速和减速。
*   **符合人类驾驶规范**：确保“正常”的驾驶行为，不会让周围驾驶员感到不适。

平衡这些目标很困难，因为必须为每一项找到合适的系数。例如，如果最小化燃油消耗在奖励中占主导地位，RL自动驾驶车辆会学会在高速公路中间停下来，因为那样最节能。为了防止这种情况，我们引入了动态最小和最大距离阈值，以确保在优化燃油效率的同时，行为安全合理。我们还惩罚了自动驾驶车辆后方人类驾驶车辆的燃油消耗，以防止它学会一种自私行为，即以牺牲周围交通为代价来优化自身的节能效果。总体而言，我们的目标是在节能与合理安全的驾驶行为之间取得平衡。

**模拟结果**

动态最小和最大距离阈值的示意图，自动驾驶车辆可在该范围内自由操作，以尽可能高效地平滑交通。

自动驾驶车辆学到的典型行为是保持比人类驾驶员稍大的车距，这使它们能更有效地吸收前方可能出现的突发性交通减速。在模拟中，这种方法在最拥堵的场景下，即使道路上自动驾驶车辆比例低于5%，也能为所有道路使用者节省高达20%的燃油。而且这些自动驾驶车辆不必是特殊车辆！它们可以仅仅是配备了智能自适应巡航控制（ACC）的标准消费级汽车，这正是我们大规模测试的对象。

RL自动驾驶车辆的平滑行为。红色：数据集中的人类驾驶轨迹。蓝色：车队中连续的自动驾驶车辆，其中AV 1最接近人类驾驶轨迹后方。自动驾驶车辆之间通常有20到25辆人类驾驶车辆。每辆自动驾驶车辆的减速幅度和加速速度都不如其前导车辆那么大，导致波动幅度随时间减小，从而实现节能。

**100辆自动驾驶车辆实地测试：大规模部署强化学习**

实验周期间停放在我们运营中心的100辆车。

鉴于模拟结果前景良好，自然的下一步就是弥合模拟与高速公路之间的差距。我们将训练好的RL控制器部署在100辆车上，在I-24公路高峰时段进行了数天测试。这项大规模实验我们称之为MegaVanderTest，是有史以来规模最大的混合自主性交通平滑实验。

在实地部署RL控制器之前，我们在模拟环境中对其进行了广泛的训练和评估，并在硬件上进行了验证。

总体而言，部署步骤包括：
- **在数据驱动的模拟环境中进行训练**：我们使用来自I-24州际公路的交通数据创建了一个具有真实交通波动态的训练环境，然后在各种新的交通场景中验证训练后的智能体（Agent）的性能和鲁棒性。
- **在硬件上部署**：在机器人软件中验证后，训练好的控制器被上传到车辆上，并能够控制车辆的设定速度。我们通过车辆的自适应巡航控制系统进行操作，该系统充当底层安全控制器。
- **模块化控制框架**：测试期间的一个关键挑战是无法获取前导车辆的信息传感器。为了克服这一点，强化学习控制器被集成到一个分层系统——MegaController中。该系统结合了一个考虑下游交通状况的速度规划器指南，并将强化学习控制器作为最终决策者。
- **在硬件上进行验证**：强化学习智能体被设计在大多数车辆由人类驾驶的环境中运行，这需要能够适应不可预测行为的稳健策略。我们通过在人类谨慎监督下，让强化学习控制的车辆上路行驶，并根据反馈调整控制，来验证这一点。

验证完成后，强化学习控制器被部署在100辆汽车上，并在I-24州际公路的早高峰时段行驶。周围的交通参与者并不知道实验正在进行，从而确保了驾驶员行为的无偏性。实验期间，我们通过沿高速公路布置的数十个高空摄像头收集数据，并通过计算机视觉流程提取了数百万条独立的车辆轨迹。基于这些轨迹计算的指标表明，正如模拟结果和先前较小规模验证部署所预期的那样，自动驾驶车辆周围的燃油消耗呈现下降趋势。例如，我们可以观察到，人类驾驶员跟在我们自动驾驶车辆后面的距离越近，他们平均消耗的燃料似乎就越少（这是使用校准后的能量模型计算的）：
*（图表说明：）下游交通中，平均燃油消耗作为与最近启用的强化学习控制自动驾驶车辆后方距离的函数。随着人类驾驶员在自动驾驶车辆后方距离的增加，他们的平均燃油消耗也随之上升。*

另一种衡量影响的方法是测量速度和加速度的方差：方差越低，交通波的振幅就应该越小，这正是我们从现场测试数据中观察到的。总体而言，尽管从大量摄像头视频数据中获取精确测量结果很复杂，但我们观察到，在我们控制的车辆周围实现了**15%到20%的节能趋势**。

*（图表说明：）实验单日内，高速公路上所有车辆的数据点，绘制在速度-加速度空间中。红线左侧的集群代表拥堵，而右侧的集群对应自由流。我们观察到，当自动驾驶车辆存在时，拥堵集群更小，这是通过计算软凸包的面积或拟合高斯核来测量的。*

**最终思考**
这次100辆汽车的现场运行测试是去中心化的，自动驾驶车辆之间没有明确的合作或通信，这反映了当前自动驾驶部署的现状，并让我们朝着更顺畅、更节能的高速公路迈进了一步。然而，仍有巨大的改进潜力。通过更好的人类驾驶模型，将模拟扩展到更快、更准确，对于弥合模拟与现实之间的差距至关重要。为自动驾驶车辆配备额外的交通数据，无论是通过先进的传感器还是集中规划，都可以进一步提高控制器的性能。例如，虽然多智能体强化学习在改进协同控制策略方面前景广阔，但如何通过5G网络实现自动驾驶车辆之间的显式通信，从而进一步提高稳定性并进一步缓解走走停停的交通波，仍然是一个悬而未决的问题。至关重要的是，我们的控制器能够与现有的自适应巡航控制系统无缝集成，使得大规模现场部署成为可能。配备智能交通平顺控制系统的车辆越多，我们在道路上看到的交通波就越少，这意味着每个人的污染和燃油消耗都将减少！

许多贡献者参与了MegaVanderTest的实现！完整名单可在CIRCLES项目页面上找到，同时还有关于该项目的更多详细信息。

阅读更多：[paper]。


> 本文由AI自动翻译，原文链接：[Scaling Up Reinforcement Learning for Traffic Smoothing: A 100-AV Highway Deployment](http://bair.berkeley.edu/blog/2025/03/25/rl-av-smoothing/)
> 
> 翻译时间：2026-01-05 17:18
