---
title: 从原理出发：深入理解LLM连续批处理技术
title_original: Continuous batching from first principles
date: '2025-11-25'
source: Hugging Face Blog
source_url: https://huggingface.co/blog/continuous_batching
author: null
summary: 本文从注意力机制和KV缓存的基础原理出发，系统推导了连续批处理技术的产生逻辑。文章首先解释了LLM生成Token的序列依赖特性及其计算瓶颈，随后详细拆解了注意力层中查询、键、值的交互过程，特别是注意力掩码在因果预测中的作用。通过分析不同阶段（预填充与解码）中张量形状的变化，揭示了传统批处理的局限性，从而自然引出连续批处理通过动态调度多用户请求、最大化GPU利用率来提升吞吐量的核心价值。全文以可视化方式辅助理解，为高效LLM推理服务提供了理论基础。
categories:
- AI基础设施
tags:
- 大语言模型
- 推理优化
- 注意力机制
- KV缓存
- 计算效率
draft: false
translated_at: '2026-01-06T01:03:56.611Z'
---

连续批处理
TL;DR：在这篇博客文章中，我们从注意力机制和KV缓存出发，通过优化吞吐量推导出连续批处理。

如果你曾使用过Qwen、Claude或任何其他AI聊天机器人，你可能注意到一个现象：回复的第一个词需要一段时间才会出现，随后词语会以（希望是）规律且快速的频率逐个出现在屏幕上。这是因为从本质上讲，所有LLM都只是花哨的下一个Token预测器。LLM首先处理你的整个提示词以生成一个新Token，然后逐个添加Token，每次都会读取之前生成的所有内容，直到它判定生成结束。

这个生成过程在计算上非常昂贵：每生成一个Token都需要将输入通过数十亿参数进行传递。为了使这些模型在实际应用中可行，尤其是在同时服务许多用户时，研究人员和工程师开发了一系列高效的推理技术。

其中最具影响力的优化之一是连续批处理，它试图通过并行处理多个对话并在对话结束时将其换出，以最大化性能。

为了理解连续批处理的工作原理以及它为何在高负载服务场景中如此有效，我们将从LLM如何处理Token的基础知识开始构建。

注意力
注意力机制是LLM工作的核心部分。语言模型通过将文本分解成我们称为Token的片段来处理文本。我们可以概念上将"Token"视为"单词"，但有时一个单词可能由多个Token组成。对于每个Token序列，网络会计算下一个Token应该是什么的预测。

网络中的许多操作是按Token进行的：每个Token被独立处理，给定Token的输出仅取决于该Token的内容，而不取决于序列中的任何其他Token。像这样的操作包括层归一化或矩阵乘法。然而，为了在句子中的单词之间建立联系，我们需要Token能够相互影响的操作。

这就是注意力的用武之地。注意力层是不同Token相互交互的唯一地方。理解网络如何将Token连接在一起意味着理解注意力。

让我们看看这在实践中是如何工作的，以只有一个输入提示词的情况为例。

考虑初始提示词 "I am sure this project"，被分词为7个Token：[<bos>, I, am, sure, this, pro, ject]。其中<bos>，即"序列开始"，是一个我们添加到提示词开头的特殊Token，用于告诉语言模型新的对话从这里开始。

每个Token在网络内部用一个长度为d（隐藏维度）的向量表示。因此，这七个输入的Token形成一个形状为的张量。其中1是序列数量，或批大小，在我们的例子中仅为1。7是序列长度，d是隐藏维度，或每个Token表示的大小。接下来，我们将使用n代替7作为序列长度。

输入张量随后被三个矩阵投影：查询投影、键投影和值投影。这产生三个张量、和，形状均为，其中是注意力头的维度。我们分别称它们为查询状态、键状态和值状态。这在下面的左图中表示。

接下来，张量和相乘以测量Token之间的相似性，产生一个形状为的张量。这就是为什么我们说注意力在序列长度上具有二次复杂度。计算需要次操作，因此成本是序列长度的平方。这在上图的右侧表示。

然后，我们对应用一个布尔注意力掩码来控制哪些Token可以交互，如下图所示。在此图中，注意力掩码是一个因果掩码，意味着每个Token只与出现在它之前的Token交互。这遵循了原因必须在其结果之前的直觉，因此得名因果掩码。注意力掩码至关重要，因为它决定了网络中所有Token的交互。将所有注意力掩码值设置为False，整个网络中就没有Token会与另一个Token交互。我们将在几段后更仔细地检查注意力掩码。

最后，在应用注意力掩码之后，我们进行按Token的softmax（等同于按行的softmax），并将结果与值投影相乘，得到一个注意力头的输出，形状为。我们在下图中提供了整个过程的视觉总结。

我们将在这篇文章中使用大量的注意力可视化，为了简化，我们将把上图稍微浓缩一下。

为什么这很重要：在连续批处理中，、和可能拥有不同数量的Token，因为正如我们将看到的，我们将同时处理不同的阶段（预填充和解码）。为了更通用，假设的形状为，的形状为，的形状为。

那么注意力分数的形状为，注意力掩码具有相同的形状，因为它是逐点应用于分数的。

应用注意力掩码和按行softmax后，我们乘以。由于我们将一个形状为的矩阵与一个形状为的矩阵相乘，内部维度必须匹配：。这意味着和总是具有相同的长度，因此我们可以通过仅显示来简化我们的可视化。

如果这看起来很抽象，不用担心：图示会使其具体化。

此外，既然我们知道注意力掩码应用于，我们知道它们具有相同的形状。我们将用注意力掩码代替表示注意力分数。最后，由于、和是的直接投影，无需表示。这给出了简化的图，其中我们只表示和注意力掩码：

这种表示也强调了如何读取注意力掩码。

我们逐行读取掩码，这等同于逐Token读取：每一行对应一个Token的注意力计算。位置（第i行，第j列）的绿色方块表示True：Token j可以影响Token i。白色方块表示False：不允许交互。

例如，查看Token "am"的第三行。"I"列是绿色的，所以"I"影响"am"的计算。"pro"列是白色的，所以"pro"不影响"am"。这就是因果掩码在起作用：未来的Token不能影响过去的Token。

模型的最后一层为每个输入Token输出一个Token预测。在我们的上下文中，生成单个提示词的延续，我们只关心最后一个Token的下一个Token预测。上图中的最后一个Token是"ject"，相关的预测是"will"。

我们刚刚描述的过程，即我们获取整个输入序列，通过多个注意力层传递它，并计算下一个Token的分数，称为预填充。这是因为，正如我们稍后将看到的，我们执行的大部分计算可以被缓存和重用——因此，我们正在预填充缓存。由于使用了这个缓存，序列生成可以在一个称为解码的阶段以更少的计算量进行。在解码阶段，生成一个新Token将比初始的完整序列计算快得多。让我们看看原因。

为了继续生成，我们开始一个新的前向传播，直观上看起来像这样：

为了计算新Token的注意力分数，我们仍然需要先前Token的键投影和值投影。因此，我们需要重复旧Token（上图中灰色部分）与和的矩阵乘法，以检索之前已经计算过一次的结果。换句话说，我们正在浪费计算资源。

让我们看看如何避免这种情况。

**KV缓存**
首先我们注意到，最后一个token不会影响其他token的注意力计算：
这遵循因果掩码的思想：由于"will"出现在所有先前token之后，它不会改变它们的注意力计算。对于文本生成，因果注意力是目前最常见的，因此从现在起我们将重点关注这种情况。请记住，非因果注意力方案也可以使用，尤其是在处理图像时。考虑到我们只需要"will"这个token的下一个token预测，我们可以通过仅计算该token的输出来简化注意力机制。

此外，我们在前一次前向传播中已经计算了token"<bos>"、……、"ject"的 和 状态：如果它们已被存储，我们就不需要重新计算。这就是KV缓存：在生成过程中创建的键和值状态列表。它本质上允许将生成一个token的计算成本从 降低到 ，避免了键和值投影的重新计算，但需要付出 的内存成本。

在上图中，只有白色的token被计算：我们不是计算8个token的键和值，而是只计算1个。你可以看到，通过KV缓存，节省了大量计算。

你可以查看[这篇文章](https://example.com)以获取更多KV缓存的可视化，或[这篇文章](https://example.com)查看实际实现示例。

让我们更具体地讨论缓存大小，因为这是一个检查模型中形状的好机会。对于一个具有 个注意力层、 个注意力头且头维度为 的模型，存储一个token所需的总缓存大小为 ，其中系数2用于同时考虑 和 。

例如，Llama-2-7B模型有 层， 个头， 的维度，每层每个token需要 个值。使用float16精度，这在内存中占用 字节，即 KB。

当我们想要生成下一个token时，KV缓存很有用，这个阶段我们称之为解码。但在预填充阶段，当我们处理初始提示词且有许多输入token时，它也可能有用。特别是当有大型初始提示词无法一次性全部放入GPU内存时。

**分块预填充**
到目前为止，我们看了一个有 个token的预填充示例，但实际上初始提示词可能长得多。例如，在使用Cursor时，你可以将你的代码库添加到提示词中作为上下文：这会显著增加提示词大小。在这种情况下，存储 个token激活值所需的内存可能大于GPU的可用内存。因此，我们无法在单次前向传播中执行预填充：我们必须将预填充分成块。这被称为分块预填充，它将是实现高效推理所需的组件之一。

假设可用内存非常有限，我们每次前向传播只能处理 个token。如果我们有一个包含 个token的初始提示词，我们需要将其分成 块（将7/4 = 1.75向上取整为2）。我们使用相同的 和 符号说明下面的示例：

我们可以做到这一点，这要归功于KV缓存。我们在第一次预填充分块期间存储KV状态，在第二次预填充分块期间，我们将存储的KV状态前置到新的KV状态之前。我们也相应地调整注意力掩码。从视觉上看，这就像我们在中间分割了未分块的预填充。

关键洞见：缓存的KV状态让我们能够增量处理提示词而不丢失信息。

虽然我们在这里展示了一个将预填充分成2块的例子，但分块预填充可以按我们想要的任何方式分割预填充，灵活适应内存限制。

我们现在终于掌握了理解连续批处理所需的所有工具。

**连续批处理**
在我们之前的例子中，我们只考虑了批大小为1的情况，即我们一次只为一个提示词生成token。在评估或模型服务的背景下，我们希望为大量提示词生成token。为了提高吞吐量（即每秒生成的token数），最佳做法是为一组多个提示词并行生成token。

要将提示词批处理在一起，最简单的方法是为两个输入张量（token序列和注意力掩码）添加一个维度。然而，这带来了对输入形状的约束：我们需要所有提示词具有相同的长度，因为张量必须是矩形的。为了实现这一点，我们通常在左侧添加填充，使得新的token预测始终来自最右侧的token。我们也相应地修改每个提示词的注意力掩码，如下所示：

其中填充token <pad> 用橙色表示。然后我们可以像以前一样执行前向传播，只是增加了批大小的维度。这被称为批处理生成：对于长度相同的提示词是高效的，但当长度变化时是浪费的。

下图通过4个生成步骤说明了这一点：一个预填充步骤（顶部）和3个解码步骤（每个"前向传播"行下方）。

其中 <eos> 表示"序列结束"，这是一个特殊token，用于指示模型已到达相应序列的生成结束。

批处理生成的缺点是，如果一个提示词通过生成一个 <eos> token在其他提示词之前完成生成，那么所有后续生成的token都是无用的。这种情况会持续到批次中最长的请求完成为止。当然，我们可以从批次中移除已达到 <eos> token的提示词，以节省一些计算和内存，但节省资源不是这里的目标：吞吐量才是。

我们不只是从批次中移除已完成的提示词，而是可以用一个等待生成的提示词替换它。我们将这称为动态调度或动态批处理。动态调度在确保前向传播生成的任何token都相关的同时，能很好地维持吞吐量。但由于我们批处理提示词的方式，它有一个主要缺点：在交换提示词时需要大量填充。这是因为新插入的提示词需要进行预填充，而其他提示词正在一次解码一个token。因此，填充量几乎与新插入提示词中的token数量一样多。

当批大小增加且初始提示词很长时，问题变得更加严重。填充成本随批大小和提示词长度呈二次方增长。如果我们有一个处于解码阶段的提示词批次，其中一个完成，动态地在批次中引入一个包含 个初始token的提示词需要 个填充token。例如，当 和 时，我们需要 个填充token！

此外，像CUDA图或 `torch.compile` 这样的实际优化需要静态张量形状。这迫使我们将所有提示词填充到一个固定的最大长度，极大地增加了填充浪费。

此时，我们的主要问题是填充，这是我们将句子批处理在一起所添加维度的结果。因此，理想情况是完全摆脱这个维度，彻底重新思考批处理。如果我们这样做，将提示词批处理在一起的唯一方法就是连接它们：

但我们不希望提示词0的token与提示词1的token交互！幸运的是，我们有一种方法来控制token之间如何交互：注意力掩码。我们如何做到这一点如下所示：

虽然我们使用不同深浅的绿色来说明注意力掩码的不同部分，但这仍然是一个布尔掩码，只有绿色代表 `True`，白色代表 `False`。

这种将提示词批处理在一起的方式称为不规则批处理（因为序列长度是"不规则的"或不均匀的），它提供了增加吞吐量的好处，而无需引入填充token。

在上图中，我们使用不规则批处理将两个完整的提示词组合在一起，但我们可以批处理尽可能多的提示词，只要内存允许。唯一的限制是 ，即我们可以放入一个批次的token数量， 取决于GPU上的可用内存。

不规则批处理是连续批处理的关键组成部分之一。

为了最大化吞吐量，我们可以按照如下算法将预填充和解码序列结合起来：
- 我们尝试始终达到每批次Token的内存预算上限
- 我们首先将所有处于解码阶段的提示词加入批次，每个提示词计为1个Token
- 我们利用分块预填充的灵活性，根据需要分割输入，用预填充阶段的提示词填满剩余空间

动态调度是构成连续批处理技术的最后一块拼图：一旦提示词处理完成，我们就立即将其从批次中移除，并用对应新请求的分块提示词替换它们。

这种不规则批处理与动态调度的结合被称为连续批处理，正是这项技术驱动着现代LLM（大语言模型）服务系统。

**结论**

连续批处理结合了三大关键技术，以最大化LLM服务中的吞吐量：
- **KV缓存**：避免重新计算过往Token的表征
- **分块预填充**：在内存限制内处理可变长度的提示词
- **具有动态调度的不规则批处理**：消除填充浪费，并保持GPU完全被利用

通过移除批次维度并使用注意力掩码来控制Token间的交互，连续批处理允许在同一批次中混合预填充和解码阶段，从而极大地提升了服务多个请求的效率。这就是像ChatGPT这样的服务能够高效处理数千并发用户的原因。

在本系列的下一篇文章中，我们将通过分页注意力来探索高效的KV缓存管理。如果您希望深入了解其他关于连续批处理的专题，请在评论区告诉我们！

致谢：感谢Arthur Zucker为本文图表制作了初始概念。同时感谢Arthur Zucker、Luc Georges、Lysandre Debut、Merve Noyan和Pedro Cuenca提供的所有有益审阅。


> 本文由AI自动翻译，原文链接：[Continuous batching from first principles](https://huggingface.co/blog/continuous_batching)
> 
> 翻译时间：2026-01-06 01:03
