---
title: Claude 3.7 Sonnet发布：首个混合推理模型与Claude Code工具
title_original: Claude 3.7 Sonnet and Claude Code
date: '2025-09-16'
source: Anthropic
source_url: https://www.anthropic.com/news/claude-3-7-sonnet
author: ''
summary: Anthropic发布Claude 3.7 Sonnet，这是目前最智能的模型，也是市场上首个混合推理模型。它能够在标准模式和扩展思考模式间切换，提供即时响应或深度思考过程。该模型在编码和前端开发方面表现突出，同时推出了Claude
  Code命令行工具，帮助开发者从终端处理工程任务。模型已在各平台上线，定价与前代相同，扩展思考模式在免费版中不可用。
categories:
- AI产品
tags:
- Claude 3.7 Sonnet
- 混合推理模型
- AI编程
- Anthropic
- 大语言模型
draft: false
translated_at: '2026-02-10T04:31:03.238819'
---

# Claude 3.7 Sonnet 与 Claude Code

今天，我们发布 Claude 3.7 Sonnet¹，这是我们迄今为止最智能的模型，也是市场上首个混合推理模型。Claude 3.7 Sonnet 能够生成近乎即时的响应，或进行扩展的、逐步的思考，并且这些思考过程对用户可见。API 用户还可以对模型思考时长进行细粒度控制。

Claude 3.7 Sonnet 在编码和前端 Web 开发方面表现出特别显著的改进。伴随该模型，我们还推出了一款用于智能体编码的命令行工具 Claude Code。Claude Code 目前以有限的研究预览版形式提供，使开发者能够直接从终端将大量工程任务委托给 Claude 处理。

![显示 Claude Code 入门界面的屏幕截图](/images/posts/7eed0a90bfe0.jpg)

Claude 3.7 Sonnet 现已登陆所有 Claude 计划——包括免费版、Pro 版、团队版和企业版——以及 Claude 开发者平台、Amazon Bedrock 和 Google Cloud 的 Vertex AI。扩展思考模式在所有平台上均可用，除了免费的 Claude 层级。

在标准和扩展思考两种模式下，Claude 3.7 Sonnet 的定价与其前代模型相同：输入 Token 每百万个 3 美元，输出 Token 每百万个 15 美元——这包含了思考 Token 的成本。

## Claude 3.7 Sonnet：让前沿推理变得实用

我们开发 Claude 3.7 Sonnet 的理念与市场上其他推理模型不同。正如人类使用同一个大脑进行快速反应和深度思考一样，我们相信推理应该是前沿模型的一种集成能力，而非完全独立的另一个模型。这种统一的方法也为用户创造了更无缝的体验。

Claude 3.7 Sonnet 通过几种方式体现了这一理念。首先，Claude 3.7 Sonnet 集普通 LLM（大语言模型）和推理模型于一体：你可以选择何时让模型正常回答，何时让它思考更长时间后再回答。在标准模式下，Claude 3.7 Sonnet 代表了 Claude 3.5 Sonnet 的升级版。在扩展思考模式下，它会在回答前进行自我反思，从而提升其在数学、物理、指令遵循、编码和许多其他任务上的表现。我们发现，在这两种模式下，提示词对模型的作用方式大体相似。

其次，当通过 API 使用 Claude 3.7 Sonnet 时，用户还可以控制思考的预算：你可以告诉 Claude 思考不超过 N 个 Token，N 可以是其 128K Token 输出限制内的任意值。这让你可以在速度（和成本）与答案质量之间进行权衡。

第三，在开发我们的推理模型时，我们略微减少了对数学和计算机科学竞赛题的优化，转而将重点转向更能反映企业实际如何使用 LLM（大语言模型）的现实世界任务。

早期测试证明了 Claude 在编码能力方面的全面领先地位：Cursor 指出，Claude 在处理现实世界编码任务方面再次成为业界最佳，在处理复杂代码库到高级工具使用等领域的表现均有显著提升。Cognition 发现它在规划代码变更和处理全栈更新方面远优于任何其他模型。Vercel 强调了 Claude 在复杂智能体工作流程方面的卓越精度，而 Replit 已成功部署 Claude 来从头构建复杂的 Web 应用和仪表板，而其他模型在此类任务上会停滞不前。在 Canva 的评估中，Claude 始终能生成具备生产就绪性、设计品味上乘且错误大幅减少的代码。

![Claude 3.7 Sonnet 在 SWE-bench Verified 上实现了最先进的性能，该基准评估 AI 模型解决现实世界软件问题的能力。有关脚手架（scaffolding）的更多信息，请参阅附录。](/images/posts/d32c84844bcd.jpg)

![Claude 3.7 Sonnet 在 TAU-bench 上实现了最先进的性能，这是一个测试 AI 智能体在涉及用户和工具交互的复杂现实世界任务中表现的框架。有关脚手架（scaffolding）的更多信息，请参阅附录。](/images/posts/4b6eb537f7b6.jpg)

![Claude 3.7 Sonnet 在指令遵循、通用推理、多模态能力和智能体编码方面表现出色，扩展思考模式在数学和科学领域带来了显著提升。除了传统基准测试，它甚至在我们进行的 Pokémon 游戏测试中超越了所有之前的模型。](/images/posts/58b23084e540.jpg)

## Claude Code

自 2024 年 6 月以来，Sonnet 已成为全球开发者的首选模型。今天，我们通过以有限研究预览版形式推出 Claude Code——我们的首款智能体编码工具——来进一步赋能开发者。

Claude Code 是一个积极的协作者，能够搜索和阅读代码、编辑文件、编写和运行测试、提交代码并推送到 GitHub，以及使用命令行工具——让你在每一步都保持参与。

Claude Code 是一个早期产品，但已成为我们团队不可或缺的工具，特别是在测试驱动开发、调试复杂问题和进行大规模重构方面。在早期测试中，Claude Code 能够一次性完成通常需要 45 分钟以上手动工作的任务，从而减少了开发时间和开销。

在接下来的几周里，我们计划根据我们的使用情况持续改进它：增强工具调用的可靠性、增加对长时间运行命令的支持、改进应用内渲染效果，并扩展 Claude 对其自身能力的理解。

我们推出 Claude Code 的目标是更好地了解开发者如何使用 Claude 进行编码，以便为未来的模型改进提供参考。通过加入此预览，你将获得与我们用于构建和改进 Claude 的相同强大工具的访问权限，你的反馈将直接影响其未来。

## 与 Claude 协作处理你的代码库

我们还改进了 Claude.ai 上的编码体验。我们的 GitHub 集成现已面向所有 Claude 计划开放——使开发者能够将他们的代码仓库直接连接到 Claude。

Claude 3.7 Sonnet 是我们迄今为止最好的编码模型。凭借对你个人、工作和开源项目的更深入理解，它将成为你在最重要的 GitHub 项目中修复错误、开发功能和构建文档的更强大伙伴。

## 负责任地构建

我们对 Claude 3.7 Sonnet 进行了广泛的测试和评估，并与外部专家合作，确保其符合我们在安全性、可靠性和可靠性方面的标准。与其前代模型相比，Claude 3.7 Sonnet 还能更细致地区分有害请求和良性请求，将不必要的拒绝减少了 45%。

本次发布的系统卡片涵盖了多个类别的新安全结果，提供了我们负责任扩展政策评估的详细细分，其他 AI 实验室和研究人员可以将其应用于他们的工作。该卡片还讨论了计算机使用带来的新兴风险，特别是提示词注入攻击，并解释了我们如何评估这些漏洞以及训练 Claude 来抵抗和缓解它们。此外，它还探讨了推理模型可能带来的安全益处：理解模型如何做出决策的能力，以及模型推理是否真正值得信赖和可靠。阅读完整的系统卡片以了解更多信息。

## 展望未来

Claude 3.7 Sonnet 和 Claude Code 标志着我们朝着能够真正增强人类能力的 AI 系统迈出了重要一步。凭借其深度推理、自主工作和有效协作的能力，它们让我们更接近一个 AI 能够丰富和扩展人类所能实现成就的未来。

![显示 Claude 从助手到先驱发展历程的里程碑时间线](/images/posts/5243f8f0f186.jpg)

我们很高兴看到你探索这些新功能，并期待看到你将用它们创造出什么。一如既往，在我们持续改进和发展模型的过程中，我们欢迎你的反馈。

#### 附录

¹关于命名的经验教训。

### 评估数据来源

- Grok
- Gemini 2 Pro
- o1 和 o3-mini
- Supplementary o1
- o1 TAU-bench
- Supplementary o3-mini
- Deepseek R1

### TAU-bench

关于脚手架（scaffolding）的信息

这些分数是通过在航空客服Agent策略的提示词附录中添加指令实现的，该指令引导Claude更好地利用“规划”工具。在多轮交互轨迹中，鼓励模型在解决问题时写下其思考过程（这与我们通常的思维模式不同），以最大限度地发挥其推理能力。为了适应Claude因进行更多思考而产生的额外步骤，最大步骤数（按模型完成次数计算）从30步增加到了100步（大多数轨迹在30步内完成，仅有一个轨迹超过了50步）。

此外，Claude 3.5 Sonnet（新版）的TAU-bench分数与我们最初发布时报告的结果有所不同，这是因为之后引入了对数据集的小幅改进。我们在更新后的数据集上重新运行了测试，以便与Claude 3.7 Sonnet进行更准确的比较。

### SWE-bench Verified

解决像SWE-bench这样的开放式智能体任务有多种方法。有些方法将决定调查或编辑哪些文件、运行哪些测试的复杂性，大部分卸载给更传统的软件，让核心语言模型只在预定义的位置生成代码，或从更有限的操作集合中进行选择。Agentless（Xia等人，2024）是一个流行的框架，用于评估Deepseek的R1及其他模型，它通过基于提示词和嵌入的文件检索机制、补丁定位以及针对回归测试的40选1拒绝采样来增强智能体。其他框架（例如Aide）则通过重试、N选1或蒙特卡洛树搜索（MCTS）等形式，为模型提供额外的测试时计算资源作为补充。

对于Claude 3.7 Sonnet和Claude 3.5 Sonnet（新版），我们采用了一种更简单、框架支持最少的方法，即模型在单次会话中决定运行哪些命令以及编辑哪些文件。我们主要的“无扩展思考”pass@1结果，只是为模型配备了这里描述的两个工具——一个bash工具和一个通过字符串替换操作的文件编辑工具——以及我们在TAU-bench结果中提到的“规划工具”。由于基础设施限制，在我们的内部基础设施上，实际上只有489/500个问题是可以解决的（即，标准解决方案能通过测试）。为了与官方排行榜保持一致，在我们计算基础的pass@1分数时，将11个无法解决的问题计为失败。为了透明起见，我们单独发布了在我们的基础设施上无法运行的测试用例。

对于我们的“高计算量”数据，我们采用了额外的复杂性和并行测试时计算，具体如下：

- 我们使用上述框架对多次并行尝试进行采样。
- 我们丢弃那些破坏了仓库中可见回归测试的补丁，类似于Agentless采用的拒绝采样方法；请注意，没有使用任何隐藏的测试信息。
- 然后，我们使用一个评分模型对剩余的尝试进行排序（该模型类似于我们研究文章中描述的用于GPQA和AIME的评分模型），并选择最佳的一次尝试进行提交。

这在我们基础设施上可运行的n=489个已验证任务子集上，得到了70.3%的分数。如果没有这个框架，Claude 3.7 Sonnet在SWE-bench Verified上使用同一子集只能达到63.7%。与我们内部基础设施不兼容而被排除的11个测试用例是：

- scikit-learn__scikit-learn-14710
- django__django-10097
- psf__requests-2317
- sphinx-doc__sphinx-10435
- sphinx-doc__sphinx-7985
- sphinx-doc__sphinx-8475
- matplotlib__matplotlib-20488
- astropy__astropy-8707
- astropy__astropy-8872
- sphinx-doc__sphinx-8595
- sphinx-doc__sphinx-9711

---

> 本文由AI自动翻译，原文链接：[Claude 3.7 Sonnet and Claude Code](https://www.anthropic.com/news/claude-3-7-sonnet)
> 
> 翻译时间：2026-02-10 04:31
