---
title: 构建深度研究Agent：实现业界领先的技术与理念
title_original: 'Building Deep Research: How we Achieved State of the Art'
date: '2025-11-24'
source: Hugging Face Blog
source_url: https://huggingface.co/blog/Tavily/tavily-deep-research
author: null
summary: 本文分享了构建业界领先AI研究型Agent的经验，强调需面向未来设计Agent控制框架，避免过度优化当前模型。核心在于简化编排逻辑、利用模型与工具的新兴能力，并重点实施上下文工程。文章提出通过高级网络检索和模拟人类研究流程（收集、提炼、决策循环）来管理上下文，以克服长期任务中上下文窗口维护的挑战，从而实现高效、可扩展的深度研究。
categories:
- AI产品
tags:
- 研究型Agent
- 上下文工程
- Agent控制框架
- AI研究
- Tavily
draft: false
translated_at: '2026-01-06T01:03:03.199Z'
---

构建深度研究：我们如何实现业界领先
研究型Agent正迅速成为AI最重要的应用之一。研究是一项基础性的知识工作：收集、阅读和整合信息是写作、决策乃至编码等一切活动的基石。然而，人类驱动的研究受限于记忆力、阅读速度和时间。相比之下，AI研究型Agent能够处理海量信息、即时整合洞察，并能轻松扩展。正因如此，研究型Agent正成为当前AI的首要应用场景，并很快将成为内容生成、编码、销售等更广泛Agent工作流的核心组成部分。在本文中，我们将分享在构建业界领先的研究型Agent过程中获得的技术与理念经验，以及我们对这一领域发展方向的看法。

**面向未来构建**
**Agent控制框架**
构建Agent控制框架的任务，是创建一个通过上下文管理、工具调用、循环控制、编排和错误处理来增强模型运行时执行的软件层。然而，在快速改进的模型之上构建应用，是一项现代工程挑战。我们如何设计出能够吸收未来模型版本性能提升的软件？

这需要预测模型将如何演进，对其进步保持乐观，限制假设，并避免手工优化。

七个月前，我们艰难地吸取了这一教训，当时我们不得不放弃深度研究的首次尝试，并从头开始重建整个系统。最初的架构复杂而精密（我们曾认为这是好事），但当新一代模型出现时，其假设却成了瓶颈。

**模型**
在过去的七个月里，模型能力已经悄然但显著地演进（尤其是在工具调用能力方面）。这一单一的优化重点推动我们从工作流转向了Agent。我们相信，未来的模型将经过训练来解决当前Agent开发者的痛点。每个模型最终都由一个控制框架所使用，因此模型应该朝着服务该框架的方向演进。我们希望看到模型在高召回率摘要（用于上下文压缩）、工具调用可靠性和写作简洁性方面有所改进。

**工具**
同样，工具也应演进以支持LLM和广泛采用的Agent控制框架。最好的工具应该在工具端进行一些上下文工程，并将其从Agent中抽象出来。它们应该只返回最相关的数据，而不是将大量Token倾泻到上下文窗口中。作为工具提供商，我们在高级搜索功能上投入了大量精力，该功能内置了上下文工程。这反过来降低了下游Agent流程的幻觉和延迟。

**关键经验**
为了构建能够持续改进的Agent，我们遵循了几条指导原则：
- 简化编排逻辑，倾向于自主性。
- 密切关注模型和工具的优化方向，并利用其新兴能力。
- 专注于上下文工程（下一节将详细讨论）。

**上下文工程——一项精心策划的实践**
长期的研究任务暴露了当前Agent设计中的一个根本性挑战：随着时间的推移，维护一个干净、优化的上下文窗口的任务。如果工程师不密切关注策划上下文这一任务，Agent几乎注定会失败。以下概述了我们在深度研究领域围绕这一概念的思考。

**上下文管理的网络检索**
使用Tavily的高级搜索是克服这一挑战自然而然的第一步，因为它抽象了对原始网络内容的处理，只返回每个来源中最相关的内容块。通过利用此功能，我们让Tavily搜索承担繁重的工作，并使Tavily研究从中受益，以低延迟的方式收集最有价值的内容。

确保Agent不会过度拟合单一的研究线索，是构建有效上下文收集流程的下一步。在这方面，全局状态持久化和来源去重至关重要，在我们的案例中，它带来了三方面的好处：
- 确保Agent只接触到新鲜信息。
- 使工程师能够识别信息范围何时在缩小，并提示Agent探索未开发的相关领域。
- 有助于在后续生成过程中进行有效的来源归属。

在Tavily，与网络交互是我们的核心业务。构建一个专为深度研究设计的精细化网络检索系统，是我们整个深度研究Agent设计的基础构件。

**模拟人机网络交互**
人类的研究方式本质上是非结构化、迭代式的。我们首先定义任务：我们要完成什么以及需要什么信息。接着我们从来源收集数据，提取关键见解并将其保存在短期记忆中，让这些提炼出的想法指导我们后续的行动。

这个循环不断重复：收集信息、提炼信息、决定下一步做什么。只有当我们积累了足够的理解来生成最终交付物时，我们才会回到原始来源，将其作为参考来组装最终成果。

我们认为，深度研究Agent应以类似的方式设计，即工具输出应被提炼为反思，并且只有过去的反思集合才应作为工具调用者的上下文。与人类类似，只有当Agent开始准备最终交付物时，才必须提供原始信息作为上下文，以确保没有信息损失。

**事半功倍**
这种方法不同于基于ReAct Agent架构中的传统上下文结构。通常，工具调用和输出会通过工具调用循环传播，先前检索/生成的Token会在每次后续迭代中持续存在于上下文窗口中。这种模式可以在LangChain的Open Deep Research Agent实现中看到，从Token消耗的角度来看，它可以用以下二次级数建模，其中 是每次工具调用迭代时调用工具模型所用的Token数量， 是工具调用迭代的次数。

相反，我们提出的上下文工程方法消除了这种Token传播（因为知识提炼，即使聚合起来，与从网络收集的Token数量相比也是微不足道的），并且可以用以下线性级数建模。

比较这两种方法，每个Agent节省的Token数量因子为 ，当在多Agent系统中外推并进行大规模消耗时，节省的Token绝对值变得更加显著。

通过这种方法，我们能够将Token消耗减少66%（与Open Deep Research相比），同时在DeepResearch Bench上实现了SOTA——质量与效率的交集得到了充分体现。

**Agent产品化——一项持续的挑战**
构建生产级Agent是一种平衡行为。我们倾向于自主性以最大化性能和质量，同时仍然满足对延迟、成本和可靠性的严格要求。

**处理非确定性的工程实践**
LLM本质上是非确定性的，我们发现，给予它们有护栏约束的自由进行推理和迭代能产生最好的结果。自主性一旦出错，可能导致Agent行为偏离轨道。工具可能被错误调用，LLM可能过度拟合某个子主题，预期的推理模式可能中断。没有单一的安全措施能捕捉所有这些问题。

需要转变工程思维：将故障模式视为核心设计考虑因素，而不是事后补救。

简单的防护措施，如工具调用重试或模型级联虽有一定帮助，但主动预测异常、在提示词中强化正确模式并进行边缘情况测试，才是实现生产级、长期运行Agent（智能体）的关键。

**优化工具策略——少即是多**
根据我们的经验，向Agent（智能体）提供一套精简而核心的工具集，优于提供庞大复杂的工具集。我们曾倾向于过度设计，添加许多理论上看似有用的工具，但实际上这引入了新的故障模式，并使得LLM（大语言模型）更难持续选择正确的工具并进行有效迭代。

**评估方法**
我们使用评估来引导开发过程，但也认识到其局限性。以LLM（大语言模型）作为评判者的评估难以信赖：当前模型具有非确定性，其推理过程难以解释，并且可能成为瓶颈，特别是对于运行时间可能长达数天的单个实验的长期运行Agent（智能体）而言。

我们并未为追求评估分数而优化，而是为获取方向性反馈而优化。核心问题始终是：这项改动是否让Agent（智能体）在实践中更可靠、更有用？评估成为了验证该方向的工具，而非优化目标。直觉和细致的Agent（智能体）运行轨迹监控，始终能提供比任何单一评估分数更高信息量的反馈。总体而言，最佳结果很少是数值上的最高分。对于生产系统而言，降低Token使用量、提升可靠性、减少延迟和降低故障率等改进，比评估分数提高一分更有价值。

如果您有兴趣在实践中体验这些发现的成果，可以在此处注册申请Tavily Research的早期访问权限。

---

> 本文由AI自动翻译，原文链接：[Building Deep Research: How we Achieved State of the Art](https://huggingface.co/blog/Tavily/tavily-deep-research)
> 
> 翻译时间：2026-01-06 01:03
