---
title: Workers如何驱动Cloudflare内部维护调度管道
title_original: How Workers powers our internal maintenance scheduling pipeline
date: '2025-12-22'
source: Cloudflare Blog
source_url: https://blog.cloudflare.com/building-our-maintenance-scheduler-on-workers/
author: Kevin Deems
summary: 本文介绍了Cloudflare如何利用其无服务器平台Workers构建一个集中化、自动化的维护调度系统，以应对全球庞大基础设施的维护挑战。随着数据中心数量增长，手动协调维护已不可行。该系统将维护操作（如路由器下线）和业务约束（如客户专用出口IP规则）建模为图结构中的顶点和边，通过编程方式强制执行安全规则，防止多起维护事件重叠导致的服务中断。文章重点阐述了从初期简单方案遇到内存限制，到借鉴图数据库思想进行优化，最终在Workers上实现高效、可扩展调度器的工程实践。
categories:
- 基础设施
tags:
- Cloudflare Workers
- 基础设施运维
- 调度系统
- 无服务器计算
- 图处理
draft: false
---

Cloudflare在全球超过330个城市拥有数据中心，因此您可能会认为，在规划数据中心运营时，我们可以随时轻松中断其中几个而用户不会察觉。然而，现实情况是，中断性维护需要精心规划，并且随着Cloudflare的发展，通过基础设施和网络运营专家之间的手动协调来管理这些复杂性几乎变得不可能。

人类实时追踪每一个重叠的维护请求或考虑每一个客户特定的路由规则已不再可行。我们达到了这样一个临界点：仅靠人工监督已无法保证世界某一部分的常规硬件更新不会无意中与另一部分的关键路径发生冲突。

我们意识到需要一个集中化、自动化的"大脑"来充当安全屏障——一个能够同时洞察我们整个网络状态的系统。通过在Cloudflare Workers上构建这个调度器，我们创建了一种以编程方式强制执行安全约束的方法，确保无论我们行动多快，都不会牺牲客户所依赖服务的可靠性。

在这篇博文中，我们将解释我们是如何构建它的，并分享我们目前看到的结果。

**构建一个降低关键维护操作风险的系统**

想象一个边缘路由器，它作为一个小的冗余网关组中的一个，共同将公共互联网连接到在大都市区域运营的众多Cloudflare数据中心。在人口稠密的城市，我们需要确保位于这一小簇路由器后面的多个数据中心不会因为所有路由器同时离线而被切断。

另一个维护挑战来自我们的零信任产品——专用CDN出口IP，该产品允许客户选择特定的数据中心，使其用户流量从这些数据中心退出Cloudflare并发送到地理上靠近的源服务器，以实现低延迟。（为了简洁起见，本文将专用CDN出口IP产品称为"Aegis"，这是其旧名称。）如果客户选择的所有数据中心同时离线，他们将遇到更高的延迟甚至可能产生5xx错误，这是我们必须避免的。

我们的维护调度器解决了这类问题。我们可以确保在特定区域始终至少有一个边缘路由器处于活动状态。并且在安排维护时，我们可以看到多个预定事件的组合是否会导致客户Aegis池的所有数据中心同时离线。

在我们创建调度器之前，这些同时发生的中断性事件可能导致客户服务中断。现在，我们的调度器会通知内部操作员潜在的冲突，使我们能够提议新的时间，以避免与其他相关的数据中心维护事件重叠。

我们将这些操作场景（例如边缘路由器可用性和客户规则）定义为维护约束，这使我们能够规划更可预测和更安全的维护。

每个约束都始于一组拟议的维护项，例如网络路由器或服务器列表。然后，我们查找日历中与拟议维护时间窗口重叠的所有维护事件。

接下来，我们聚合产品API，例如Aegis客户IP池列表。Aegis返回一组IP范围，其中客户请求从特定的数据中心ID出口，如下所示。

[
{
"cidr": "104.28.0.32/32",
"pool_name": "customer-9876",
"port_slots": [
{
"dc_id": 21,
"other_colos_enabled": true,
},
{
"dc_id": 45,
"other_colos_enabled": true,
}
],
"modified_at": "2023-10-22T13:32:47.213767Z"
},
]

在此场景中，数据中心21和45彼此关联，因为我们需要至少一个数据中心在线，以便Aegis客户9876能够从Cloudflare接收出口流量。如果我们试图同时让数据中心21和45下线，我们的协调器会发出警报，提示该客户的工作负载将遭受意外影响。

最初，我们有一个简单直接的解决方案，将所有数据加载到单个Worker中。这包括所有服务器关系、产品配置以及用于计算约束的产品和基础设施健康度指标。即使在概念验证阶段，我们也遇到了"内存不足"错误。

我们需要更加了解Workers平台的限制。这要求只加载处理约束业务逻辑绝对必要的数据。如果收到德国法兰克福路由器的维护请求，我们几乎肯定不关心澳大利亚的情况，因为跨区域没有重叠。因此，我们应该只加载德国邻近数据中心的数据。我们需要一种更有效的方式来处理数据集中的关系。

**在Workers上进行图处理**

当我们审视约束时，出现了一种模式：每个约束都可以归结为两个概念：对象和关联。在图论中，这些组件分别称为顶点和边。一个对象可以是一个网络路由器，一个关联可以是数据中心中需要该路由器在线的Aegis池列表。我们从Facebook的TAO研究论文中汲取灵感，在我们的产品和基础设施数据之上建立了一个图接口。API如下所示：

type ObjectID = string
interface MainTAOInterface<TObject, TAssoc, TAssocType> {
object_get(id: ObjectID): Promise<TObject | undefined>
assoc_get(id1: ObjectID, atype: TAssocType): AsyncIterable<TAssoc>
}

核心见解在于关联是有类型的。例如，约束会调用图接口来检索Aegis产品数据。

async function constraint(c: AppContext, aegis: TAOAegisClient, datacenters: string[]): Promise<Record<string, PoolAnalysis>> {
const datacenterEntries = await Promise.all(
datacenters.map(async (dcID) => {
const iter = aegis.assoc_get(c, dcID, AegisAssocType.DATACENTER_INSIDE_AEGIS_POOL)
const pools: string[] = []
for await (const assoc of iter) {
pools.push(assoc.id2)
}
return [dcID, pools] as const
}),
)
const datacenterToPools = new Map<string, string[]>(datacenterEntries)
const uniquePools = new Set<string>()
for (const pools of datacenterToPools.values()) {
for (const pool of pools) uniquePools.add(pool)
}
const poolTotalsEntries = await Promise.all(
[...uniquePools].map(async (pool) => {
const total = aegis.assoc_count(c, pool, AegisAssocType.AEGIS_POOL_CONTAINS_DATACENTER)
return [pool, total] as const
}),
)
const poolTotals = new Map<string, number>(poolTotalsEntries)
const poolAnalysis: Record<string, PoolAnalysis> = {}
for (const [dcID, pools] of datacenterToPools.entries()) {
for (const pool of pools) {
poolAnalysis[pool] = {
affectedDatacenters: new Set([dcID]),
totalDatacenters: poolTotals.get(pool),
}
}
}
return poolAnalysis
}

我们在上面的代码中使用了两种关联类型：
*   `DATACENTER_INSIDE_AEGIS_POOL`：检索数据中心所在的Aegis客户池。
*   `AEGIS_POOL_CONTAINS_DATACENTER`：检索Aegis池需要为其提供流量的数据中心。

这些关联是彼此的倒排索引。访问模式与之前完全相同，但现在图实现可以更好地控制查询的数据量。以前，我们需要将所有Aegis池加载到内存中，并在约束业务逻辑内部进行过滤。现在，我们可以直接仅获取对应用程序重要的数据。

这个接口之所以强大，是因为我们的图实现可以在幕后提高性能，而不会使业务逻辑复杂化。这使我们能够利用Workers的可扩展性和Cloudflare的CDN，非常快速地从我们的内部系统获取数据。

我们转而使用新的图实现，发送更具针对性的API请求。

响应体量一夜之间缩减了百倍，从加载少量大型请求转变为处理大量微小请求。
虽然这解决了内存占用过高的问题，但我们却面临子请求激增的难题——原本少量的大型HTTP请求被数量级增长的小型请求所取代。一夜之间，我们开始持续突破子请求限制。

为解决此问题，我们在图计算实现层与fetch API之间构建了智能中间件层。
```javascript
export const fetchPipeline = new FetchPipeline()
.use(requestDeduplicator())
.use(lruCacher({
maxItems: 100,
}))
.use(cdnCacher())
.use(backoffRetryer({
retries: 3,
baseMs: 100,
jitter: true,
}))
.handler(terminalFetch);
```
若您熟悉Go语言，可能曾见过singleflight包。我们借鉴此思路，在fetch管道中设计的首个中间件组件可对进行中的重复HTTP请求进行去重处理，使所有请求等待同一个Promise返回数据，避免在同一Worker内产生重复请求。随后，我们采用轻量级LRU（最近最少使用）缓存机制对已处理的请求进行内部缓存。

完成上述处理后，我们利用Cloudflare的`caches.default.match`函数将所有GET请求缓存在Worker运行区域。鉴于我们拥有多个性能特征各异的数据源，需谨慎设置缓存存活时间（TTL）。例如：实时数据仅缓存1分钟；相对静态的基础设施数据根据类型可缓存1-24小时；电源管理数据可能手动更改且频率较低，因此可在边缘节点进行长期缓存。

除这些层级外，我们还配置了标准指数退避、重试和抖动机制。这有助于减少因下游资源临时不可用而产生的无效fetch调用。通过适度退避，我们提升了后续请求成功的概率。反之，若Worker持续无间隔发送请求，当源站开始返回5xx错误时极易突破子请求限制。

整合所有优化后，我们实现了约99%的缓存命中率。缓存命中率指从Cloudflare高速缓存内存响应（"命中"）的HTTP请求占比，相较于需访问控制平面数据源的慢速请求（"未命中"），计算公式为：命中数/(命中数+未命中数)。高命中率意味着更优的HTTP请求性能和更低成本，因为从Worker缓存查询数据比跨区域从源服务器获取快一个数量级。经过参数调优，我们的内存缓存与CDN缓存命中率显著提升。由于工作负载多为实时数据，我们永远无法达到100%命中率——每分钟至少需请求一次最新数据。

我们已探讨了fetch层的优化，但尚未说明如何加速源站HTTP请求。我们的维护协调器需实时响应数据中心网络质量下降和设备故障。通过分布式Prometheus查询引擎Thanos，我们将边缘性能指标高效传递至协调器。

为说明图处理接口的选择如何影响实时查询，让我们通过示例解析。为分析边缘路由器健康状态，我们可发送以下查询：
```
sum by (instance) (network_snmp_interface_admin_status{instance=~"edge.*"})
```
原本我们需要向存储Prometheus指标的Thanos服务请求所有边缘路由器的当前健康状态列表，然后在Worker内手动筛选与维护相关的路由器。这种方式存在多重缺陷：例如Thanos返回数MB的响应数据需进行编解码；Worker在处理特定维护请求时需缓存并解析这些大型HTTP响应，却要过滤掉大部分数据；由于TypeScript是单线程且JSON解析受CPU限制，发送两个大型HTTP请求会导致其中一个被阻塞，等待另一个完成解析。

现在我们直接通过图计算精准定位目标关系，例如边缘路由器与核心路由器间的接口连接（记为`EDGE_ROUTER_NETWORK_CONNECTS_TO_SPINE`）：
```
sum by (lldp_name) (network_snmp_interface_admin_status{instance=~"edge01.fra03", lldp_name=~"spine.*"})
```
结果数据平均仅1KB而非数MB，体积缩小约1000倍。这同时大幅降低了Worker内的CPU消耗，因为我们将多数反序列化工作卸载给Thanos处理。如前所述，这意味着我们需要发起更多此类小型fetch请求，但Thanos前端的负载均衡器可将请求均匀分发，从而提升该场景下的吞吐量。

我们的图计算实现与fetch管道成功驯服了海量微小实时请求形成的"惊群效应"。然而，历史数据分析带来了不同的I/O挑战——我们不再获取小型特定关系，而是需要扫描数月数据以发现冲突的维护窗口。过去Thanos会向对象存储R2发起大量随机读取。为在不损失性能的前提下解决带宽消耗过大的问题，我们采用了可观测性团队今年内部开发的新方案。

众多维护场景必须依赖历史数据来验证解决方案的准确性，并确保其能随Cloudflare网络扩张而扩展。我们既要避免引发故障，也不希望无故阻止合理的实体维护计划。为平衡这两个目标，我们可以利用两个月甚至一年前的维护事件时间序列数据，统计维护事件违反约束条件（如边缘路由器可用性或Aegis防护）的频率。今年早些时候我们曾发文介绍如何使用Thanos实现边缘软件自动发布与回滚。

Thanos主要面向Prometheus进行数据分发，但当Prometheus存储周期无法满足查询需求时，必须从对象存储（我们采用R2）下载数据。Prometheus TSDB数据块最初为本地SSD设计，依赖的随机访问模式在迁移至对象存储后成为瓶颈。当调度器需要分析数月历史维护数据以识别冲突约束时，对象存储的随机读取会产生巨大I/O开销。为此我们实现了转换层，将这些数据块转化为Apache Parquet文件。Parquet是大数据分析原生列式存储格式，按列而非按行组织数据，结合丰富的统计信息使我们能够仅提取所需数据。

此外，由于我们将TSDB数据块重写为Parquet文件，还能以支持大块顺序读取的方式存储数据。例如：
```
sum by (instance) (hmd:release_scopes:enabled{dc_id="45"})
```
在上例中，我们会选择元组`(__name__, dc_id)`作为主排序键，使名为`hmd:release_scopes:enabled`且`dc_id`值相同的指标在存储中相邻排列。

现在我们的Parquet网关通过精准的R2范围请求，仅获取查询相关的特定列，将有效载荷从数兆字节压缩至数千字节。更重要的是，由于这些文件段不可变，我们可以在Cloudflare CDN上进行强缓存。

这使得R2转变为低延迟查询引擎，让我们能即时根据长期趋势对复杂维护场景进行回测，避免了原始TSDB格式导致的超时和高尾延迟问题。

下图展示了近期的一次负载测试结果：在相同的查询模式下，Parquet 的 P90 性能最高可达旧系统的 15 倍。

若想更深入地了解 Parquet 的实现原理，您可以观看在 PromCon EU 2025 上的这场演讲：《超越 TSDB：利用 Parquet 解锁现代规模的 Prometheus》。

通过运用 Cloudflare Workers，我们成功将一个内存耗尽的系统，转变为能够智能缓存数据并利用高效可观测性工具实时分析产品与基础设施数据的平台。我们还构建了一个维护调度器，以平衡网络增长与产品性能。

但“平衡”本身就是一个动态目标。

我们每天都在全球范围内增加硬件设备，而要在不影响客户流量的前提下维护这些设备，其所需的逻辑随着产品种类和维护操作类型的增加呈指数级复杂化。我们已经攻克了第一阶段的挑战，但如今正面临着仅在此巨大规模下才会显现的、更为微妙且复杂的难题。

我们需要不畏艰难挑战的工程师。加入我们的基础设施团队，与我们共同构建未来。

---

> 本文由AI自动翻译，原文链接：[How Workers powers our internal maintenance scheduling pipeline](https://blog.cloudflare.com/building-our-maintenance-scheduler-on-workers/)
> 
> 翻译时间：2026-01-04 23:58
