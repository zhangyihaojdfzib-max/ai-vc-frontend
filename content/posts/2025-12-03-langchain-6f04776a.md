---
title: 深度智能体评估：我们的经验与模式总结
title_original: 'Evaluating Deep Agents: Our Learnings'
date: '2025-12-03'
source: LangChain Blog
source_url: https://blog.langchain.com/evaluating-deep-agents-our-learnings/
author: LangChain Accounts
summary: 本文基于LangChain团队在开发四个深度智能体应用过程中的实践经验，系统总结了评估深度智能体的关键模式。文章指出，与传统LLM评估不同，深度智能体需要为每个数据点定制测试逻辑，包括对轨迹、最终响应和其他状态的特定断言。文中介绍了单步、完整回合和多回合三种运行方式，并强调了环境设置的重要性。通过具体示例和代码片段，展示了如何利用LangSmith的测试集成来实现对智能体行为的精细化验证。
categories:
- AI产品
tags:
- 智能体评估
- LangChain
- 深度智能体
- 测试模式
- AI开发
draft: false
translated_at: '2026-01-06T01:10:41.349Z'
---

在LangChain的过去一个月里，我们在Deep Agents框架之上发布了四个应用：
- DeepAgents CLI：一个编码Agent（智能体）
- LangSmith Assist：一个应用内Agent（智能体），用于协助处理LangSmith中的各种事务
- 个人邮件助手：一个通过用户交互学习的邮件助手
- Agent Builder：一个由元深度Agent（智能体）驱动的无代码Agent（智能体）构建平台

构建并发布这些Agent（智能体）意味着需要为每个应用添加评估，我们在此过程中收获颇丰！在本文中，我们将深入探讨评估深度Agent（智能体）的以下模式。
- 深度Agent（智能体）需要为每个数据点定制测试逻辑——每个测试用例都有其自身的成功标准。
- 让深度Agent（智能体）运行单步对于验证特定场景下的决策非常有效（还能节省Token！）
- 完整的Agent（智能体）回合非常适合测试关于Agent（智能体）“最终状态”的断言。
- 多Agent（智能体）回合可以模拟真实的用户交互，但需要保持在预设轨道上。
- 环境设置很重要——深度Agent（智能体）需要干净、可复现的测试环境

术语表
在深入探讨之前，我们先定义本文中使用的几个术语。

运行Agent（智能体）的方式：
- 单步：将核心Agent（智能体）循环限制为仅运行一个回合，确定Agent（智能体）将采取的下一步行动。
- 完整回合：在单个输入上完整运行Agent（智能体），这可能包含多次工具调用迭代。
- 多回合：完整地多次运行Agent（智能体）。通常用于模拟Agent（智能体）与用户之间具有多次来回交互的“多轮”对话。

我们可以测试的内容：
- 轨迹：Agent（智能体）调用的工具序列，以及Agent（智能体）生成的特定工具参数。
- 最终响应：Agent（智能体）返回给用户的最终响应。
- 其他状态：Agent（智能体）在运行过程中生成的其他值（例如文件、其他产物）

#1：深度Agent（智能体）需要为每个数据点定制更多测试逻辑（代码）
传统的LLM（大语言模型）评估是直接的：
1) 构建示例数据集
2) 编写评估器
3) 在数据集上运行你的应用以生成输出，并用你的评估器对这些输出进行评分

每个数据点都被同等对待——通过相同的应用逻辑运行，由相同的评估器评分。

深度Agent（智能体）打破了这一假设。你希望测试的不仅仅是最终消息。“成功标准”也可能更具体于每个数据点，并且可能涉及针对Agent（智能体）轨迹和状态的特定断言。

考虑这个例子：
我们有一个能够记住用户偏好的日历调度深度Agent（智能体）。用户要求其Agent（智能体）“记住不要在上午9点前安排会议”。我们希望断言日历调度Agent（智能体）在其文件系统中更新了自己的记忆以记住此信息。

为了测试这一点，我们可能希望编写断言来验证：
1) Agent（智能体）在memories.md文件路径上调用了edit_file
2) Agent（智能体）在其最终消息中向用户传达了记忆更新
3) memories.md文件确实包含了关于不安排早期会议的信息。你可以：
- 使用正则表达式查找“9am”的提及
- 或者使用LLM-as-judge（LLM作为评判者）并设定具体的成功标准，对文件更新进行更全面的分析

LangSmith的Pytest和Vitest集成支持这种类型的定制测试。你可以为每个测试用例对Agent（智能体）的轨迹、最终消息和状态做出不同的断言。

```python
# Mark as a LangSmith test case
@pytest.mark.langsmith
def test_remember_no_early_meetings() -> None:
    user_input = "I don't want any meetings scheduled before 9 AM ET"
    # We can log the input to the agent to LangSmith
    t.log_inputs({"question": user_input})
    response = run_agent(user_input)
    # We can log the output of the agent to LangSmith
    t.log_outputs({"outputs": response})
    agent_tool_calls = get_agent_tool_calls(response)
    # We assert that the agent called the edit_file tool to update its memories
    assert any([tc["name"] == "edit_file" and tc["args"]["path"] == "memories.md" for tc in agent_tool_calls])
    # We log feedback from an llm-as-judge that the final message confirmed the memory update
    communicated_to_user = llm_as_judge_A(response)
    t.log_feedback(key="communicated_to_user", score=communicated_to_user)
    # We log feedback from an llm-as-judge that the memories file now contains the right info
    memory_updated = llm_as_judge_B(response)
    t.log_feedback(key="memory_updated", score=memory_updated)
```

有关如何使用Pytest的通用代码片段，请查看这些文档：
https://docs.smith.langchain.com/evaluation/testing

此LangSmith集成会自动将所有测试用例记录到实验中，因此你可以查看失败测试用例的轨迹（以调试出错原因）并随时间跟踪结果。

#2：单步评估既有价值又高效
在为深度Agent（智能体）运行评估时，我们大约一半的测试用例看起来像是单步评估，即在特定系列输入消息之后，LLM（大语言模型）立即决定做什么？

这对于验证Agent（智能体）在特定场景中是否以正确的参数调用了正确的工具特别有用。常见的测试用例包括：
- 它是否调用了正确的工具来搜索会议时间？
- 它是否检查了正确的目录内容？
- 它是否更新了其记忆？

回归通常发生在单个决策点，而不是整个执行序列中。如果使用LangGraph，其流式功能允许你在单个工具调用后中断Agent（智能体）以检查输出——这样你就可以在无需完整Agent（智能体）序列开销的情况下及早发现问题。

在下面的代码片段中，我们在工具节点之前手动引入了一个断点，使我们能够轻松地让Agent（智能体）运行单步。然后我们可以检查并对该单步之后的状态进行断言。

```python
@pytest.mark.langsmith
def test_single_step() -> None:
    state_before_tool_execution = await agent.ainvoke(
        inputs,
        # interrupt_before specifies nodes to stop before
        # interrupting before the tool node allows us to inspect the tool call args
        interrupt_before=["tools"]
    )
    # We can see the message history of the agent, including the latest tool call
    print(state_before_tool_execution["messages"])
```

#3：完整Agent（智能体）回合让你获得完整视图
将单步评估视为你的“单元测试”，确保Agent（智能体）在特定场景中采取预期行动。同时，完整的Agent（智能体）回合也很有价值——它们向你展示了Agent（智能体）所采取的端到端行动的完整视图。

完整的Agent（智能体）回合让你以多种方式测试Agent（智能体）行为：
1) 轨迹：评估完整轨迹的一种非常常见的方法是确保在行动过程中的某个时刻调用了特定工具，但具体何时调用并不重要。在我们的日历调度器示例中，调度器可能需要多次工具调用来找到一个适合所有参与方的合适时间段。
2) 最终响应：在某些情况下，最终输出的质量比Agent（智能体）采取的具体路径更重要。我们发现这对于编码和研究等更开放式的任务确实如此。
3) 其他状态：评估其他状态与评估Agent（智能体）的最终响应非常相似。一些Agent（智能体）会创建产物，而不是以聊天格式响应用户。通过检查LangGraph中Agent（智能体）的状态，可以轻松检查和测试这些产物。
- 对于编码Agent（智能体）→ 读取并测试Agent（智能体）编写的文件。
- 对于研究Agent（智能体）→ 断言Agent（智能体）找到了正确的链接或来源。

完整的Agent（智能体）回合让你获得Agent（智能体）执行的完整视图。LangSmith使得将完整的Agent（智能体）回合视为轨迹变得非常容易，你可以看到延迟和Token使用等高级指标，同时还可以分析每个模型调用或工具调用的具体步骤。

#4：跨多回合运行Agent（智能体）可模拟完整的用户交互
某些场景需要测试具有多个顺序用户输入的多轮对话中的Agent（智能体）。

挑战在于，如果你简单地硬编码一系列输入，而Agent偏离了预期路径，后续硬编码的用户输入可能就会变得不合理。
我们通过在Pytest和Vitest测试中添加条件逻辑来解决这个问题。例如，我们会：
- 运行第一轮对话，然后检查Agent的输出。
- 如果输出符合预期，则运行下一轮。
- 如果不符合预期，则提前让测试失败。（这是可行的，因为我们有灵活性在每一步之后添加检查。）
这种方法让我们能够运行多轮评估，而无需为Agent所有可能的分支路径建模。如果我们想单独测试第二轮或第三轮对话，只需设置一个从该点开始、具有适当初始状态的测试即可。
#5：搭建正确的评估环境很重要
深度Agent是有状态的，旨在处理复杂、长期运行的任务——这通常需要更复杂的环境来进行评估。
与那些环境仅限于少数通常无状态工具的简单LLM评估不同，深度Agent每次评估运行都需要一个全新、干净的环境，以确保结果可复现。
编码Agent清晰地说明了这一点。Harbor为TerminalBench提供了一个在专用Docker容器或沙箱内运行的评估环境。对于DeepAgents CLI，我们采用了一种更轻量级的方法：为每个测试用例创建一个临时目录，并在其中运行Agent。
更广泛的要点是：深度Agent评估需要每个测试都能重置的环境——否则你的评估会变得不稳定且难以复现。
提示：模拟你的API请求
LangSmith Assist需要连接到真实的LangSmith API。针对实时服务运行评估可能既缓慢又昂贵。相反，可以将HTTP请求记录到文件系统中，并在测试执行期间重放它们。对于Python，vcr效果很好；对于JS，我们通过一个Hono应用来代理fetch请求。
模拟或重放API请求使深度Agent评估更快、更容易调试，特别是当Agent严重依赖外部系统状态时。
使用LangSmith评估深度Agent
以上技术是我们在为深度Agent驱动的应用程序编写自己的测试套件时看到的常见模式。对于你的具体应用，你可能只需要上述模式的一个子集——因此，你的评估框架具备灵活性非常重要。如果你正在构建一个深度Agent并开始进行评估，不妨看看LangSmith的测试集成功能！

---

> 本文由AI自动翻译，原文链接：[Evaluating Deep Agents: Our Learnings](https://blog.langchain.com/evaluating-deep-agents-our-learnings/)
> 
> 翻译时间：2026-01-06 01:10
