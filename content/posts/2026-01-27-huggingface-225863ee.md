---
title: 解锁GPT-OSS的智能体强化学习训练：实践回顾与挑战
title_original: 'Unlocking Agentic RL Training for GPT-OSS: A Practical Retrospective'
date: '2026-01-27'
source: Hugging Face Blog
source_url: https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl
author: ''
summary: 本文探讨了为GPT-OSS模型解锁智能体强化学习训练的实践过程。智能体强化学习通过与环境交互收集同策略数据，优化长视野决策，适用于需多步推理和工具调用的场景。文章以领英的AI智能体应用为例，说明其重要性。实验使用verl框架和GPT-OSS-20B模型，发现训练中存在KL散度爆炸和奖励不增的问题，并通过分析PPO同策略中重要性采样比率偏差，揭示了MoE对数概率不匹配的根本原因。
categories:
- AI研究
tags:
- 智能体强化学习
- GPT-OSS
- PPO算法
- 模型训练
- MoE模型
draft: false
translated_at: '2026-01-27T04:40:12.612008'
---

# 解锁GPT-OSS的智能体强化学习训练：一次实践回顾

智能体强化学习（RL）扩展了传统的大语言模型训练，其优化目标不仅限于单轮响应，而是通过训练期间与环境的直接交互来学习整个决策过程。与依赖静态数据集的传统单轮强化学习或基于离线偏好的方法不同，智能体强化学习通过主动收集同策略数据来训练策略，这些数据是在智能体规划动作、调用工具、观察结果以及在模拟或真实环境中跨多步轨迹调整其行为时产生的。这种交互驱动的优化将功劳分配给了长视野决策，其中诸如查询重构、工具选择和执行顺序等中间选择会直接影响下游的成功。训练遵循一个迭代闭环：智能体与环境交互以收集推演轨迹，计算这些轨迹上的奖励，根据观察到的结果更新策略，然后使用更新后的策略驱动下一轮交互和数据收集，例如使用GRPO或PPO算法。

领英是一家以AI为先的公司，构建了智能体来帮助专业人士取得更大成功。在这种场景下，模型必须基于不完整信息进行推理，与结构化服务交互，并适应跨多步骤不断演变的用户意图，而不是产生单一的静态响应。这些能力对于支持招聘人员、求职者、知识寻求者和学习者等最终用户目标的智能体尤为关键，例如检索信息、优化查询、协调工具和执行多步骤工作流。通过交互学习稳健的决策策略，智能体强化学习为通过端到端优化构建可扩展、可靠且适应性强的AI系统提供了原则性基础。

GPT-OSS模型已显示出与OpenAI o3-mini和o4-mini相当的性能[参考]，但其对智能体强化学习训练的适用性尚未得到验证。最近的工作大多集中在无需工具调用的微调上，例如：使用gpt-oss和Hugging Face Transformers进行微调以及unsloth教程：如何微调gpt-oss。本篇博客探讨了为GPT-OSS解锁智能体强化学习训练的历程，将其作为智能体应用的潜在骨干模型。

在我们的实验中，我们使用verl作为训练框架，因为它是开源社区中最受欢迎的框架之一。我们使用gsm8k、Retool任务和可验证指令遵循任务，这些任务在强化学习训练中常用。我们重点展示GPT-OSS-20B模型的实验结果，我们的注意力汇聚点修复也适用于GPT-OSS-120B。此外，我们还使用了Qwen-2.5-32B模型来作为强化学习训练期间标准指标趋势的基准。

## GPT-OSS强化学习训练的挑战

verl一直是团队使用的开源框架，团队之前曾与之合作并做出贡献，以帮助普及智能体强化学习训练。随着GPT-OSS中新的Harmony聊天模板的引入，第一步是确保训练框架完全支持Harmony所需的更新后的消息格式和对话语义。这一步有助于确保在新模板下，推演生成、轨迹构建和工具解析保持一致和正确。

团队使用ReTool作为验证代码正确性的代表性示例。ReTool是一项智能体编码任务，要求模型在代码编译器工具的帮助下解决数学问题。这种设置使模型能够专注于核心推理和算法逻辑，而将实际的算术和执行委托给工具。在一个回合中，模型与代码工具多次交互，使用执行结果作为反馈来完善其解决方案。在轨迹结束时，模型产生最终答案，并据此计算奖励。

在最初的训练运行中，我们观察到KL散度和熵值爆炸，同时奖励没有增加，这表明GPT-OSS训练设置存在潜在问题，如图1所示。

![批次中的平均梯度范数](/images/posts/e3be76a151f7.png)

![批次中的平均奖励](/images/posts/345c77e5ff35.png)

图1. 左图：与GPT-OSS 20B相比，Qwen32b的奖励显著更高；右图：随着训练进行，梯度范数爆炸。

## verl中的实用调试之旅：恢复PPO同策略完整性

### 恢复PPO同策略完整性：修复MoE对数概率不匹配问题

![](/images/posts/fb7ef8a3eaa1.png)

图2. 即使在同策略训练中，重要性采样裁剪值也非零。

我们专注于同策略方法，因为它们提供了更高的稳定性和更可靠的收敛性。纯同策略近端策略优化（PPO）的基础要求重要性采样比率必须恰好为1。重要性比率的数学定义是：

ratio=π(a∣s)πold(a∣s)\text{ratio} = \frac{\pi(a \mid s)}{\pi_{\text{old}}(a \mid s)}ratio=πold​(a∣s)π(a∣s)​

这一要求确保策略更新仅针对当前策略π(a | s) = πold(a | s)生成的数据执行，防止意外的裁剪。

我们在ReTool训练中观察到了非零的裁剪值，如图2所示，这源于两个对数概率之间的不匹配：

- 当前对数概率log_prob：log(π(a | s))
- 旧对数概率old_log_prob：log(πold(a | s))

根本原因：双重前向传递与MoE架构

在verl 0.3.0之前，实现依赖于对同一状态-动作对进行两次独立的前向传递（一次计算当前log_prob，一次检索存储的old_log_prob）。

在像GPT-OSS这样的混合专家（MoE）架构中，门控网络将输入路由到不同的专家。由于实现因素（例如，细微的浮点差异或显式的随机性），两次传递之间的专家路由可能略有不同。感兴趣的读者可以进一步阅读通过对齐训练和推理路由器来稳定MoE强化学习。

这种路由差异导致：

log⁡(π(a∣s))≠log⁡(πold(a∣s))\log(\pi(a \mid s)) \neq \log(\pi_{\text{old}}(a \mid s))log(π(a∣s))=log(πold​(a∣s))

由此产生的比率偏离了1，错误地触发了PPO裁剪，违反了核心的同策略假设。

解决方案：通过对数概率替换强制比率=1

该修复方案通过在已知环境为同策略（即小批量大小等于全局批量大小）时，逻辑上覆盖有缺陷的计算来解决此问题：

```python
if on_policy:
    old_log_prob = log_prob.detach()
else:
    old_log_prob = model_inputs["old_log_probs"]

```

通过将old_log_prob设置为新计算的log_prob（分离以防止梯度流经参考值），重要性比率在数学上被强制恢复为1。此策略绕过了MoE非确定性路由引起的不稳定性，并保证了PPO训练期间严格的同策略行为。

### 纠正训练-推理不匹配

尽管修复对数概率不匹配将重要性采样裁剪比率降至零，但梯度范数继续爆炸，奖励也未能改善。为了隔离问题，我们将训练简化为GSM8K，这是一个无需智能体工具使用的单步任务。同样的不稳定性仍然存在，如图3中的绿色曲线所示，这表明在verl下使用GPT-OSS进行基本强化学习训练存在根本性问题。

我们假设**训练-推理不匹配**可能是一个潜在原因：推理时执行（如vLLM和SGLang等引擎为吞吐量进行激进优化）与FSDP下的训练时执行（优先考虑数值精度和稳定性）之间的差异，可能会将原本的**在线策略强化学习**有效地转变为**离线策略优化**。

这篇博客详细阐述了为何此类不匹配会导致梯度不稳定和奖励无法提升。图3比较了使用与不使用rollout校正的训练运行情况（详见这篇verl博客）。应用rollout校正后，训练动态显著改善，梯度范数保持稳定而非爆炸。

然而，如图4左图所示，奖励仅略有提升，在简单的GSM8K任务上的收敛速度相比更小的稠密模型变体仍然明显更慢。

![](/images/posts/dd75fd426ef3.png)

![](/images/posts/d73169b24eb9.png)

![](/images/posts/1c64231c410e.png)

**图3. 不同训练配置下的梯度范数行为。** 绿色：未使用rollout校正的训练，表现出不稳定的梯度。红色：冻结注意力层以将问题隔离到注意力机制的训练，实现了部分稳定。蓝色：启用rollout校正（序列级重要性采样）的训练，产生了稳定的梯度范数。

![批次中的平均奖励](/images/posts/2bcc7e4f9e5f.png)

![批次中rollout策略与训练策略之间的最大绝对对数困惑度差异](/images/posts/de7c80d92167.png)

**图4. 左图：** 即使在应用rollout校正后，GSM8K上的奖励提升仍然缓慢，性能与训练期间冻结注意力层的运行相当。**右图：** 在推理引擎（支持注意力汇聚前向传递的Triton内核的SGLang）与训练堆栈（使用FlashAttention-v2的FSDP）之间观察到显著的对数困惑度不匹配，表明存在较大的训练-推理不一致性。

为了进一步隔离根本原因，我们在训练期间冻结了注意力层，并观察到与未冻结运行相似的奖励动态（图4中的蓝色曲线与黄色曲线）。这表明学习主要由MoE层驱动，而注意力机制的有效贡献低于预期。此外，我们观察到使用不同注意力内核的推理引擎与分布式训练堆栈之间存在显著的Token级概率不匹配。这些观察共同促使我们对注意力机制进行更深入的调查。

## FlashAttentionV3中的注意力汇聚支持

GPT-OSS中使用的注意力汇聚是可学习的标量参数（每个注意力头一个），在softmax计算中充当"虚拟Token"。它们允许模型将注意力权重分配给一个学习到的汇聚点，而不是强迫所有注意力都集中在内容Token上，这已被证明可以提高流式推理和使用滑动窗口注意力的训练中的注意力稳定性。

经过深入调查，我们发现了几个主要问题：

-   verl在`fsdp_worker`中硬编码了FlashAttention v2，它不支持注意力汇聚。
-   FlashAttention v2和v3不支持注意力汇聚的反向传播，因此即使启用了FlashAttention v3，它也无法按预期工作。
-   由于前向传播尚未合并到原始的FlashAttention v3仓库中，我们利用了vLLM FlashAttention分支（PR #75）中的前向传播，并实现了反向传播来计算汇聚点梯度。

### 标准注意力

```python
scores = QK^T / sqrt(d)               
probs = softmax(scores, dim=-1)       
output = probs @ V                   

```

### 带汇聚点的注意力（GPT-OSS）

```python
scores = QK^T / sqrt(d)                               
combined = concat([scores, sink_param], dim=-1)       
probs = softmax(combined, dim=-1)                     
probs_content = probs[..., :-1]                       
output = probs_content @ V                           

```

**关键区别：** 汇聚点参与softmax归一化，但不贡献于输出。

### 数学公式

第i行中内容Token j的注意力权重定义为：

Pij=exp⁡(Sij)∑j′=1Nkexp⁡(Sij′)+exp⁡(Sh)P_{ij}
=
\frac{\exp(S_{ij})}
{\sum_{j'=1}^{N_k} \exp(S_{ij'}) + \exp(S_h)}Pij​=∑j′=1Nk​​exp(Sij′​)+exp(Sh​)exp(Sij​)​

其中：

-   Sij= QiKj⊤/ √d 是注意力分数
-   Pij是内容Token的注意力权重
-   Sh是头h的可学习汇聚点参数

**汇聚点概率：**

汇聚点概率被计算但不用于输出：

Pi,h=exp⁡(Sh)∑j′=1Nkexp⁡(Sij′)+exp⁡(Sh)P_{i,h}
=
\frac{\exp(S_h)}
{\sum_{j'=1}^{N_k} \exp(S_{ij'}) + \exp(S_h)}Pi,h​=∑j′=1Nk​​exp(Sij′​)+exp(Sh​)exp(Sh​)​

### 反向传播

损失L关于汇聚点参数Sh的梯度为：

∂L∂Sh=−∑iPi,h(∂L∂Si,h−∑j∈{1,…,Nk}Pij∂L∂Sij)\frac{\partial L}{\partial S_h}
=
-
\sum_i
P_{i,h}
\left(
\frac{\partial L}{\partial S_{i,h}}
-
\sum_{j \in \{1,\ldots,N_k\}}
P_{ij}
\frac{\partial L}{\partial S_{ij}}
\right)∂Sh​∂L​=−i∑​Pi,h​​∂Si,h​∂L​−j∈{1,…,Nk​}∑​Pij​∂Sij​∂L​​

-   Pi,h是第i行的汇聚点注意力概率
-   ∂L/∂Sij是关于注意力分数（包括汇聚点）的梯度

**简化梯度：**

由于汇聚点被计算但不用于输出，其梯度∂L/∂Si,h= 0。

因此，反向传播方程简化为：

∂L∂Sh=−∑iPi,h(∑j∈{1,…,Nk}Pij∂L∂Sij)\frac{\partial L}{\partial S_h}
=
-
\sum_i
P_{i,h}
\left(
\sum_{j \in \{1,\ldots,N_k\}}
P_{ij}
\frac{\partial L}{\partial S_{ij}}
\right)∂Sh​∂L​=−i∑​Pi,h​​j∈{1,…,Nk​}∑​Pij​∂Sij​∂L​​

前向传播改编自vLLM的FlashAttention分支，我们实现了反向传播来计算汇聚点参数的梯度。该实现将在内部审查流程后发布。

### 结果

在FlashAttention v3中应用修复后，我们观察到GPT-OSS-20B在一系列强化学习任务上的收敛速度显著加快。这些任务包括数学推理的单轮RL（GSM8K — 图5中的红色曲线）、指令遵循（VerifyIf，在领域外多if基准上评估 — 图6），以及使用工具的多轮智能体RL（ReTool — 图7）。

在所有设置中，训练变得稳定并展现出持续的奖励提升。

![](/images/posts/6eedac3e832d.png)

**图5. 单轮GSM8K，** 红色曲线在没有修复的情况下比其他曲线收敛得快得多。

![批次中的平均熵](/images/posts/53f2f87362fb.png)

![批次中的平均梯度范数](/images/posts/26359d0797e4.png)

![批次中的平均奖励](/images/posts/98b202f73817.png)

**图6. 在可验证的指令遵循任务上，** 未修复的运行崩溃了（蓝色），而修复后的运行显示出稳定的奖励提升。

![批次中的平均梯度范数](/images/posts/608cd6cc40be.png)

![批次中的平均奖励](/images/posts/3dddd3f4df6a.png)

![aime_2025的val score accuracy mean@30](/images/posts/269c2ef32636.png)

**图7. 在Retool任务上，** 修复后的运行显示出稳定的奖励提升且没有梯度爆炸（fa2是未修复的flash attention 2，而fa3是修复后的flash attention 3）。修复后，验证准确率得分现在上升了。

## 内存高效训练

### 缓解由重复MoE专家实例化引起的FSDP内存激增

我们持续遇到的一个问题是在FSDP前向传播期间过度的内存分配，当在16个H200节点上训练GPT-OSS-20B bf16模型时（最大响应长度：16k，提示长度：8k），这导致了重复的内存不足（OOM）故障。对于一个200亿参数的MoE模型来说，这种行为是高度出乎意料的。

```text
2025-11-27T11:15:27.927Z [36m(TaskRunner pid=32081)[0m 文件 "/home/jobuser/.local/lib/python3.10/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py", 第 123 行, 在 forward 函数中
2025-11-27T11:15:27.927Z [36m(TaskRunner pid=32081)[0m hidden_states = hidden_states.repeat(num_experts, 1)
2025-11-27T11:15:27.927Z [36m(TaskRunner pid=32081)[0m torch.OutOfMemoryError: CUDA 内存不足。尝试分配 180.00 GiB。GPU 0 的总容量为 139.72 GiB，其中 110.94 GiB 空闲。进程 685851 已使用 24.88 GiB 内存。进程 692458 已使用 3.87 GiB 内存。在已分配的内存中，23.28 GiB 由 PyTorch 分配，84.43 MiB 由 PyTorch 保留但未分配。
```

我们确定该问题源于 Hugging Face Transformers 中 MoE 前向传播路径的两种不同实现。其他用户也报告了此问题：https://github.com/huggingface/transformers/issues/40073；当 verl 在 FSDP 下计算对数概率时，会触发推理前向传播路径。在当前 Hugging Face 的实现中，该路径会为所有专家复制隐藏状态并执行批处理矩阵乘法，从而在 GPU 内存中实例化极大的张量。相比之下，训练前向路径使用 for 循环顺序处理每个专家，然后合并结果。虽然速度较慢，但这种方法的内存效率显著更高。

```python
    @GPUMemoryLogger(role="dp actor", logger=logger)
    def compute_log_prob(self, data: DataProto, calculate_entropy=False) -> torch.Tensor:
        """
        ....
        """
        
        self.actor_module.eval()
        ...

```

我们修补了 Hugging Face 的实现，以使用内存效率更高的执行路径，避免重复实例化专家。

### 结合 Flash Attention V3 的序列并行

Agentic RL 要求智能体在多步中与环境交互，同时维护不断扩展的上下文。每一步的观察和环境反馈都会附加到上下文中，并用作后续决策的输入，这给训练期间的内存效率和可扩展性带来了重大挑战。

在全分片数据并行（FSDP）下，模型参数、优化器状态和梯度在整个世界大小（即训练集群中的所有 GPU）上进行分片。每个 GPU 仅存储和更新其分配的参数分片，而 rollout 数据在所有 GPU 上复制——这意味着每个 GPU 都会处理每个 rollout 的完整智能体交互历史。

在前向传播过程中，当计算到达参数在本地不可用的层时，会触发 `all_gather` 操作以在 GPU 间实例化完整参数。在反向传播过程中，相应的 `reduce_scatter` 操作聚合梯度，并确保每个 GPU 仅保留其本地分片。这提供了一定程度的扩展性：随着 GPU 数量的增加，每个 GPU 的内存占用会减少。

FSDP 通过在 GPU 间分片模型参数、梯度和优化器状态，提供了模型级别的扩展。序列并行（或上下文并行）则通过在不同设备间划分输入序列，进一步降低了每个 GPU 的内存消耗，从而降低了每个 GPU 的峰值激活内存。

随着序列并行维度的增加，每个 GPU 的最大激活内存相应减少。我们实现的序列并行能够感知注意力汇聚点，并与 FlashAttention v3 兼容（图 8，右）。

![SP  (2)](/images/posts/9274c236c10d.png)

图 8. 左：无序列并行的推理。右：带序列并行的推理，在注意力层前后执行额外的 all-to-all 通信。这在不同并行工作节点间划分序列，并将注意力计算的峰值内存占用减少与序列并行度成比例的量。

序列并行沿序列维度进行扩展，以减少每个 GPU 的激活内存占用。来自所有序列的输入 Token 通过移除填充 Token 被打包成一个连续的列表，同时使用位置 ID 来区分属于不同序列的 Token。这种设计天然受益于 FlashAttention 的变长支持。对于序列并行，注意力层之外的层没有位置间依赖关系；因此，它们不需要每个 GPU 持有完整的序列分片，并且这些层不需要额外的通信。

然而，注意力层要求属于同一序列的所有 Token 都位于同一个 GPU 上，以便正确计算注意力权重。为了满足这个约束，会执行一次 all-to-all 通信来收集序列元素，分割在注意力头级别进行。这种设计避免了注意力计算本身内部的通信，否则这种通信将极其昂贵。在注意力层之后，一次 all-to-all 通信将输出重新分发回其原始的序列并行布局，之后剩余的非注意力层可以在无需进一步同步的情况下继续执行。

## 结论

我们为 GPT-OSS 骨干模型启用 Agentic RL 训练的历程是一次务实的回顾，突显了要解锁开源 LLM 的高级能力，需要细致、深入的工程工作。

我们做出的贡献改变了 GPT-OSS 在 Agentic 应用中的可行性，具体通过：

- **稳定 PPO**：我们贡献了一个修复方案，以恢复同策略完整性，覆盖了由 MoE 架构的非确定性引起的对数概率不匹配问题（图 2）。
- **启用注意力汇聚点支持**：我们成功地将注意力汇聚点反向传播实现并集成到 FlashAttention v3 中，纠正了先前导致不稳定和收敛缓慢的灾难性训练-推理不匹配问题（图 5、6 和 7）。
- **扩展内存效率**：我们引入了关键的内存优化，包括修补 MoE 实例化过程，以及将序列并行与新的注意力汇聚点支持相结合，从而能够使用对多步智能体至关重要的长上下文窗口进行训练（图 8）。

这些工程努力验证了 GPT-OSS 作为构建下一代智能、多步决策智能体的可扩展且高性能的骨干模型。

## 致谢

感谢 Deepak Agarwal、Bee-Chung Chen、Animesh Singh、Gungor Polatkan、Balaji Krishnapuram 和 Jitendra Agarwal 的领导支持。

## 参考文献

1. Feng, Jiazhan, et al.Retool: Reinforcement Learning for Strategic Tool Use in LLMs.arXiv preprint arXiv:2504.11536 (2025).
2. Xiao, Guangxuan, et al.Efficient Streaming Language Models with Attention Sinks.arXiv preprint arXiv:2309.17453 (2023).
3. When Speed Kills Stability: Demystifying RL Collapse from the Training–Inference Mismatch.https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda

Feng, Jiazhan, et al.Retool: Reinforcement Learning for Strategic Tool Use in LLMs.arXiv preprint arXiv:2504.11536 (2025).

Xiao, Guangxuan, et al.Efficient Streaming Language Models with Attention Sinks.arXiv preprint arXiv:2309.17453 (2023).

When Speed Kills Stability: Demystifying RL Collapse from the Training–Inference Mismatch.https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda

---

> 本文由AI自动翻译，原文链接：[Unlocking Agentic RL Training for GPT-OSS: A Practical Retrospective](https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl)
> 
> 翻译时间：2026-01-27 04:40
