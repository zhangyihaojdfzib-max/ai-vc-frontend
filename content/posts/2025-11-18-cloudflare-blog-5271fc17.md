---
title: 2025年11月18日Cloudflare全球服务中断事件分析
title_original: Cloudflare outage on November 18, 2025
date: '2025-11-18'
source: Cloudflare Blog
source_url: https://blog.cloudflare.com/18-november-2025-outage/
author: Matthew Prince
summary: 2025年11月18日，Cloudflare因数据库权限变更导致Bot Management系统特征文件异常增大，超出软件限制，引发全球网络核心服务大规模中断。故障持续约6小时，影响CDN、安全服务、Turnstile、Workers
  KV等多个核心产品。事件非攻击所致，经排查后通过回滚文件版本和重启服务逐步恢复。Cloudflare对此深表歉意，并承诺将深入分析系统与流程缺陷，防止类似事件再次发生。
categories:
- AI基础设施
tags:
- Cloudflare
- 服务中断
- 故障分析
- 数据库运维
- Bot Management
draft: false
translated_at: '2026-01-06T01:16:34.793Z'
---

2025年11月18日11:20 UTC（本文所有时间均为UTC），Cloudflare的网络开始出现核心网络流量交付的重大故障。对于试图访问我们客户网站的互联网用户而言，这表现为一个显示Cloudflare网络内部错误的页面。

该问题并非由任何类型的网络攻击或恶意活动直接或间接导致。相反，它是由我们一个数据库系统的权限变更触发的，该变更导致数据库向我们的Bot Management系统使用的"特征文件"中输出了多个重复条目。这导致该特征文件的大小翻倍。随后，这个超出预期的特征文件被传播到了构成我们网络的所有机器上。

在我们网络上路由流量的软件会读取此特征文件，以使我们的Bot Management系统能够跟上不断变化的威胁。该软件对特征文件的大小设定了限制，而翻倍后的文件大小超出了此限制，从而导致软件运行失败。

在我们最初错误地怀疑所观察到的症状是由超大规模DDoS攻击引起之后，我们正确地识别出了核心问题，并得以停止传播超出预期的特征文件，并用该文件的早期版本替换了它。到14:30，核心流量已基本恢复正常。在接下来的几个小时里，我们努力缓解了流量恢复上线时网络各部分增加的负载。截至17:06，Cloudflare的所有系统均已恢复正常运行。

我们对给客户乃至整个互联网造成的影响深表歉意。鉴于Cloudflare在互联网生态系统中的重要性，我们任何系统的任何中断都是不可接受的。我们的网络曾有一段时间无法路由流量，这对我们团队的每一位成员来说都深感痛心。我们知道我们今天让大家失望了。

这篇文章将深入详述事件发生的具体经过，以及哪些系统和流程出现了故障。同时，这也是我们为确保此类中断不再发生而计划采取行动的开始——尽管并非终点。

下图显示了Cloudflare网络提供的5xx错误HTTP状态码数量。正常情况下，这个数值应该非常低，在中断发生前也确实如此。

11:20之前的数量是我们网络中观察到的5xx错误的预期基线。随后的峰值和波动则显示了我们的系统因加载错误的特征文件而出现故障。值得注意的是，我们的系统随后会恢复一段时间。这对于内部错误而言是非常不寻常的行为。

解释是，该文件由一个在ClickHouse数据库集群上运行的查询每五分钟生成一次，该集群当时正逐步更新以改进权限管理。只有当查询在已更新的集群部分运行时，才会生成错误数据。因此，每五分钟就有机会生成一组正确或错误的配置文件，并迅速传播到整个网络。

这种波动使得情况变得不明朗，因为整个系统会恢复，然后再次故障，这取决于有时是好、有时是坏的配置文件被分发到我们的网络。最初，这让我们相信可能是攻击所致。最终，每个ClickHouse节点都生成了错误的配置文件，波动在故障状态下稳定下来。

错误一直持续到14:30开始识别并解决根本问题。我们通过停止生成和传播错误的特征文件，并手动将一个已知良好的文件插入特征文件分发队列来解决问题。然后强制重启了我们的核心代理。

上图中剩余的长尾部分是我们的团队重启了进入错误状态的剩余服务，5xx错误码数量在17:06恢复正常。

以下服务受到影响：

| 服务/产品 | 影响描述 |
|---|
| 核心CDN和安全服务 | HTTP 5xx状态码。本文开头的截图显示了呈现给最终用户的典型错误页面。 |
| Turnstile | Turnstile 加载失败。 |
| Workers KV | Workers KV 返回的HTTP 5xx错误数量显著增加，因为对KV"前端"网关的请求由于核心代理故障而失败。 |
| Dashboard | 虽然仪表板大部分功能正常，但由于登录页面上的Turnstile不可用，大多数用户无法登录。 |
| Email Security | 虽然电子邮件处理和投递未受影响，但我们观察到暂时无法访问一个IP信誉源，这降低了垃圾邮件检测的准确性，并阻止了一些新域名年龄检测的触发，未观察到对客户的关键影响。我们还看到一些自动移动操作失败；所有受影响的消息均已审查并修复。 |
| Access | 对于大多数用户，身份验证失败广泛存在，从事件开始时开始，一直持续到13:05启动回滚。任何现有的Access会话均未受影响。<br>所有失败的身份验证尝试都导致错误页面，这意味着在身份验证失败期间，这些用户均未到达目标应用程序。此期间成功的登录在此次事件中被正确记录。<br>当时尝试的任何Access配置更新要么完全失败，要么传播非常缓慢。所有配置更新现已恢复。 |

除了返回HTTP 5xx错误外，我们在影响期间还观察到来自CDN的响应延迟显著增加。这是由于我们的调试和可观测性系统消耗了大量CPU，这些系统会自动为未捕获的错误添加额外的调试信息。

**Cloudflare如何处理请求，以及今天哪里出了问题**

每个发送到Cloudflare的请求都会在我们的网络中沿着明确定义的路径传输。它可能来自加载网页的浏览器、调用API的移动应用，或来自其他服务的自动化流量。这些请求首先在我们的HTTP和TLS层终止，然后流入我们的核心代理系统（我们称之为FL，即"Frontline"），最后通过Pingora，后者执行缓存查找或在需要时从源站获取数据。

我们之前在此处分享了关于核心代理工作原理的更多细节。

当请求通过核心代理时，我们会运行网络中可用的各种安全和性能产品。代理应用每个客户的独特配置和设置，从执行WAF规则和DDoS防护，到将流量路由到Developer Platform和R2。它是通过一组特定领域的模块来实现的，这些模块将配置和策略规则应用于流经我们代理的流量。

其中一个模块——Bot Management，就是今天中断的根源。

Cloudflare的Bot Management包含一个机器学习模型（以及其他系统），我们用它为流经我们网络的每个请求生成机器人分数。我们的客户使用机器人分数来控制允许或拒绝哪些机器人访问其站点。

该模型以一个"特征"配置文件作为输入。在此上下文中，特征是机器学习模型用来预测请求是否自动化的单个特征。特征配置文件是各个特征的集合。

该特征文件每隔几分钟刷新一次，并发布到我们的整个网络，使我们能够对互联网上流量模式的变化做出反应。它使我们能够应对新型机器人和新的机器人攻击。因此，随着恶意行为者快速改变策略，频繁且快速地部署该文件至关重要。

我们底层用于生成此文件的ClickHouse查询行为（下文解释）发生变更，导致文件中出现了大量重复的"特征"行。

这改变了之前固定大小的特征配置文件的大小，导致机器人模块触发错误。
因此，核心代理系统（负责处理客户流量）为任何依赖机器人模块的流量返回了HTTP 5xx错误码。这也影响了依赖核心代理的Workers KV和Access服务。
与本次事件无关的是，我们当时（以及目前）正在将客户流量迁移到新版本的代理服务，内部代号为FL2。两个版本都受到了该问题的影响，尽管观察到的影响有所不同。
部署在新FL2代理引擎上的客户观察到了HTTP 5xx错误。部署在旧代理引擎（称为FL）上的客户没有看到错误，但机器人评分未能正确生成，导致所有流量收到的机器人评分均为零。部署了规则以拦截机器人的客户会看到大量误报。未在其规则中使用我们机器人评分的客户则未受到任何影响。
另一个我们观察到的明显症状让我们误判，并使我们相信这可能是一次攻击：Cloudflare的状态页面宕机了。该状态页面完全托管在Cloudflare基础设施之外，不依赖Cloudflare。虽然事后证明这只是巧合，但它导致部分诊断问题的团队成员认为攻击者可能同时针对我们的系统和状态页面。当时访问状态页面的访客会看到一条错误信息：
在内部事件聊天室中，我们担心这可能是近期一系列高流量Aisuru DDoS攻击的延续：
查询行为变更
我在上面提到，底层查询行为的变更导致特征文件包含了大量重复行。所讨论的数据库系统使用的是ClickHouse软件。
为了理解背景，了解ClickHouse分布式查询的工作原理会有所帮助。一个ClickHouse集群包含许多分片。为了从所有分片查询数据，我们在一个名为`default`的数据库中拥有所谓的分布式表（由`Distributed`表引擎支持）。`Distributed`引擎查询`r0`数据库中的底层表。底层表是数据存储在ClickHouse集群每个分片上的位置。
对分布式表的查询通过一个共享系统账户运行。作为改进分布式查询安全性和可靠性工作的一部分，正在进行的工作是让它们在初始用户账户下运行。
在今天之前，当从ClickHouse系统表（如`system.tables`或`system.columns`）查询表元数据时，ClickHouse用户只能看到`default`数据库中的表。
由于用户已经隐式地可以访问`r0`中的底层表，我们在11:05进行了一项更改，使这种访问变得显式，以便用户也能看到这些表的元数据。通过确保所有分布式子查询都能在初始用户下运行，可以以更细粒度的方式评估查询限制和访问授权，避免一个用户的错误子查询影响他人。
上述更改导致所有用户都能访问他们有权访问的表的准确元数据。不幸的是，过去存在一个假设，即像下面这样的查询返回的列列表将只包含`default`数据库：
```
SELECT
name,
type
FROM system.columns
WHERE
table = 'http_requests_features'
order by name;
```
请注意，该查询没有按数据库名称进行过滤。随着我们逐步向给定ClickHouse集群的用户推出显式授权，在11:05的更改之后，上述查询开始返回列的"重复项"，因为这些列是针对存储在`r0`数据库中的底层表的。
不幸的是，这正是机器人管理功能文件生成逻辑为构建本节开头提到的文件中每个输入"特征"所执行的那类查询。
上述查询将返回一个列的表，如下所示（简化示例）：
然而，作为授予用户的额外权限的一部分，响应现在包含了`r0`模式的所有元数据，这实际上使响应中的行数增加了一倍以上，最终影响了最终文件输出中的行数（即特征数）。
在我们代理服务上运行的每个模块都有许多限制，以避免无限制的内存消耗，并作为性能优化预分配内存。在这个特定实例中，机器人管理系统对运行时可使用的机器学习特征数量有限制。目前该限制设置为200，远高于我们当前使用的约60个特征。再次强调，设置此限制是出于性能原因，我们为特征预分配了内存。
当包含超过200个特征的错误文件传播到我们的服务器时，触发了此限制——导致系统恐慌。进行此检查并成为未处理错误来源的FL2 Rust代码如下所示：
这导致了以下恐慌，进而导致了5xx错误：
```
thread fl2_worker_thread panicked: called Result::unwrap() on an Err value
```
事件期间的其他影响
依赖我们核心代理的其他系统在事件期间也受到了影响。这包括Workers KV和Cloudflare Access。团队在13:04减少了对这些系统的影响，当时对Workers KV进行了补丁以绕过核心代理。随后，所有依赖Workers KV的下游系统（如Access本身）观察到错误率降低。
Cloudflare仪表板也受到了影响，原因是内部使用了Workers KV，并且Cloudflare Turnstile作为我们登录流程的一部分被部署。
Turnstile受此次中断影响，导致没有活动仪表板会话的客户无法登录。如下图所示，这表现为两个时间段内的可用性降低：从11:30到13:10，以及14:40到15:30之间。
第一个时段，从11:30到13:10，是由于对Workers KV的影响，一些控制平面和仪表板功能依赖于它。这在13:10得到恢复，当时Workers KV绕过了核心代理系统。
仪表板的第二个影响时段发生在恢复特征配置数据之后。积压的登录尝试开始使仪表板不堪重负。这种积压，加上重试尝试，导致延迟升高，降低了仪表板的可用性。扩展控制平面并发性后，大约在15:30恢复了可用性。
现在我们的系统已重新上线并正常运行，我们已经开始着手研究未来如何加强它们以抵御此类故障。具体来说，我们正在：
*   以与处理用户生成输入相同的方式，强化对Cloudflare生成的配置文件的接收
*   为功能启用更多的全局紧急关闭开关
*   消除核心转储或其他错误报告耗尽系统资源的可能性
*   审查所有核心代理模块中错误条件的故障模式
今天是Cloudflare自2019年以来最严重的一次中断。我们曾有过导致仪表板不可用的中断。有些导致新功能在一段时间内不可用。但在过去6年多的时间里，我们没有发生过另一次导致大部分核心流量停止流经我们网络的中断。
像今天这样的中断是不可接受的。我们设计了具有高度弹性的系统，以确保流量始终持续流动。过去我们发生中断时，总是促使我们构建新的、更具弹性的系统。
我代表Cloudflare的整个团队，为我们今天给互联网造成的困扰表示歉意。

| 时间 (UTC) | 状态 | 描述 |
|---|
| 11:05 | 正常。 | 数据库访问控制变更部署。 |
| 11:28 | 影响开始。 |

| 部署到达客户环境，首次在客户HTTP流量中观察到错误。 |
11:32-13:05 | 团队调查了流量水平升高和Workers KV服务的错误。
| 最初的症状似乎是Workers KV响应率下降，导致对其他Cloudflare服务的下游影响。
尝试了流量调控和账户限制等缓解措施，以使Workers KV服务恢复正常运行水平。
首次自动化测试在11:31检测到问题，手动调查于11:32开始。事件电话会议在11:35创建。 |
13:05 | 实施了Workers KV和Cloudflare Access的旁路方案——影响减小。 | 在调查期间，我们对Workers KV和Cloudflare Access使用了内部系统旁路，使其回退到我们核心代理的先前版本。尽管该问题在我们代理的先前版本中也存在，但如下所述，其影响较小。 |
13:37 | 工作重点是将Bot Management配置文件回滚到最后一个已知的良好版本。 | 我们确信Bot Management配置文件是此次事件的触发因素。团队通过多个工作流研究修复服务的方法，最快的工作流是恢复该文件的先前版本。 |
14:24 | 停止创建和传播新的Bot Management配置文件。 | 我们确定Bot Management模块是500错误的来源，并且这是由错误的配置文件引起的。我们停止了新Bot Management配置文件的自动部署。 |
14:24 | 新文件测试完成。 | 我们观察到使用旧版本配置文件成功恢复，随后专注于加速全球范围内的修复。 |
14:30 | 主要影响已解决。受下游影响的服务开始观察到错误减少。 | 正确的Bot Management配置文件已在全球部署，大多数服务开始正常运行。 |
17:06 | 所有服务已解决。影响结束。 | 所有下游服务已重启，所有操作完全恢复。 |

> 本文由AI自动翻译，原文链接：[Cloudflare outage on November 18, 2025](https://blog.cloudflare.com/18-november-2025-outage/)
> 
> 翻译时间：2026-01-06 01:16
