---
title: 差分Transformer V2：提升推理效率与训练稳定性的改进架构
title_original: Differential Transformer V2
date: '2026-01-20'
source: Hugging Face Blog
source_url: https://huggingface.co/blog/microsoft/diff-attn-v2
author: ''
summary: 本文介绍了差分Transformer V2，这是对原始差分Transformer架构的改进版本，主要聚焦于提升推理效率、增强大规模语言模型（LLM）的训练稳定性以及简化架构设计。关键改进包括：通过引入额外的Q2参数实现无需定制内核即可匹配基线Transformer的解码速度，并直接兼容FlashAttention；移除了可能导致训练后期不稳定的逐头RMSNorm；采用更简单的参数化与初始化方法。初步实验表明，该架构在万亿级别Token的预训练中，相比基线Transformer能显著降低语言建模损失，减少训练过程中的损失与梯度尖峰，并减小激活异常值幅度。
categories:
- AI研究
tags:
- Transformer架构
- 大语言模型
- 推理优化
- 训练稳定性
- 注意力机制
draft: false
translated_at: '2026-01-20T04:42:47.383484'
---

# Differential Transformer V2

- 
- 

![Microsoft's avatar](/images/posts/53e58d9cf63d.png)

摘要代码动机更快的解码与无需定制内核Softmax幅度约束超越Softmax约束与注意力汇的消除实验观察讨论差分运算的构建设计消融实验其他叶添竹，董力，孙雨韬，韦福如

- 摘要
- 代码
- 动机更快的解码与无需定制内核Softmax幅度约束超越Softmax约束与注意力汇的消除
- 实验观察
- 讨论差分运算的构建设计消融实验其他

- 更快的解码与无需定制内核
- Softmax幅度约束
- 超越Softmax约束与注意力汇的消除

- 差分运算的构建
- 设计消融实验
- 其他

我们介绍**Differential Transformer V2**，这是**Differential Transformer**的改进版本。此修订版侧重于推理效率、生产级LLM的训练稳定性以及架构的优雅性。

1.  **更快的推理与无需定制注意力内核**
    我们不再强制要求注意力参数数量与基线**Transformer**匹配，而是为 $Q_2$ 引入了额外的参数。这一设计使得 DIFF V2 能够匹配基线 Transformer 的解码速度，并直接使用**FlashAttention**而无需定制内核。
2.  **改进的训练稳定性**
    我们移除了差分注意力后的逐头 RMSNorm。我们发现，在LLM大规模预训练后期，逐头 RMSNorm 可能导致不稳定性。
3.  **更简单的参数化与初始化**
    我们用特定于Token、逐头投影的 $\lambda$ 替换了全局共享的 $\lambda$。这消除了 $\lambda$ 的指数重参数化和初始化。

我们在生产级LLM上进行了预训练实验，包括密集模型和一个30A3的MoE模型，使用了万亿级别的Token和6e-4到1e-3的大学习率。实验观察如下：

-   与 Transformer 相比，**语言建模损失显著降低**。
-   训练过程中损失和梯度尖峰减少，尤其是在大学习率设置下，此时基线 Transformer 变得不稳定。
-   激活异常值的幅度减小。

实验仍在进行中。我们期望在训练的后期阶段探索：

-   在训练中期和后期，学习效率是否得到改善。
-   在下游长上下文基准测试中的性能是否有所提升。

实验完成并评估结果后，我们将准备一份更正式的报告。

我们在下方比较 DIFF V2 与 DIFF V1：

```
def DiffAttnV1(
        layer_index, q1, q2, k1, k2, v,
        lam_q1, lam_k1, lam_q2, lam_k2,
):
        """
      q1, q2: (N, h/2, d)
      k1, k2: (N, h_kv/2, d)
      v:      (N, h_kv/2, 2d)
      lam_*: (d,)
      """
      attn1 = flash_attn_func(q1, k1, v)
        attn2 = flash_attn_func(q2, k2, v)
        
        lam_init = 0.8 - 0.6 * \
            exp(-0.3 * layer_index)
        lam1 = exp(sum(lam_q1 * lam_k1)
    lam2 = exp(sum(lam_q2 * lam_k2)
    lam = lam1 - lam2 + lam_init
    attn = attn1 - lam * attn2
    
    attn = rmsnorm(attn)
    attn = attn * (1 - lam_init)
    return attn

```

```
def DiffAttnV2(
        q, k, v, lam
):
        """
      q:   (N, 2h, d)
      k:   (N, h_kv, d)
      v:   (N, h_kv, d)
      lam: (N, h, 1)
      """
        
        attn = flash_attn_func(q, k, v)
        attn1, attn2 = (attn[:, 0::2], 
                        attn[:, 1::2])
        
        lam_val = sigmoid(lam)
        attn = attn1 - lam_val * attn2
    return attn
    
    
    
    
    
    

```

完整代码位于：[unilm/Diff-Transformer/Diff-Transformer-V2 at master · microsoft/unilm](https://github.com/microsoft/unilm/tree/master/Diff-Transformer/Diff-Transformer-V2)
在脚本中，`h` 代表查询头的数量，`h_kv` 代表键值头的数量，`d` 代表头维度。DIFF V2 中的 $\lambda$ 是为每个Token的每个头从 $X$ 投影得到的。

DIFF V2 将查询头的数量加倍，同时保持键值头的数量不变，差分运算后额外的维度被缩减回 `h*d`，因此 $W_O$ 投影与基线 Transformer 保持一致。

### 更快的解码与无需定制内核

与基线 Transformer 相比，DIFF V2 引入了额外的查询头，但**没有增加键值头的数量**。由于LLM解码通常是内存受限的，这一设计使得 DIFF V2 能够达到与标准 Transformer 相当的解码速度。此外，由于查询、键和值的头维度是对齐的，DIFF V2 无需定制注意力内核。相比之下，DIFF V1 在解码时可能更慢，因为值缓存必须加载两次，并且需要定制注意力内核。DIFF V2 还可以在解码时增加注意力模块的算术强度。

在预训练期间，在 H 系列和 B 系列 GPU 上使用前沿的 FlashAttention 内核时，DIFF V2 引入的吞吐量降低可以忽略不计。对于长序列预填充，我们建议将 DIFF V2 与诸如 **YOCO** 等技术结合使用，后者已经将预填充复杂度降低到相对于序列长度的线性时间。

另一种视角是将 DIFF V2 与具有相同查询维度 `2h*d` 的 Transformer 进行比较。在此比较下，两种模型展现出相同的注意力内核速度，而 DIFF V2 在输出投影中拥有更少的参数和浮点运算次数。

### Softmax幅度约束

在标准的缩放点积注意力中，令 $Q, K, V \in \mathbb{R}^{n \times d}$ 为查询、键和值。上下文向量 $C$ 定义为：

C=Softmax(QKTd)V=AVC = \text{Softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V = AVC=Softmax(d​QKT​)V=AV

其中 $A \in \mathbb{R}^{n \times n}$ 是注意力权重矩阵。让我们关注 $C$ 的单个行向量 $\mathbf{c}_i$，它是值向量 $\mathbf{v}_j$ 的加权和：

ci=∑j=1naijvj\mathbf{c}_i = \sum_{j=1}^{n} a_{ij} \mathbf{v}_jci​=j=1∑n​aij​vj​

我们定义**上下文RMS**来表示该输出的幅度：

RMS(ci)=1d∥ci∥2\text{RMS}(\mathbf{c}_i) = \sqrt{\frac{1}{d} \|\mathbf{c}_i\|^2}RMS(ci​)=d1​∥ci​∥2​

权重 $a_{ij}$ 非负且和为 1。假设值向量 $\mathbf{v}_j$ 不相关且 RMS 为 1，则上下文 RMS 严格限定在 $[\frac{1}{\sqrt{n}},1)$ 范围内，具体取决于注意力分布的变化：

-   如果注意力完全集中在一个Token上，上下文 RMS 为 $1$。
-   如果注意力在所有Token上均匀分布，上下文 RMS 降至 $\frac{1}{\sqrt{n}}$。
-   在其他情况下，上下文 RMS 介于 $\frac{1}{\sqrt{n}}$ 和 $1$ 之间。

在 DIFF V1 中，我们在上下文向量上添加了逐头 RMSNorm：

c^i=ciRMS(ci)\mathbf{\hat{c}}_i = \frac{\mathbf{c}_i}{\text{RMS}(\mathbf{c}_i)}c^i​=RMS(ci​)ci​​

如果模型在一个头中学习到均匀的注意力分布，上下文 RMS 约为 $1/\sqrt{n}$。为了将其归一化回 $1$，RMSNorm 必须将向量乘以 $\sqrt{n}$ 的缩放因子。对于 $n = 8192$，$\sqrt{n} \approx 90.5$。这意味着 RMSNorm 层对输出应用了约**100倍**的放大。在大规模预训练中，我们发现这会导致巨大的梯度和数值不稳定性。

一个典型现象是，当DIFF V1以较大的学习率进行预训练时，其梯度范数在后期阶段相比Transformer会出现更大幅度的增长，同时方差也更高。在DIFF V2中，移除了逐头RMSNorm后，梯度范数的尺度变得与Transformer相当，并且梯度范数的尖峰也减少了（下文将进一步讨论）。

我们在DIFF V1中采用逐头RMSNorm设计，主要是因为其值头维度翻倍，并且所有Token共享一个全局的$\lambda$。鉴于DIFF V2在这两个方面都进行了修改，我们发现移除RMSNorm现在是安全的。

### 超越Softmax约束与消除注意力沉没

我们证明DIFF V2能够克服上述Softmax的约束。它也有助于消除注意力沉没。

- 在原始的Softmax注意力中：

aij=Softmax(zij)=exp⁡(zij)∑k=1nexp⁡(zik)ci=∑j=1naijvj=∑j=1nSoftmax(zij)vjRMS(ci)∈[1n,1)a_{ij} = \text{Softmax}(z_{ij}) =  \frac{\exp(z_{ij})}{\sum_{k=1}^{n} \exp(z_{ik})} \\

\mathbf{c}_i = \sum_{j=1}^{n} a_{ij} \mathbf{v}_j = \sum_{j=1}^{n} \text{Softmax}(z_{ij}) \mathbf{v}_j \\

\text{RMS}(\mathbf{c}_i) \in \left[\frac{1}{\sqrt{n}},1\right)aij​=Softmax(zij​)=∑k=1n​exp(zik​)exp(zij​)​ci​=j=1∑n​aij​vj​=j=1∑n​Softmax(zij​)vj​RMS(ci​)∈[n​1​,1)

- 在DIFF V2中，我们为每个Token和每个头引入了一个投影的$\lambda$：

ci=∑j=1n(Softmax(zij1)−sigmoid(λi)⋅Softmax(zij2))vjRMS(ci)∈(0,2)\mathbf{c}_i = \sum_{j=1}^{n}  \left( \text{Softmax}(z_{ij}^\text{1}) - \text{sigmoid}(\lambda_i) \cdot \text{Softmax}(z_{ij}^\text{2}) \right) \mathbf{v}_j \\

\text{RMS}(\mathbf{c}_i) \in \left(0, \sqrt{2}\right)ci​=j=1∑n​(Softmax(zij1​)−sigmoid(λi​)⋅Softmax(zij2​))vj​RMS(ci​)∈(0,2​)

投影的$\lambda_i$有助于控制上下文RMS。我们观察到，将上下文RMS的下限降低到零尤为重要。这有助于消除注意力沉没并提高训练稳定性。上限只需要保持有界即可。

请注意，我们此处的分析考虑的是输出投影$W_O$之前的RMS。尽管RMS可以在输出投影后恢复和调整，但Softmax处缺乏自由度仍然会影响学习性能。

其他近期工作也缓解了这一约束：

- 在Attention Is Off By One中：

aijoff=exp⁡(zij)1+∑k=1nexp⁡(zik)ci=∑j=1naijoffvj=∑k=1nexp⁡(zik)1+∑k=1nexp⁡(zik)∑j=1nSoftmax(zij)vjRMS(ci)∈(0,1)a_{ij}^{\text{off}} = \frac{\exp(z_{ij})}{1 + \sum_{k=1}^{n} \exp(z_{ik})} \\
\ \\
\mathbf{c}_i = \sum_{j=1}^{n} a_{ij}^{\text{off}} \mathbf{v}_j = \frac{\sum_{k=1}^{n} \exp(z_{ik})}{1 + \sum_{k=1}^{n} \exp(z_{ik})} \sum_{j=1}^{n}  \text{Softmax}(z_{ij}) \mathbf{v}_j \\
\ \\
\text{RMS}(\mathbf{c}_i) \in \left(0, 1\right)aijoff​=1+∑k=1n​exp(zik​)exp(zij​)​ci​=j=1∑n​aijoff​vj​=1+∑k=1n​exp(zik​)∑k=1n​exp(zik​)​j=1∑n​Softmax(zij​)vj​RMS(ci​)∈(0,1)

- 在gpt-oss中，为每个头引入了一个可学习的标量$s$：

aijoss=exp⁡(zij)exp⁡(s)+∑k=1nexp⁡(zik)ci=∑j=1naijossvj=∑k=1nexp⁡(zik)exp⁡(s)+∑k=1nexp⁡(zik)∑j=1nSoftmax(zij)vjRMS(ci)∈(0,1)a_{ij}^{\text{oss}} = \frac{\exp(z_{ij})}{\exp(s) + \sum_{k=1}^{n} \exp(z_{ik})} \\
\ \\
\mathbf{c}_i = \sum_{j=1}^{n} a_{ij}^{\text{oss}} \mathbf{v}_j = \frac{\sum_{k=1}^{n} \exp(z_{ik})}{\exp(s) + \sum_{k=1}^{n} \exp(z_{ik})} \sum_{j=1}^{n}  \text{Softmax}(z_{ij}) \mathbf{v}_j \\
\ \\
\text{RMS}(\mathbf{c}_i) \in \left(0, 1\right)aijoss​=exp(s)+∑k=1n​exp(zik​)exp(zij​)​ci​=j=1∑n​aijoss​vj​=exp(s)+∑k=1n​exp(zik​)∑k=1n​exp(zik​)​j=1∑n​Softmax(zij​)vj​RMS(ci​)∈(0,1)

- 在Gated Attention中，乘以了一个投影的逐元素sigmoid门：

ci=sigmoid(gi)⊙∑j=1nSoftmax(zij)vjRMS(ci)∈(0,1)\mathbf{c}_i = \text{sigmoid} (\mathbf{g}_i) \odot \sum_{j=1}^{n}  \text{Softmax}(z_{ij}) \mathbf{v}_j \\

\text{RMS}(\mathbf{c}_i) \in \left(0, 1\right)ci​=sigmoid(gi​)⊙j=1∑n​Softmax(zij​)vj​RMS(ci​)∈(0,1)

## 实验观察

我们在生产级规模的LLM上进行了预训练实验，包括密集模型和一个30A3的MoE模型，使用了万亿级别的Token和6e-4到1e-3的大学习率。

实验仍在进行中。我们目前观察到：

- 与Transformer相比，语言建模损失显著降低（差距在0.02到0.03之间）。
- 训练过程中损失和梯度尖峰减少，尤其是在大学习率设置下，Transformer基线变得不稳定时。
- 激活异常值的幅度减小。

我们期望在训练的后期阶段探索：
- 训练中期和后期的学习效率。
- 在下游长上下文基准测试上的性能（缓解上下文退化）。

### 差分操作的构建

理论上，一个具有$2h$个注意力头的标准Transformer可以通过学习$W_O^{2i}=-W_O^{2i+1}, i=0,1,\ldots,h-1$来学习差分操作，其中$W_O^{i}$表示头$i$的输出投影，且头$2i$和$2i+1$属于同一个GQA组。

假设1。在实践中，这样的解决方案很难通过优化学习到，因为它要求两组参数收敛到彼此的精确负值。

假设2。模型可以学习差分操作，并且模型选择在训练中学习它。那么，像DIFF V2那样在输出投影之前显式构建它，可以节省一半的$W_O$参数。节省的参数数量也相当可观。在当前GQA设置下，注意力模块中的参数主要由$W_Q$和$W_O$主导；因此，大约可以节省**25%的注意力模块参数**。节省下来的参数预算可以重新分配到模型的其他部分。

即使DIFF V2在重新分配参数后，没有实现比基线更低的损失而只是与之持平，如果它能提供额外的益处，例如提高训练稳定性、更好地控制异常值或更高的训练效率，那么该方法仍然是值得的。这类似于GQA，它在匹配MHA损失的同时，以减少KV缓存作为额外益处。因此，关键问题变成了实证性能。

1.  减去两个**不**在同一GQA组中的头，这意味着它们**不**共享相同的键和值。

```
# Ablation 1
# ❌ DIFF V2的错误实现！
...
attn = flash_attn_func(q, k, v)
nh = attn.size(1)
attn1, attn2 = (attn[:, :nh//2], 
                    attn[:, nh//2:])
...

```

```
# DIFF V2
# ✅ DIFF V2的正确实现
...
attn = flash_attn_func(q, k, v)

attn1, attn2 = (attn[:, 0::2], 
                    attn[:, 1::2])
...

```

在我们的大学习率设置中，与DIFF V2相比，消融实验1的设置表现出明显的训练不稳定性（损失和梯度尖峰更多）和更高的损失。如DIFF V1论文所述，两个相减的头应该共享值以构建差分操作。

1.  减去两个注意力图时不使用$\lambda$缩放因子，即使用`attn1 - attn2`而不是`attn1 - lam_val * attn2`。这导致初始化时上下文RMS过小。
2.  直接使用投影的$\lambda$而不应用`sigmoid`操作。上下文RMS的上界无界。

消融实验2和消融实验3都导致比DIFF V2更高的语言建模损失。消融实验2保持了与DIFF V2相似的训练稳定性，而消融实验3则稳定性较差（但仍比消融实验1稳定）。

- 在DIFF中，qk对数中的异常值可能小于基线模型中的异常值。这一点已在DIFF V1中分析过：DIFF能够在使用较小qk对数的同时，实现与基线相当的注意力稀疏性。我们进一步提出，DIFF的差分机制通过抵消较小的注意力值，可能有助于缓解这篇博客和论文中讨论的注意力舍入误差问题。
- DIFF V2兼容稀疏注意力。在许多现有的稀疏注意力框架中，为最大化加速效果，同一GQA组内的查询头需要关注相同的键值块。常见策略是基于各头部的平均注意力对数来选择键值块。
对于DIFF V2，问题转变为如何为包含成对差分头部的更大GQA组设计有效的块选择策略。这可能需要在选择过程中分别处理两类差分头部，或者在实际应用中简单的注意力对数平均可能已足够。从概念上讲，这与标准Transformer的块稀疏注意力相比并未引入根本性差异。

·注册或登录以发表评论

- 
-

---

> 本文由AI自动翻译，原文链接：[Differential Transformer V2](https://huggingface.co/blog/microsoft/diff-attn-v2)
> 
> 翻译时间：2026-01-20 04:42
