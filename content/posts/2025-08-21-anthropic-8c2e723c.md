---
title: 公私合作构建AI核安全屏障：Anthropic与NNSA开发风险监控工具
title_original: Developing nuclear safeguards for AI through public-private partnership
date: '2025-08-21'
source: Anthropic
source_url: https://www.anthropic.com/news/developing-nuclear-safeguards-for-ai-through-public-private-partnership
author: ''
summary: 本文介绍了Anthropic与美国能源部国家核安全管理局（NNSA）合作，共同应对AI模型可能被滥用于核扩散的风险。双方已从风险评估阶段进入工具开发阶段，联合研发出一款能自动识别敏感核话题对话的分类器，准确率达96%。该工具已部署于Claude的流量监控中，旨在构建综合的模型滥用识别体系。文章强调这种公私合作模式融合了产业与政府的优势，为AI开发者提供了可借鉴的核安全保障蓝图，并计划通过前沿模型论坛分享相关方法。
categories:
- 政策监管
tags:
- AI安全
- 核不扩散
- 公私合作
- 风险监控
- Anthropic
draft: false
translated_at: '2026-02-12T04:30:28.253092'
---

# 通过公私合作建立人工智能核安全保障机制

核技术本质上是双刃剑：驱动核反应堆的物理原理同样可能被滥用于武器开发。随着AI模型能力不断增强，我们需要密切关注它们是否会以威胁国家安全的方式向用户提供危险技术知识。

核武器相关信息尤为敏感，这使得私营企业独立评估相关风险面临挑战。正因如此，去年四月我们与美国能源部（DOE）下属国家核安全管理局（NNSA）建立合作，共同评估我们模型的核扩散风险，并持续开展相关评估工作。

如今，我们正超越风险评估阶段，着手构建风险监控工具。通过与NNSA及DOE国家实验室的协作，我们联合开发出一款分类器——一种能自动识别内容类别的AI系统。在初步测试中，该系统能以96%的准确率区分涉及敏感核话题与普通核话题的对话。

作为我们识别模型滥用综合体系的一部分，该分类器已部署于Claude的流量监控中。早期部署数据显示，分类器在实际Claude对话场景中表现良好。

我们将向前沿模型论坛（Frontier Model Forum）——前沿AI公司的行业组织——分享我们的方法，希望这种合作模式能为所有AI开发者提供可借鉴的蓝图，使其能与NNSA合作实施类似保障措施。

除了保障前沿AI模型免遭核技术滥用的现实意义，这项开创性工作还彰显了公私合作的力量。这种合作模式融合了产业界与政府的互补优势，直面风险挑战，使AI模型能为所有用户提供更可靠、更值得信赖的服务。

关于我们与NNSA合作及保障机制开发的完整细节，请访问我们的研究博客red.anthropic.com。该平台主要发布Anthropic前沿红队（及公司其他团队）关于前沿AI模型对国家安全影响的研究成果。点击此处阅读更多内容。

## 相关内容

### 应对数据中心电价上涨问题

### 推出Claude Opus 4.6

我们正在升级最智能的模型。在智能体编码、计算机使用、工具调用、搜索和金融领域，Opus 4.6已成为行业领先模型，且往往优势显著。

### Claude是思考的空间

我们做出选择：Claude将保持无广告模式。我们将解释广告激励机制为何与真正有用的AI助手不相容，以及我们计划如何在保障用户信任的前提下扩大服务覆盖。

---

> 本文由AI自动翻译，原文链接：[Developing nuclear safeguards for AI through public-private partnership](https://www.anthropic.com/news/developing-nuclear-safeguards-for-ai-through-public-private-partnership)
> 
> 翻译时间：2026-02-12 04:30
