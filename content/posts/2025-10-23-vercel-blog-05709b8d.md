---
title: Vercel AI Cloud 推出零配置后端部署
title_original: Zero-config backends on Vercel AI Cloud - Vercel
date: '2025-10-23'
source: Vercel Blog
source_url: https://vercel.com/blog/zero-config-backends-on-vercel-ai-cloud
author: ''
summary: Vercel AI Cloud 宣布推出零配置后端部署功能，支持主流 Python 和 TypeScript 框架。该功能通过“框架定义基础设施”自动识别并配置所需资源，使开发者无需管理配置文件或
  Docker 镜像即可部署后端。后端在现代 AI 应用中扮演着关键角色，负责数据处理、工作流编排和智能体驱动等复杂任务。Vercel 的更新旨在简化后端开发流程，提供与前端一致的高效体验，并集成了可观测性工具与资源自动优化。
categories:
- AI基础设施
tags:
- Vercel
- 后端部署
- AI开发平台
- 无服务器架构
- 零配置
draft: false
translated_at: '2026-02-10T04:36:54.344466'
---

您可以直接部署后端

您在 Vercel 所期望的易用性，现已延伸至您的后端。

自从我们在 Vercel Ship 大会上推出 AI Cloud 以来，团队一直在构建超越简单“提示词-响应”模式的 AI 应用程序。这些应用编排多步骤工作流、生成子智能体、并运行耗时数小时甚至数天的流程。它们需要能够处理数据、运行推理并响应实时事件的后端。

您现在可以零配置地部署最流行的 Python 和 TypeScript 后端框架。Vercel 会识别您的框架，并自动配置运行所需的基础设施。

## AI 应用为何需要强大的后端

后端负责处理和移动数据、运行推理以及响应实时事件。它们处理从数据的前后处理到管理上传、压缩、缓存和日志等一切事务。此外，它们也越来越多地为那些能够规划、推理并随时间推移而行动的智能体提供动力。

一年前，大多数 AI 应用只是向模型发送一个提示词并返回响应。如今的后端则作为编排层运行，它们生成多个子智能体，或执行跨越数分钟、数小时乃至数天的工作流。这种日益增长的复杂性使得可靠性和迭代速度变得比以往任何时候都更加重要。

在过去的四个月里，我们推出了一系列更新，旨在使 Vercel 成为这些后端和后台智能体的一流平台。

### 面向后端的框架定义基础设施

框架定义基础设施意味着 Vercel 能够读取您的后端框架并理解您的意图。当您部署 Next.js 时，平台会自动知道如何构建、路由和优化您的应用。同样的模式现已延伸至后端。

![Vercel automatically maps your framework to the right infrastructure.](/images/posts/f8feb27c6c66.jpg)

![Vercel automatically maps your framework to the right infrastructure.](/images/posts/7d21f10cb534.jpg)

![Vercel automatically maps your framework to the right infrastructure.](/images/posts/a9fcdbca5446.jpg)

![Vercel automatically maps your framework to the right infrastructure.](/images/posts/0c188fe1082e.jpg)

Vercel 支持诸如 Python 的 FastAPI 和 Flask，或 TypeScript 的 Express、Hono、NestJS 和 Nitro 等框架。每个框架都带有描述路由、并发和执行的约定。Vercel 利用这些上下文信息来推断您的代码应如何扩展、请求应如何路由以及应使用何种计算模型。您的框架定义了基础设施。

您编写后端代码，Vercel 负责确定其运行方式。无需配置文件、Docker 镜像或自定义设置。

这意味着：

*   您可以零配置部署任何受支持的后端框架
*   您的后端会自动按端点、按函数、按请求进行扩展
*   您的可观测性数据（包括日志、追踪和指标）与前端位于同一个仪表板中
*   平台持续优化计算资源分配，以匹配您工作负载的行为

您可以零配置部署任何受支持的后端框架

您的后端会自动按端点、按函数、按请求进行扩展

您的可观测性数据（包括日志、追踪和指标）与前端位于同一个仪表板中

平台持续优化计算资源分配，以匹配您工作负载的行为

FDI 消除了编写后端代码与在生产环境中运行它之间的鸿沟。开发者可以完全专注于应用逻辑，而 Vercel 则确保其在大规模运行时的高效与可靠。

## 更优的后端体验

我们改进了在 Vercel 上构建、测试和提供后端服务的体验。这些更新使得后端开发与前端开发一样符合人体工程学且快速。

### 流行框架的零配置支持

Vercel 现在支持越来越多的 Python 和 Node.js 框架库，包括 Express、Hono、FastAPI 和 Flask。无需一行配置即可部署它们。平台会在构建时自动检测并优化您的代码。

除了零配置、框架定义的体验外，我们还在引入原生的 Vercel Python SDK。

现在可通过 `pip install vercel` 进行 Beta 版体验，该 SDK 提供了自然的 API 抽象来访问 Vercel 产品和服务，包括用于运行不受信任代码的沙盒、用于跨函数存储和检索数据的运行时缓存，以及用于频繁读取资源的 Blob 存储。

构建一个 Python vibe 编码 IDE

使用此模板，结合 Next.js、FastAPI、OpenAI Agents SDK 和 Vercel AI Cloud，构建一个浏览器内、AI 原生的 IDE。

立即部署

### 活跃 CPU 计费 与 流体计算

对于间歇性运行的工作负载，例如需要暂停等待输入或等待外部 API 的智能体，流体计算确保您只需为代码实际运行的时间付费。活跃 CPU 计费精确测量执行时间，而非挂钟时间。空闲等待不产生费用。

![Fluid compute bills by execution time, not wall time.](/images/posts/0373a5e2264f.jpg)

![Fluid compute bills by execution time, not wall time.](/images/posts/132c948d3b78.jpg)

![Fluid compute bills by execution time, not wall time.](/images/posts/2059f9ad536f.jpg)

![Fluid compute bills by execution time, not wall time.](/images/posts/69396d5fdc54.jpg)

流体计算上的 AI 工作负载针对按需可扩展性进行了优化，提高了效率，其定价模型也降低了长时间运行 LLM 调用的成本。

了解流体计算在 Vercel 上的工作原理

流体计算通过在 LLM 调用期间复用空闲计算资源来提升无服务器性能，保持函数活跃并动态扩展。

了解更多

### 面向长时间运行后端的持久化编排

对于需要持久性或长期可靠性的后端，工作流开发套件将持久性和可观测性融入您的代码中。它适用于编排多步骤流程，如 AI 智能体循环、数据管道或计划自动化任务。

![Vercel automatically detects when a function is durable and dynamically provisions the ideal infrastructure to support it in real time.](/images/posts/5db6f97bbc62.jpg)

![Vercel automatically detects when a function is durable and dynamically provisions the ideal infrastructure to support it in real time.](/images/posts/e2dd241309ae.jpg)

![Vercel automatically detects when a function is durable and dynamically provisions the ideal infrastructure to support it in real time.](/images/posts/736f72608cf7.jpg)

![Vercel automatically detects when a function is durable and dynamically provisions the ideal infrastructure to support it in real time.](/images/posts/95edabeea163.jpg)

### 从新的后端 AI 模板开始

我们正在发布一个后端模板库，展示可用于生产环境的 AI 模式，包括：

*   具有多轮记忆和流式响应的聊天机器人系统
*   能够跨代码库推理并建议更改的编程助手
*   大规模处理数据摄取、嵌入/向量和检索的 RAG（检索增强生成）管道

具有多轮记忆和流式响应的聊天机器人系统

能够跨代码库推理并建议更改的编程助手

大规模处理数据摄取、嵌入/向量和检索的 RAG（检索增强生成）管道

每个模板都展示了 AI Cloud 从编排到推理的完整能力。

## 全栈一体化平台

在 Vercel 上部署的每个后端，都运行在为 AI Cloud 提供动力的同一套编排、计算和存储层之上。这些系统共同为快速、可靠且经济高效的后端开发奠定了基础。

从实时 API 到长时间运行的后台智能体，您可以在 Vercel 上构建、测试和部署应用的每一个部分。

---

> 本文由AI自动翻译，原文链接：[Zero-config backends on Vercel AI Cloud - Vercel](https://vercel.com/blog/zero-config-backends-on-vercel-ai-cloud)
> 
> 翻译时间：2026-02-10 04:36
