---
title: 三步在英特尔CPU上本地运行视觉语言模型
title_original: Get your VLM running in 3 simple steps on Intel CPUs
date: '2025-10-15'
source: Hugging Face Blog
source_url: https://huggingface.co/blog/openvino-vlm
author: ''
summary: 本文介绍了如何在资源有限的英特尔CPU设备上，通过Optimum Intel和OpenVINO工具，仅用三个步骤本地部署和优化视觉语言模型（VLM）。首先将模型转换为OpenVINO中间表示格式，然后通过量化技术（仅权重量化或静态量化）压缩模型，以降低内存占用并提升推理速度，从而实现无需GPU或昂贵硬件的本地AI运行，保障数据隐私与可靠性。
categories:
- AI基础设施
tags:
- 视觉语言模型
- 模型优化
- 英特尔CPU
- 本地部署
- 量化
draft: false
translated_at: '2026-01-27T04:38:39.248103'
---

# 在英特尔CPU上通过3个简单步骤运行您的VLM

随着大语言模型（LLM）能力的不断增强，一类新的模型已经出现：视觉语言模型（VLM）。这些模型可以分析图像和视频，以描述场景、创建字幕并回答关于视觉内容的问题。

虽然在自己的设备上运行AI模型可能很困难，因为这些模型通常对计算资源要求很高，但它也带来了显著的好处：包括提高隐私性（因为您的数据保留在本地机器上），以及增强速度和可靠性（因为您不依赖于互联网连接或外部服务器）。这正是像 **Optimum Intel** 和 **OpenVINO** 这样的工具，以及像 **SmolVLM** 这样小巧高效的模型发挥作用的地方。在这篇博客文章中，我们将引导您完成三个简单的步骤，在本地运行一个VLM，无需昂贵的硬件或GPU（不过，您也可以在英特尔GPU上运行本文中的所有代码示例）。

## 使用 Optimum 部署您的模型

像 SmolVLM 这样的小型模型专为低资源消耗而设计，但它们可以进一步优化。在这篇博客文章中，我们将看到如何优化您的模型，以降低内存使用并加速推理，使其在资源有限的设备上部署更加高效。

要学习本教程，您需要安装 `optimum` 和 `openvino`，您可以通过以下命令完成：

```bash
pip install optimum-intel[openvino] transformers==4.52.*
```

## 步骤 1：转换您的模型

首先，您需要将您的模型转换为 **OpenVINO IR**。有多种方法可以实现：

1. 您可以使用 **Optimum CLI**

```bash
optimum-cli export openvino -m HuggingFaceTB/SmolVLM2-256M-Video-Instruct smolvlm_ov/
```

2. 或者，您可以在加载模型时**即时**转换它：

```python
from optimum.intel import OVModelForVisualCausalLM

model_id = "HuggingFaceTB/SmolVLM2-256M-Video-Instruct"
model = OVModelForVisualCausalLM.from_pretrained(model_id)
model.save_pretrained("smolvlm_ov")
```

## 步骤 2：量化

现在是时候优化您的模型了。量化降低了模型权重和/或激活值的精度，从而得到更小、更快的模型。本质上，这是一种将值从高精度数据类型（如32位浮点数 FP32）映射到低精度格式（通常是8位整数 INT8）的方法。虽然这个过程提供了几个关键好处，但也可能导致潜在的精度损失。

![量化](/images/posts/7d632e0282e1.png)

Optimum 支持两种主要的训练后量化方法：

- 仅权重量化
- 静态量化

让我们分别探讨它们。

### 选项 1：仅权重量化

仅权重量化意味着只量化权重，而激活值保持其原始精度。因此，模型变得更小、内存效率更高，加载时间得到改善。但由于激活值未被量化，推理速度的提升有限。仅权重量化是一个简单的第一步，因为它通常不会导致显著的精度下降。

自 OpenVINO 2024.3 起，如果模型的权重已被量化，相应的激活值也将在运行时被量化，从而根据设备带来额外的加速。

为了运行它，您需要创建一个量化配置 `OVWeightQuantizationConfig`，如下所示：

```python
from optimum.intel import OVModelForVisualCausalLM, OVWeightQuantizationConfig

q_config = OVWeightQuantizationConfig(bits=8)
q_model = OVModelForVisualCausalLM.from_pretrained(model_id, quantization_config=q_config)
q_model.save_pretrained("smolvlm_int8")
```

或者等效地使用 CLI：

```bash
optimum-cli export openvino -m HuggingFaceTB/SmolVLM2-256M-Video-Instruct --weight-format int8 smolvlm_int8/
```

## 选项 2：静态量化

使用静态量化，权重和激活值都在推理前被量化。为了获得激活值量化参数的最佳估计，我们执行一个校准步骤。在此步骤中，一个小的代表性数据集会被输入模型。在我们的案例中，我们将使用 `contextual` 数据集的 50 个样本，并对视觉编码器应用静态量化，而模型的其余部分将应用仅权重量化。实验表明，对视觉编码器应用静态量化提供了明显的性能提升，而不会显著降低精度。由于视觉编码器在每次生成中只被调用一次，因此优化此组件所带来的整体性能增益低于优化更频繁使用的组件（如语言模型）所获得的增益。尽管如此，这种方法在某些场景下可能是有益的。例如，当需要简短答案，特别是输入包含多张图像时。

```python
from optimum.intel import OVModelForVisualCausalLM, OVPipelineQuantizationConfig, OVQuantizationConfig, OVWeightQuantizationConfig

q_config = OVPipelineQuantizationConfig(
    quantization_configs={
        "lm_model": OVWeightQuantizationConfig(bits=8),
        "text_embeddings_model": OVWeightQuantizationConfig(bits=8),
        "vision_embeddings_model": OVQuantizationConfig(bits=8),
    },
    dataset=dataset,
    num_samples=num_samples,
)
q_model = OVModelForVisualCausalLM.from_pretrained(model_id, quantization_config=q_config)
q_model.save_pretrained("smolvlm_static_int8")
```

量化激活值会引入微小误差，这些误差可能会累积并影响精度，因此事后的仔细测试非常重要。更多信息和示例可以在**我们的文档**中找到。

### 步骤 3：运行推理

您现在可以使用量化后的模型运行推理：

```python
generated_ids = q_model.generate(**inputs, max_new_tokens=100)
generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)
print(generated_texts[0])
```

如果您有最新的英特尔笔记本电脑、英特尔 AI PC 或英特尔独立 GPU，您可以通过在加载模型时添加 `device="gpu"` 将模型加载到 GPU 上：

```python
model = OVModelForVisualCausalLM.from_pretrained(model_id, device="gpu")
```

我们还**创建了一个 Space**，以便您可以试用**原始模型**及其通过分别应用**仅权重量化**和**混合量化**获得的量化变体。此演示运行在第四代英特尔至强（Sapphire Rapids）处理器上。

![HF Space](/images/posts/20db0034cc8b.png)

要复现我们的结果，请查看我们的**笔记本**。

## 评估与结论

我们运行了一个基准测试，以比较原始模型的 **PyTorch**、**OpenVINO** 和 **OpenVINO 8位 WOQ** 版本的性能。目的是评估仅权重量化对英特尔 CPU 硬件上延迟和吞吐量的影响。对于此测试，我们使用了**单张图像**作为输入。

我们测量了以下指标来评估模型的性能：

- **首 Token 时间**：生成第一个输出 Token 所需的时间。
- **每输出 Token 时间**：生成每个后续输出 Token 所需的时间。
- **端到端延迟**：生成所有输出 Token 所需的总时间。
- **解码吞吐量**：模型在解码阶段每秒生成的 Token 数量。

以下是英特尔 CPU 上的结果：

此基准测试展示了像 **SmolVLM2-256M** 这样的小型、优化的多模态模型在不同配置下的英特尔 CPU 上的表现。根据测试，PyTorch 版本显示出高延迟，首 Token 时间超过 5 秒，解码吞吐量为 0.7 Token/秒。仅使用 Optimum 转换模型并在 OpenVINO 上运行，就能将首 Token 时间大幅降低至 0.42 秒（**加速 12 倍**），并将吞吐量提升至 47 Token/秒（**加速 65 倍**）。应用 8 位仅权重量化进一步降低了首 Token 时间（**加速 1.7 倍**）并提高了吞吐量（**加速 1.4 倍**），同时还减小了模型大小并提高了效率。

上述性能声明对应的平台配置：

系统主板：MSI B860M GAMING PLUS WIFI (MS-7E42)
CPU：Intel® Core™ Ultra 7 265K
插槽/物理核心：1/20 (20线程)
超线程/睿频设置：已禁用
内存：64 GB DDR5 @ 6400 MHz
TDP：665W
BIOS：American Megatrends International, LLC. 2.A10
BIOS发布日期：2024年11月28日
操作系统：Ubuntu 24.10
内核：6.11.0–25-generic
OpenVINO版本：2025.2.0
torch：2.8.0
torchvision：0.23.0+cpu
optimum-intel：1.25.2
transformers：4.53.3
基准测试日期：2025年5月15日
基准测试方：Intel Corporation
性能可能因使用、配置和其他因素而异。请参阅以下平台配置。

## 有用链接与资源

- Notebook
- Try our Space
- Watch the webinar recording
- Optimum Intel Documentation

## 声明与免责声明

性能因使用、配置和其他因素而异。请在性能指数网站上了解更多信息。
性能结果基于配置所示日期的测试，可能未反映所有公开可用的更新。有关配置详情，请参阅备份文件。没有任何产品或组件是绝对安全的。您的成本和结果可能有所不同。英特尔技术可能需要启用硬件、软件或服务激活。
© Intel Corporation。Intel、Intel标识以及其他英特尔标志是英特尔公司或其子公司的商标。

---

> 本文由AI自动翻译，原文链接：[Get your VLM running in 3 simple steps on Intel CPUs](https://huggingface.co/blog/openvino-vlm)
> 
> 翻译时间：2026-01-27 04:38
