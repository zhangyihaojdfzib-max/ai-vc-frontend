---
title: TimesFM：仅解码器架构的时间序列预测基础模型
title_original: A decoder-only foundation model for time-series forecasting
date: '2024-02-02'
source: Google AI Blog
source_url: http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html
author: null
summary: 本文介绍了Google Research提出的TimesFM，一个仅解码器架构的时间序列预测基础模型。该模型在包含1000亿时间点的大型语料库上预训练，拥有2亿参数，能够以零样本方式在不同领域和时间粒度的未见数据集上实现接近最先进监督方法的性能。TimesFM借鉴了大型语言模型的训练范式，将时间序列片段视为Token，通过堆叠的Transformer层进行预测，旨在为用户提供开箱即用的预测能力，减少复杂训练和验证周期。
categories:
- AI研究
tags:
- 时间序列预测
- 基础模型
- Transformer
- 零样本学习
- 深度学习
draft: false
translated_at: '2026-01-06T14:49:02.034Z'
---

一种仅解码器架构的时间序列预测基础模型
2024年2月2日
作者：Rajat Sen 和 Yichen Zhou，Google Research
更新 • 2024年5月8日
《一种仅解码器架构的时间序列预测基础模型》已被 ICML 2024 接收，图表已更新以展示最新结果。该模型现已可在我们的 HuggingFace 和 GitHub 代码库中获取。
快速链接
时间序列预测在零售、金融、制造、医疗保健和自然科学等各个领域无处不在。例如，在零售用例中，据观察，提高需求预测的准确性可以显著降低库存成本并增加收入。深度学习模型已成为预测丰富、多变量时间序列数据的一种流行方法，因为它们已被证明在各种场景下表现良好（例如，深度学习模型在 M5 竞赛中表现出色）。
与此同时，用于自然语言处理任务（如翻译、检索增强生成和代码补全）的大型基础语言模型也取得了快速进展。这些模型在海量文本数据上进行训练，这些数据来源于 Common Crawl 和开源代码等多种渠道，使它们能够识别语言模式。这使它们成为非常强大的零样本工具；例如，当与检索结合时，它们可以回答有关当前事件的问题并进行总结。
尽管基于深度学习的预测模型在很大程度上优于传统方法，并且在降低训练和推理成本方面取得了进展，但它们仍面临挑战：大多数深度学习架构需要漫长而复杂的训练和验证周期，然后客户才能在新的时间序列上测试模型。相比之下，一个时间序列预测的基础模型可以在未经额外训练的情况下，对未见的时间序列数据提供不错的开箱即用预测，使用户能够专注于为实际的下游任务（如零售需求规划）优化预测。
为此，在已被 ICML 2024 接收的论文《一种仅解码器架构的时间序列预测基础模型》中，我们介绍了 TimesFM，这是一个在包含 1000 亿个真实世界时间点的大型时间序列语料库上预训练的单一预测模型。与最新的大语言模型相比，TimesFM 要小得多（2 亿参数），但我们表明，即使在这样的规模下，它在不同领域和不同时间粒度的各种未见数据集上的零样本性能，也接近在这些数据集上显式训练的最先进的监督方法。要访问该模型，请访问我们的 HuggingFace 和 GitHub 代码库。
一种仅解码器架构的时间序列预测基础模型
LLM 通常以仅解码器的方式进行训练，涉及三个步骤。首先，文本被分解为称为 Token 的子词。然后，Token 被输入到堆叠的因果 Transformer 层中，这些层为每个输入 Token 产生一个输出（它不能关注未来的 Token）。最后，对应于第 i 个 Token 的输出总结了之前所有 Token 的信息，并预测第 (i+1) 个 Token。在推理过程中，LLM 一次生成一个 Token 作为输出。例如，当提示为“法国的首都是什么？”时，它可能先生成 Token “The”，然后基于“法国的首都是什么？The”生成下一个 Token “capital”，依此类推，直到生成完整答案：“法国的首都是巴黎”。
一个时间序列预测的基础模型应该能够适应可变的上下文（我们观察到的）和预测范围（我们要求模型预测的）长度，同时有足够的能力来编码来自大型预训练数据集的所有模式。与 LLM 类似，我们使用堆叠的 Transformer 层（自注意力层和前馈层）作为 TimesFM 模型的主要构建模块。在时间序列预测的背景下，我们将一个片段（一组连续的时间点）视为一个 Token，这是近期一项长期预测工作中普及的做法。然后，任务是在给定堆叠 Transformer 层末端的第 i 个输出的情况下，预测第 (i+1) 个时间点片段。
然而，这与语言模型有几个关键区别。首先，我们需要一个带有残差连接的多层感知机块，将时间序列片段转换为一个 Token，该 Token 可以与位置编码一起输入到 Transformer 层。为此，我们使用了一个与我们之前在长期预测工作中类似的残差块。其次，在另一端，来自堆叠 Transformer 的输出 Token 可用于预测比输入片段长度更长的后续时间点，即输出片段长度可以大于输入片段长度。
考虑一个长度为 512 个时间点的时间序列，用于训练一个输入片段长度为 32、输出片段长度为 128 的 TimesFM 模型。在训练期间，模型被同时训练以使用前 32 个时间点来预测接下来的 128 个时间点，使用前 64 个时间点来预测第 65 到 192 个时间点，使用前 96 个时间点来预测第 97 到 224 个时间点，依此类推。在推理过程中，假设模型给定一个长度为 256 的新时间序列，任务是预测未来的接下来 256 个时间点。模型将首先生成第 257 到 384 个时间点的未来预测，然后基于初始的 256 长度输入加上生成的输出来生成第 385 到 512 个时间点。另一方面，如果在我们模型中输出片段长度等于输入片段长度 32，那么对于相同的任务，我们将不得不经过八个生成步骤，而不仅仅是上面的两个步骤。这增加了更多错误累积的机会，因此，在实践中，我们看到更长的输出片段长度能为长期预测带来更好的性能。
| TimesFM 架构。|
预训练数据
就像 LLM 随着更多 Token 而变得更好一样，TimesFM 需要大量合法的时间序列数据来学习和改进。我们花费了大量时间来创建和评估我们的训练数据集，以下是我们发现效果最佳的方法：
合成数据有助于打好基础。可以使用统计模型或物理模拟生成有意义的合成时间序列数据。这些基本的时间模式可以教会模型时间序列预测的“语法”。
真实世界数据增添了真实世界的“风味”。我们梳理了可用的公共时间序列数据集，并有选择地整合了一个包含 1000 亿时间点的大型语料库。在这些数据集中，有 Google Trends 和 Wikipedia Pageviews，它们追踪人们的兴趣所在，很好地反映了许多其他真实世界时间序列的趋势和模式。这有助于 TimesFM 理解大局，并在遇到训练期间未见过的特定领域上下文时更好地泛化。
零样本评估结果
我们使用流行的时间序列基准测试，在训练期间未见的数据上评估 TimesFM 的零样本性能。我们观察到，TimesFM 的性能优于大多数统计方法（如 ARIMA、ETS），并且可以匹配或超越在目标时间序列上显式训练过的强大深度学习模型（如 DeepAR、PatchTST）。
我们使用 Monash 预测档案库来评估 TimesFM 的开箱即用性能。该档案库包含来自交通、天气、需求预测等各个领域的数万个时间序列，覆盖从几分钟到年度的各种频率数据。遵循现有文献，我们检查了经过适当缩放的平均绝对误差，以便可以在不同数据集之间取平均值。我们看到，零样本 TimesFM 优于大多数监督方法，包括近期的深度学习模型。我们还使用 llmtime 提出的一种特定提示技术，将 TimesFM 与 GPT-3.5 在预测方面进行了比较。

我们证明，尽管TimesFM的规模要小几个数量级，但其性能优于llmtime(ZS)。
| TimesFM(ZS) 在 Monash 数据集上与其他监督学习和零样本方法的缩放平均绝对误差（越低越好）的几何平均数（GM，以及我们为何采用此指标）。 |
大多数 Monash 数据集是短期或中期预测，即预测长度不太长。我们还在流行的长时程预测基准上测试了TimesFM，与近期最先进的基线模型 PatchTST（以及其他长时程预测基线）进行比较。在下图中，我们绘制了在ETT数据集上预测未来96和192个时间点的平均绝对误差。该指标是在每个数据集的最后一个测试窗口上计算的（与 llmtime 论文的做法一致）。我们看到，TimesFM不仅超越了 llmtime(ZS) 的性能，而且与在相应数据集上显式训练的监督式 PatchTST 模型的表现相当。
| TimesFM(ZS) 在 ETT 数据集上对比 llmtime(ZS) 和长时程预测基线的最后一个窗口平均绝对误差（越低越好）。 |
结论
我们使用一个包含1000亿个真实世界时间点的大型预训练语料库，训练了一个仅解码器架构的时间序列预测基础模型，其中大部分数据来自 Google Trends 的搜索兴趣时间序列数据和 Wikipedia 的页面浏览量。我们表明，即使是一个相对较小、拥有2亿参数并使用我们 TimesFM 架构的预训练模型，也在来自不同领域和粒度的各种公共基准测试中展现出令人印象深刻的零样本性能。
致谢
这项工作是 Google Research 和 Google Cloud 多位同事合作的成果，包括（按字母顺序排列）：Abhimanyu Das, Weihao Kong, Andrew Leach, Mike Lawrence, Alex Martin, Rajat Sen, Yang Yang, Skander Hannachi, Ivan Kuznetsov 和 Yichen Zhou。

> 本文由AI自动翻译，原文链接：[A decoder-only foundation model for time-series forecasting](http://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html)
> 
> 翻译时间：2026-01-06 02:09
