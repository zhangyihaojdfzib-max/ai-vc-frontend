---
title: 盐堆寻沙：如何定位大规模Salt配置故障的根本原因
title_original: Finding the grain of sand in a heap of Salt
date: '2025-11-13'
source: Cloudflare Blog
source_url: https://blog.cloudflare.com/finding-the-grain-of-sand-in-a-heap-of-salt/
author: Opeyemi Onikute
summary: 本文介绍了Cloudflare在面对数千台服务器因Salt配置管理工具故障导致发布延迟时，如何构建一套自助服务系统来定位根本原因。文章首先阐述了Salt在Cloudflare基础设施中的基础架构与运行机制，分析了其常见的故障模式及其对发布流程的影响。通过将故障与git提交、外部服务问题及临时发布相关联，团队成功将边缘环境中的故障减少了超过5%，显著缩短了软件发布延迟，并减少了SRE团队繁琐的故障排查工作。
categories:
- AI基础设施
tags:
- 配置管理
- SaltStack
- 故障排查
- 基础设施即代码
- SRE
draft: false
---

当数千台服务器在15分钟内出现数百次配置变更峰值时，如何定位配置管理故障的根本原因？
这正是我们在构建基础设施以减少因Salt（一种配置管理工具）故障导致的发布延迟时所面临的挑战。（正如下文将说明的，我们最终将边缘环境中的此类故障减少了超过5%。）我们将探讨Salt的基本原理及其在Cloudflare的应用方式，随后描述常见的故障模式及其如何延迟我们向客户发布重要变更的能力。

通过首先解决架构问题，我们为自助服务机制奠定了基础，用以定位服务器、数据中心及数据中心群组中Salt故障的根本原因。该系统能够将故障与git提交记录、外部服务故障及临时发布相关联。由此带来的成果是：软件发布延迟时长得以缩短，SRE团队繁琐重复的故障排查工作整体减少。

首先，我们将介绍Cloudflare网络的基础架构以及Salt在其中的运行机制。随后，我们将阐述如何解决这项堪比"盐堆寻沙"的挑战。

配置管理（CM）确保系统与其配置信息保持一致，并随时间推移维持该信息的完整性与可追溯性。优秀的配置管理系统能防止系统"漂移"——即偏离期望状态。现代CM系统包含基础设施的详细描述、这些描述的版本控制，以及在不同环境中强制执行期望状态的其他机制。若缺乏CM，管理员必须手动配置系统，这一过程易出错且难以复现。

Salt正是此类CM工具的代表。它专为高速远程执行和配置管理而设计，采用简洁可扩展的模型来管理大规模设备集群。作为成熟的CM工具，Salt提供了跨团队和组织边界的一致性、可复现性、变更控制、可审计性与协作能力。

Salt的设计围绕主从架构、基于ZeroMQ的消息总线及声明式状态系统展开。（在Cloudflare我们通常避免使用"master"和"minion"这类术语，但本文将沿用Salt官方架构描述中的称谓。）Salt主节点作为中央控制器分发任务和配置数据，通过消息总线监听请求并向目标从节点分派指令，同时存储状态文件、支柱数据及缓存文件。Salt从节点是安装在每台受管主机/服务器上的轻量级代理，各从节点通过ZeroMQ与主节点保持连接并订阅发布的任务。当任务匹配到从节点时，它将执行请求功能并返回结果。

下图展示了本文为说明目的而简化的Salt架构示意图（依据官方文档）：
（此处保留原图描述）

状态系统提供声明式配置管理。状态通常以YAML编写，描述资源（软件包、文件、服务、用户等）及其期望属性。一个典型示例是软件包状态，它确保指定版本的软件包被安装：
```yaml
# /srv/salt/webserver/init.sls
include:
  - common

nginx:
  pkg.installed: []

/etc/nginx/nginx.conf:
  file.managed:
    - source: salt://webserver/files/nginx.conf
    - require:
      - pkg: nginx
```

状态可以调用执行模块——这些是实现系统操作的Python函数。应用状态时，Salt会返回结构化结果，包含状态成功与否（result: True/False）、注释、所做更改及持续时间等信息。

我们使用Salt管理不断增长的设备集群，此前已撰文介绍过其广泛用途。上述主从架构使我们能够以状态形式将配置推送到数千台服务器，这对维护网络至关重要。我们设计的变更传播机制包含爆炸半径防护措施。在这些防护措施下，高状态故障成为警示信号而非影响客户的事件。

这种发布设计是刻意为之——我们选择"安全失败"而非硬性失败。通过增设防护机制，在功能覆盖所有用户前安全发布新代码，我们能够自信地传播变更，因为默认情况下故障将中止Salt部署流水线。然而每次中止都会阻塞其他配置部署，并需要人工干预确定根本原因。由于步骤重复且不产生持久价值，这可能迅速演变为繁琐的过程。

我们Salt变更部署流水线的部分环节使用Apt。每X分钟有提交合并至主分支，每Y分钟这些合并被打包部署至APT服务器。从APT服务器获取Salt主节点配置的关键文件是APT源文件：
```bash
# /etc/apt/sources.list.d/saltcodebase.sources
# MANAGED BY SALT -- DO NOT MODIFY
Types: deb
URIs: mirror+file:/etc/apt/mirrorlists/saltcodebase.txt
Suites: stable canary
Components: cloudflare
Signed-By: /etc/apt/keyrings/cloudflare.gpg
```

该文件引导主节点根据特定环境选择正确的套件。通过该套件，主节点获取包含最新变更的Salt Debian软件包，安装后开始部署内含配置。机器部署配置时通过Prometheus报告健康状态。若版本健康，则进入下一环境。在推进前，版本必须通过特定浸泡阈值以暴露潜在错误，使复杂问题显现。这是理想情况。

异常情况则带来多重复杂性：由于采用渐进式部署，若某版本损坏，后续所有版本也将损坏。且损坏版本会持续被新版本覆盖，我们需要完全停止部署。在版本损坏场景中，尽快发布修复至关重要。这触及本文的核心问题：如果损坏的Salt版本已在环境中传播，我们正在放弃部署，又该如何尽快发布修复？

**痛点：Salt的故障模式与错误报告机制（及其对Cloudflare的影响）**

尽管Salt追求幂等且可预测的配置，但故障仍可能发生在渲染、编译或运行阶段。这些故障通常源于配置错误。Jinja模板错误或无效YAML会导致渲染阶段失败，例如缺少冒号、缩进错误或未定义变量。语法错误常伴随指向问题行的堆栈跟踪抛出。

另一常见故障原因是支柱数据或特征数据缺失。由于支柱数据在主节点编译，忘记更新支柱顶层文件或刷新支柱数据可能引发KeyError异常。作为通过依赖关系维持顺序的系统，配置错误的依赖项会导致状态执行顺序错乱或被跳过。当从节点无法向主节点认证，或因网络/防火墙问题无法连接主节点时，也会发生故障。

Salt通过多种方式报告错误。默认情况下，当任何状态失败时，`salt`和`salt-call`命令会以返回码1退出。Salt还为特定情况设置内部返回码：1表示编译错误，2表示状态返回False，5表示支柱编译错误。测试模式显示将进行的更改而不实际执行，有助于捕获语法或顺序问题。

可以通过 `-l debug` CLI 选项来切换调试日志（例如：`salt <minion> state.highstate -l debug`）。

状态返回结果也包含了各个状态失败的详细信息——持续时间、时间戳、函数和结果。如果我们通过引用 Salt 文件服务器中不存在的文件，在 `file.managed` 状态中引入一个故障，就会看到如下失败信息：

```
web1:
----------
ID: nginx
Function: pkg.installed
Result: True
Comment: Package nginx is already installed
Started: 15:32:41.157235
Duration: 256.138 ms
Changes:
----------
ID: /etc/nginx/nginx.conf
Function: file.managed
Result: False
Comment: Source file salt://webserver/files/nginx.conf not found in saltenv 'base'
Started: 15:32:41.415128
Duration: 14.581 ms
Changes:
Summary for web1
------------
Succeeded: 1 (changed=0)
Failed: 1
------------
Total states run: 2
Total run time: 270.719 ms
```

返回结果也可以以 JSON 格式显示：

```json
{
  "web1": {
    "pkg_|-nginx_|-nginx_|-installed": {
      "comment": "Package nginx is already installed",
      "name": "nginx",
      "start_time": "15:32:41.157235",
      "result": true,
      "duration": 256.138,
      "changes": {}
    },
    "file_|-/etc/nginx/nginx.conf_|-/etc/nginx/nginx.conf_|-managed": {
      "comment": "Source file salt://webserver/files/nginx.conf not found in saltenv 'base'",
      "name": "/etc/nginx/nginx.conf",
      "start_time": "15:32:41.415128",
      "result": false,
      "duration": 14.581,
      "changes": {}
    }
  }
}
```

输出格式的灵活性意味着人类可以在自定义脚本中解析它们。但更重要的是，它也可以被更复杂、相互关联的自动化系统所消费。我们知道可以轻松解析这些输出，以便将 Salt 故障的原因归因于某个输入——例如，源代码控制的变更、外部服务故障或软件发布。但还缺少一些东西。

配置错误是大规模系统中常见的故障原因。其中一些甚至可能导致整个系统中断，我们通过发布架构来防止这种情况。当新的发布或配置在生产环境中出现问题时，我们的 SRE 团队需要找到并修复根本原因，以避免发布延迟。正如我们之前提到的，由于系统复杂性，这种分类工作既繁琐又日益困难。

虽然一些组织使用诸如自动化根本原因分析之类的正式技术，但大多数分类工作仍然令人沮丧地需要手动完成。在评估了问题的范围后，我们决定采用自动化方法。本节描述了在生产环境中解决这个广泛而复杂问题的逐步方法。

当 Salt 高状态在 Minion 上失败时，SRE 团队面临着一个繁琐的调查过程：手动 SSH 到 Minion，在日志中搜索错误消息，追踪作业 ID（JID），并在多个关联的 Master 中找到与该 JID 关联的作业。这一切都需要在主日志的 4 小时保留窗口内争分夺秒地进行。根本问题是架构性的：作业结果存储在 Salt Master 上，而不是在执行它们的 Minion 上，这迫使操作员猜测哪个 Master 处理了他们的作业（需要 SSH 到每个 Master），并且限制了没有 Master 访问权限的用户的可见性。

我们构建了一个解决方案，直接在 Minion 上缓存作业结果，类似于 Master 上已有的 `local_cache` 返回器。这使得本地作业检索和延长保留期成为可能。这将一个多步骤、时间敏感的调查转变为单一查询——操作员可以从 Minion 本身检索作业详细信息，自动提取错误上下文，并将故障追溯到特定的文件变更和提交作者。自定义返回器智能地过滤和管理缓存大小，消除了“哪个 Master？”的问题，同时还实现了自动错误归因，减少了解决时间，并将人力从常规故障排除中解放出来。

通过将作业历史去中心化并使其在源头可查询，我们显著地接近了自助调试体验，即故障被自动情境化和归因，让 SRE 团队专注于修复而不是取证。

第二阶段：使用 Salt Blame 模块实现自助服务

一旦作业信息在 Minion 上可用，我们就不再需要确定是哪个 Master 触发了失败的作业。下一步是编写一个 Salt 执行模块，允许外部服务查询作业信息，更具体地说是失败的作业信息，而无需了解 Salt 内部原理。这促使我们编写了一个名为 Salt Blame 的模块。Cloudflare 以其无责文化而自豪，而我们的软件则不然……

Blame 模块负责整合三件事：

我们选择编写一个执行模块是为了简单起见，将外部自动化与理解 Salt 内部原理的需求解耦，并供操作员用于进一步的故障排除。编写执行模块在运维团队中已经非常成熟，并遵循明确定义的最佳实践，如单元测试、代码检查和广泛的同行评审。

这个模块非常简单，这是可以理解的。它按时间倒序遍历本地缓存中的作业，按时间顺序查找第一个失败的作业，然后查找紧接在它之前的成功作业。这样做没有其他原因，只是为了缩小真正首次失败的范围，并为我们提供前后状态结果。在这个阶段，我们有几种途径向调用者提供上下文：为了找到可能的提交责任人，我们会查看最后一次成功作业 ID 和失败之间所有的提交，以确定是否有任何提交更改了与故障相关的文件。我们还提供了失败状态列表及其输出，作为发现根本原因的另一条途径。我们了解到，这种灵活性对于覆盖广泛的故障可能性非常重要。

我们还区分了正常的状态失败和编译错误。正如 Salt 文档所述，每个作业根据结果返回不同的返回码。

我们的大多数故障表现为状态失败，这是源代码控制变更的结果。为我们的客户构建新功能的工程师可能会无意中引入一个未被我们的 CI 和 Salt Master 测试捕获的故障。在模块的第一个迭代中，列出所有失败的状态就足以精确定位高状态失败的根本原因。

然而，我们注意到我们有一个盲点。编译错误不会导致状态失败，因为没有状态运行。由于这些错误返回的返回码与我们检查的不同，模块对它们完全视而不见。大多数编译错误发生在 Salt 服务依赖项在状态编译阶段失败时。它们也可能由于源代码控制的变更而发生，尽管这种情况很少见。

通过同时考虑状态失败和编译错误，我们极大地提高了精确定位问题的能力。

我们将该模块发布给SRE团队后，他们立即意识到了它能加速Salt故障排查的优势。
# 列出所有近期失败的状态
minion~$ salt-call -l info blame.last_failed_states
local:
|_
----------
__id__:
/etc/nginx/nginx.conf
__run_num__:
5221
__sls__:
foo
changes:
----------
comment:
Source file salt://webserver/files/nginx.conf not found in saltenv 'base'
duration:
367.233
finish_time_stamp:
2025-10-22T10:00:17.289897+00:00
fun:
file.managed
name:
/etc/nginx/nginx.conf
result:
False
start_time:
10:00:16.922664
start_time_stamp:
2025-10-22T10:00:16.922664+00:00
# 列出与失败状态相关的所有提交
minion~$ salt-call -l info blame.last_highstate_failure
local:
----------
commits:
|_
----------
author_email:
[email protected]
author_name:
John Doe
commit_datetime:
2025-06-30T15:29:26.000+00:00
commit_id:
e4a91b2c9f7d3b6f84d12a9f0e62a58c3c7d9b5a
path:
/srv/salt/webserver/init.sls
message:
reviewed 5 change(s) over 12 commit(s) looking for 1 state failure(s)
result:
True
# 列出所有编译错误
minion~$ salt-call -l info blame.last_compile_errors
local:
|_
----------
error_types:
job_timestamp:
2025-10-24T21:55:54.595412+00:00
message: A service failure has occured
state: foo
traceback:
Full stack trace of the failure
urls: http://url-matching-external-service-if-found
第三阶段：自动化，自动化，再自动化！
更快的故障排查总是受欢迎的进展，工程师们也很习惯在minion上运行本地命令来排查Salt故障。但在繁忙的值班期间，时间至关重要。当故障跨越多个数据中心或机器时，在所有相关minion上运行命令很容易变得繁琐。这种解决方案还需要在多个节点和数据中心之间进行上下文切换。我们需要一种方法，能够通过单一命令聚合常见的故障类型——无论是单个minion、预生产数据中心还是生产数据中心。

我们实施了多种机制来简化排查流程并消除手动触发。我们的目标是让这些工具尽可能靠近排查地点，而聊天工具通常是首选。通过三个不同的命令，工程师现在可以直接在聊天线程中排查Salt故障。

通过采用分层方法，我们实现了对minion、数据中心以及数据中心组进行独立排查。这种层级结构使该架构完全可扩展、灵活且能自组织。工程师可以排查单个minion上的故障，同时也能根据需要排查整个数据中心。

同时排查多个数据中心的能力，对于追踪预生产数据中心故障的根本原因立即显现出价值。这些故障会延迟变更向其他数据中心的传播，并阻碍我们为客户功能、错误修复或事件补救发布变更的能力。增加此排查选项后，Salt故障的调试和修复时间缩短了5%以上，使我们能够持续为客户发布重要变更。

虽然5%看起来并非立竿见影的巨大改进，但其魔力在于累积效应。我们不会公布发布延迟时间的具体数字，但我们可以做一个简单的思想实验。如果平均每天花费的时间仅为60分钟，那么每月减少5%就能节省90分钟（1小时30分钟）。

另一个间接好处在于更高效的反馈循环。由于工程师花费更少时间摆弄复杂配置，这部分精力被转移到防止问题复发上，从而进一步以无法估量的程度减少了总体时间。我们未来的计划包括进行度量和数据分析，以理解这些直接和间接反馈循环的成果。

下图展示了预生产环境排查输出的一个示例。我们能够将故障与git提交、发布和外部服务故障关联起来。在繁忙的值班期间，这些信息对于快速修复故障至关重要。平均而言，每个minion的"blame"操作耗时不到30秒，而多个数据中心能够在一分钟或更短时间内返回结果。

下图描述了分层模型。层级中的每个步骤都是并行执行的，使我们能够获得极快的结果。

有了这些可用机制，我们通过在已知条件（尤其是那些对发布流水线有影响的条件）下触发排查自动化，进一步缩短了排查时间。这直接提高了边缘变更的速度，因为找到根本原因并向前修复或回滚所需的时间更少了。

第四阶段：度量，度量，再度量
在我们实现了极速的Salt故障排查之后，我们需要一种方法来度量根本原因。虽然单个根本原因本身价值有限，但历史分析被认为非常重要。我们希望了解故障的常见原因，特别是当它们阻碍我们为客户交付价值时。这些知识形成了一个反馈循环，可用于将故障数量保持在较低水平。

我们使用Prometheus和Grafana来跟踪故障的主要原因：git提交、发布、外部服务故障和未归因的失败状态。失败状态列表尤其有用，因为我们希望了解重复出现的问题，并推动更好地采用稳定的发布实践。我们也特别关注根本原因——由git提交引起的故障数量激增表明需要采用更好的编码规范和代码检查，外部服务故障激增表明需要调查内部系统的回归问题，而基于发布的故障激增则表明需要更好的门控和发布引导。

我们按月周期分析这些指标，通过内部工单和升级流程提供反馈机制。虽然这些工作的直接影响尚不明显，因为工作还处于初期阶段，但我们期望通过减少所见的故障数量，来改善Saltstack基础设施和发布流程的整体健康状况。

许多运维工作常被视为"必要的麻烦"。运维人员习惯于在故障发生时进行干预和修复。这种警报-响应的循环对于维持基础设施运行是必要的，但它常常导致重复性劳动。我们在之前的博客文章中讨论过重复性劳动的影响。

这项工作代表了朝着正确方向迈出的又一步——为我们的值班SRE减少更多重复性劳动，并腾出宝贵时间来处理新问题。我们希望这能鼓励其他运维工程师分享他们在各自组织中减少总体重复性劳动方面取得的进展。我们也希望这类工作能够在Saltstack内部得到采纳，尽管不同公司的生产系统缺乏同质性，使得这一点不太可能实现。

未来，我们计划提高检测的准确性，并减少依赖外部输入关联来确定失败结果的根源。我们将研究如何将更多此类逻辑移入我们原生的Saltstack模块中，从而进一步简化流程，并避免因外部系统漂移而导致的回归问题。

如果您对这类工作感兴趣，我们鼓励您查看我们的招聘页面。

---

> 本文由AI自动翻译，原文链接：[Finding the grain of sand in a heap of Salt](https://blog.cloudflare.com/finding-the-grain-of-sand-in-a-heap-of-salt/)
> 
> 翻译时间：2026-01-06 01:17
