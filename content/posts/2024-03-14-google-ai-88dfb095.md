---
title: Cappy：小型评分器提升大型多任务语言模型性能
title_original: 'Cappy: Outperforming and boosting large multi-task language models
  with a small scorer'
date: '2024-03-14'
source: Google AI Blog
source_url: http://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html
author: null
summary: 本文介绍了Google Research提出的Cappy，一个仅3.6亿参数的轻量级预训练评分器。Cappy基于RoBERTa构建，能够评估指令与候选响应的匹配度，输出0-1的正确性分数。它既可独立处理分类任务，也能作为辅助组件增强大型多任务语言模型的性能，同时避免对LLM参数进行反向传播，降低内存需求，并与闭源模型兼容。该方法在提升模型效率与可访问性方面提供了新思路。
categories:
- AI研究
tags:
- 大语言模型
- 多任务学习
- 模型优化
- 轻量级模型
- 自然语言处理
draft: false
---

Cappy：用小型评分器超越并增强大型多任务语言模型
2024年3月14日
作者：Google Research 软件工程师 Yun Zhu 和 Lijuan Liu
快速链接

大语言模型（LLM）的进步催生了一种新范式，它将各种自然语言处理（NLP）任务统一在一个遵循指令的框架内。最近的多任务LLM，如T0、FLAN和OPT-IML，就是这种范式的典范。首先，收集多任务数据，每个任务遵循一个特定于任务的模板，其中每个带标签的示例都被转换成一个指令（例如，“将这些概念组合成一个句子：滑雪、山、滑雪者”）与一个相应的响应（例如，“滑雪者滑下山”）配对。这些指令-响应对用于训练LLM，从而得到一个条件生成模型，该模型接收指令作为输入并生成响应。此外，多任务LLM展现出了卓越的任务泛化能力，因为它们可以通过理解和解决全新的指令来处理未见过的任务。

由于仅使用指令来理解和解决各种任务具有复杂性，多任务LLM的规模通常从数十亿参数到数千亿参数不等（例如，FLAN-11B、T0-11B和OPT-IML-175B）。因此，运行如此庞大的模型带来了重大挑战，因为它们需要大量的计算能力，并对GPU和TPU的内存容量提出了很高的要求，使得其训练和推理成本高昂且效率低下。为每个下游任务维护一个独特的LLM副本需要大量的存储空间。此外，最强大的多任务LLM（例如FLAN-PaLM-540B）是闭源的，无法进行适配。然而，在实际应用中，利用单个多任务LLM以零样本方式管理所有可想象的任务仍然很困难，尤其是在处理复杂任务、个性化任务以及那些无法用指令简洁定义的任务时。另一方面，下游训练数据的规模通常不足以在不融入丰富先验知识的情况下训练好一个模型。因此，人们长期以来一直希望在绕过存储、内存和访问问题的同时，利用下游监督来适配LLM。

某些参数高效的调优策略，包括提示词调优和适配器，大大降低了存储需求，但它们在调优过程中仍然需要通过LLM参数进行反向传播，从而使其内存需求保持在高位。此外，一些上下文学习技术通过将有限数量的监督示例整合到指令中来规避参数调优。然而，这些技术受到模型最大输入长度的限制，只允许使用少量样本来指导任务解决。

在NeurIPS 2023上发表的论文《Cappy：用小型评分器超越并增强大型多任务语言模型》中，我们提出了一种新颖的方法，以提升多任务LLM的性能和效率。我们引入了一个轻量级的预训练评分器Cappy，它基于RoBERTa进行持续预训练，仅有3.6亿参数。Cappy接收一个指令和一个候选响应作为输入，并输出一个介于0到1之间的分数，表示响应相对于指令的估计正确性。Cappy可以独立处理分类任务，也可以作为LLM的辅助组件，提升其性能。此外，Cappy能够高效地实现下游监督，无需任何微调，这避免了通过LLM参数进行反向传播的需求，并降低了内存要求。最后，使用Cappy进行适配不需要访问LLM参数，因为它与闭源的多任务LLM兼容，例如那些只能通过WebAPI访问的模型。

**预训练**
我们从相同的数据集收集开始，该集合包括来自PromptSource的39个不同数据集，这些数据集曾用于训练T0。这个集合涵盖了广泛的任务类型，如问答、情感分析和摘要生成。每个数据集都与一个或多个模板相关联，这些模板将原始数据集中的每个实例转换为一个指令与其真实响应配对。

Cappy的回归建模要求每个预训练数据实例包含一个指令-响应对以及该响应的正确性标注，因此我们生成了一个带有0到1范围正确性标注的数据集。对于生成任务中的每个实例，我们利用现有的多任务LLM，通过采样生成多个响应，条件是基于给定的指令。随后，我们使用响应与实例真实响应之间的相似性，为指令和每个响应组成的配对分配一个标注。具体来说，我们使用Rouge-L（一种常用于衡量整体多任务性能的指标，已被证明与人类评估高度一致）来计算这种相似性，作为一种弱监督形式。

最终，我们获得了一个包含1.6亿个实例的有效回归数据集，每个实例都配有正确性分数标注。最终的Cappy模型是在RoBERTa模型之上，使用该回归数据集进行持续预训练的结果。Cappy的预训练是在Google的TPU-v4上使用RedCoast（一个用于自动化分布式训练的轻量级工具包）进行的。

**应用Cappy**
Cappy在候选选择机制内解决实际任务。更具体地说，给定一个指令和一组候选响应，Cappy为每个候选响应生成一个分数。这是通过将指令与每个单独的响应一起输入，然后将得分最高的响应分配为其预测来实现的。在分类任务中，所有候选响应本质上是预定义的。例如，对于一个情感分类任务的指令（例如，“根据这条评论，用户会推荐这个产品吗？：‘即使对非游戏玩家来说也令人惊叹。’”），候选响应是“是”或“否”。在这种情况下，Cappy独立工作。另一方面，在生成任务中，候选响应不是预定义的，需要现有的多任务LLM来生成候选响应。在这种情况下，Cappy作为多任务LLM的辅助组件，增强其解码能力。

**使用Cappy适配多任务LLM**
当有可用的下游训练数据时，Cappy能够高效地对多任务LLM进行下游任务适配。具体来说，我们微调Cappy，将下游任务信息整合到LLM的预测中。这个过程涉及使用与构建预训练数据相同的数据标注流程，为特定的下游训练数据创建一个独立的回归数据集。因此，经过微调的Cappy与多任务LLM协作，提升了LLM在下游任务上的性能。

与其他LLM调优策略相比，使用Cappy适配LLM显著降低了对设备内存的高需求，因为它避免了为下游任务通过LLM参数进行反向传播的需要。此外，Cappy适配不依赖于对LLM参数的访问，使其与闭源的多任务LLM兼容，例如那些只能通过WebAPI访问的模型。与上下文学习方法相比（该方法通过将训练示例附加到指令前缀来规避模型调优），Cappy不受LLM最大输入长度的限制。因此，Cappy可以整合无限数量的下游训练示例。

Cappy同样可与其他适配方法结合使用，例如微调和上下文学习，从而进一步提升整体性能。

**结果**
我们在PromptSource的十一项保留语言理解分类任务中评估了Cappy的性能。实验表明，仅含3.6亿参数的Cappy在性能上超越了OPT-175B和OPT-IML-30B，并与现有最佳多任务LLM（T0-11B和OPT-IML-175B）的准确率持平。这些发现凸显了Cappy的强大能力与参数效率，这归功于其基于评分的预训练策略——该策略通过区分高质量与低质量响应来整合对比信息。相比之下，以往的多任务LLM完全依赖仅使用真实答案的教师强制训练方法。

我们还研究了Cappy在多任务LLM上对BIG-Bench复杂任务的适配效果。BIG-Bench是一组人工精选的任务集，被认为超出许多LLM的能力范围。我们聚焦全部45项生成型BIG-Bench任务，特别是那些未提供预设答案选项的任务。我们使用每个测试集上的Rouge-L分数（代表模型生成结果与对应真实答案的整体相似度）评估性能，并报告45项测试的平均得分。本实验中，所有FLAN-T5变体均作为骨干LLM，且基础FLAN-T5模型保持冻结状态。如下所示的结果表明，Cappy大幅提升了FLAN-T5模型的性能，始终优于通过LLM自身评分进行样本选择所实现的最有效基线。

**结论**
我们提出了Cappy这一创新方法，旨在提升多任务LLM的性能与效率。实验中，我们使用Cappy将单个LLM适配至多个领域。未来，Cappy作为预训练模型有望在单一LLM之外的其他创新场景中得到应用。

**致谢**
感谢Bowen Tan、Jindong Chen、Lei Meng、Abhanshu Sharma和Ewa Dominowska提供的宝贵反馈。同时感谢Eric Xing和Zhiting Hu的建设性意见。

---

> 本文由AI自动翻译，原文链接：[Cappy: Outperforming and boosting large multi-task language models with a small scorer](http://blog.research.google/2024/03/cappy-outperforming-and-boosting-large.html)
> 
> 翻译时间：2026-01-05 17:13
