---
title: llama.cpp新增模型管理功能：支持动态加载与切换
title_original: 'New in llama.cpp: Model Management'
date: '2025-12-11'
source: Hugging Face Blog
source_url: https://huggingface.co/blog/ggml-org/model-management-in-llamacpp
author: null
summary: llama.cpp服务器最新推出的路由器模式实现了类似Ollama的模型管理功能。该功能允许用户在无需重启服务器的情况下，动态加载、卸载和切换多个GGUF模型。它采用多进程架构，确保单个模型崩溃不影响其他模型运行，并支持自动发现模型、按需加载、LRU淘汰机制以及通过API或Web
  UI进行模型管理。此更新旨在方便用户进行模型A/B测试、多租户部署及开发过程中的灵活切换。
categories:
- AI基础设施
tags:
- llama.cpp
- 模型管理
- 本地部署
- 大语言模型
- 开源工具
draft: false
translated_at: '2026-01-06T00:58:49.514Z'
---

llama.cpp 新功能：模型管理

llama.cpp 服务器现已推出路由器模式，允许您动态加载、卸载和切换多个模型，而无需重启服务器。
提醒：llama.cpp 服务器是一个轻量级、兼容 OpenAI 的 HTTP 服务器，用于在本地运行 LLM（大语言模型）。
此功能是将 Ollama 风格的模型管理引入 llama.cpp 的热门需求。它采用多进程架构，每个模型在其自己的进程中运行，因此如果一个模型崩溃，其他模型不受影响。

快速开始
通过不指定模型来启动路由器模式的服务器：
```
llama-server
```
这将自动从您的 llama.cpp 缓存（`LLAMA_CACHE` 或 `~/.cache/llama.cpp`）中发现模型。如果您之前通过 `llama-server -hf user/model` 下载过模型，它们将自动可用。
您也可以指向一个包含 GGUF 文件的本地目录：
```
llama-server --models-dir ./my-models
```

功能特性
- **自动发现**：扫描您的 llama.cpp 缓存（默认）或自定义的 `--models-dir` 文件夹以查找 GGUF 文件
- **按需加载**：模型在首次被请求时自动加载
- **LRU 淘汰**：当达到 `--models-max` 限制（默认：4）时，最近最少使用的模型将被卸载
- **请求路由**：请求中的 `model` 字段决定了由哪个模型处理该请求

示例
与特定模型对话
```
curl http://localhost:8080/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
"model": "ggml-org/gemma-3-4b-it-GGUF:Q4_K_M",
"messages": [{"role": "user", "content": "Hello!"}]
}'
```
首次请求时，服务器会自动将模型加载到内存中（加载时间取决于模型大小）。对同一模型的后续请求是即时的，因为它已经加载。

列出可用模型
```
curl http://localhost:8080/models
```
返回所有已发现的模型及其状态（`loaded`、`loading` 或 `unloaded`）。

手动加载模型
```
curl -X POST http://localhost:8080/models/load \
-d '{"model": "my-model.gguf"}'
```

卸载模型以释放显存
```
curl -X POST http://localhost:8080/models/unload \
```

关键选项
| 标志 | 描述 |
|---|---|
| `--models-dir PATH` | 包含您的 GGUF 文件的目录 |
| `--models-max N` | 同时加载的最大模型数（默认：4） |
| `--no-models-autoload` | 禁用自动加载；需要显式调用 `/models/load` |

所有模型实例都继承自路由器的设置：
```
llama-server --models-dir ./models -c 8192 -ngl 99
```
所有加载的模型都将使用 8192 上下文窗口和完全的 GPU 卸载。您也可以使用预设文件定义每个模型的设置：
```
llama-server --models-preset config.ini
```
```
[my-model]
model = /path/to/model.gguf
ctx-size = 65536
temp = 0.7
```

Web UI 中也支持
内置的 Web UI 也支持模型切换。只需从下拉菜单中选择一个模型，它就会自动加载。

加入讨论
我们希望此功能能让您更轻松地进行不同模型版本的 A/B 测试、运行多租户部署，或者仅仅在开发过程中无需重启服务器即可切换模型。
有问题或反馈吗？请在下方留言或在 GitHub 上提交问题。

> 本文由AI自动翻译，原文链接：[New in llama.cpp: Model Management](https://huggingface.co/blog/ggml-org/model-management-in-llamacpp)
> 
> 翻译时间：2026-01-06 00:58
