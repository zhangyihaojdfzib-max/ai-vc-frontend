---
title: LangChain团队如何重建聊天机器人并提升Agent可靠性
title_original: Why We Rebuilt LangChain’s Chatbot and What We Learned
date: '2025-11-05'
source: LangChain Blog
source_url: https://blog.langchain.com/rebuilding-chat-langchain/
author: LangChain
summary: 本文介绍了LangChain团队重建其官方聊天机器人的过程与经验。最初构建的chat.langchain.com未能被内部工程师有效使用，因为他们需要结合文档、知识库和代码库的三步工作流来解决问题。团队通过自动化这一流程，创建了包含三个专门子Agent的深度Agent系统，显著提升了问题解决效率。随后，他们意识到应将这一成功模式应用于公开产品，并采用createAgent架构处理简单文档问题，结合代码分析能力应对复杂问题，最终构建出更可靠、实用的生产级Agent应用。
categories:
- AI产品
tags:
- LangChain
- 智能体开发
- 聊天机器人
- AI应用实践
- 技术架构
draft: false
translated_at: '2026-01-06T01:14:28.377Z'
---

作者：Liam Bush

**背景**
每个成功的平台都需要可靠的支持，但我们发现自己的团队花费大量时间追踪技术问题的答案。这种摩擦不仅拖慢了工程师的效率，更是我们用户面临的关键瓶颈。

我们决定利用我们倡导的工具来解决这个问题：LangChain、LangGraph 和 LangSmith。我们最初构建 chat.langchain.com 作为一个原型，明确设计用于两个功能：
- **产品问答**：帮助用户——以及我们自己的团队——即时获得关于产品问题的权威答案。
- **客户原型**：作为一个生动的示例，展示客户如何使用 LangChain 技术栈构建复杂、可靠的 Agent（智能体）。

我们有强烈的意图和可用的产品。但我们要坦白一件事：我们的支持工程师并没有积极使用 LangChain Chatbot。这正是我们真正学习的开始。这个故事讲述了我们如何修复自己的 Agent（智能体）——以及我们在构建真正可靠、生产级的、可供客户调整和使用的应用程序方面学到了什么。

我们的团队没有积极使用 Chat LangChain，并不是因为它坏了，也不是因为他们不相信它。而是因为当有人问“为什么流式传输在生产环境中不工作？”时，他们需要的不仅仅是仅以文档作为资源。我们都知道文档永远不够用。

因此，他们建立了自己的工作流程：
- **步骤 1**：搜索我们的文档（docs.langchain.com）以了解功能应该做什么。
- **步骤 2**：检查我们的知识库（support.langchain.com），看看其他用户是否遇到了同样的问题以及是如何解决的。
- **步骤 3**：打开 Claude Code，搜索实际实现，并验证代码实际做了什么。

文档用于官方说明。知识库用于现实问题。代码库用于获取真相。

**我们决定将其自动化**
这个三步流程效果非常好。我们看着他们每天这样做几十次，然后想：如果我们把这个确切的工作流程自动化会怎样？

于是，我们构建了一个内部的 Deep Agent（一个用于构建能够处理复杂、多步骤任务的 Agent（智能体）的库），它包含三个专门的子 Agent（智能体）——一个用于文档搜索，一个用于知识库搜索，一个用于代码库搜索——每个子 Agent（智能体）在将见解传递给主协调 Agent（智能体）之前，都会提出后续问题并过滤结果。

主 Agent（智能体）会综合所有信息，并提供如下答案：

**示例输出：**
"要从子图进行流式传输，请根据 LangGraph 流式传输文档，在你的流配置中设置 `subgraphs: true`。有一篇名为《[升级后为什么 Token 流式传输不工作](https://support.langchain.com/articles/7150806184-Why-is-token-by-token-streaming-not-working-after-upgrading-LangGraph?)[?](https://www.notion.so/263808527b1780db9f26fa75aed5e7e3?pvs=21)》的支持文章解释了这个问题——你需要启用子图流式传输才能从嵌套 Agent（智能体）获得 Token 级别的更新。具体实现在 pregel/main.py 的第 3373-3279 行，其中 `subgraphs` 标志控制嵌套图的输出是否包含在流中。"

我们的工程师很喜欢它。
它每周为他们节省了数小时处理复杂调试的时间。他们描述一个生产问题，就能得到一个全面的答案，其中引用了文档、参考了已知解决方案，并指出了相关的具体代码行。

**然后我们意识到**
接着有人问了一个显而易见的问题：如果这对我们如此有效，为什么我们公开的 Chat LangChain 不这样工作呢？

这是个合理的观点。我们的公开工具是将文档分块成片段，生成嵌入/向量，存储在向量数据库中。随着文档更新，我们必须不断重新索引。用户能得到答案，但引用需要改进，上下文也是零散的。

我们只是通过复制有效的方法，就在内部意外地构建了更好的东西。是时候将同样的方法应用到公开产品中了。

当我们开始重建时，我们很快意识到需要结合两种不同的架构，以应对两大类问题。大多数问题可以使用文档和知识库来回答。其余的问题则需要分析代码基础。

**我们如何构建新的 Agent（智能体）**

**对于简单文档：Create Agent（智能体）**
我们选择 `createAgent`（langchain 中的 Agent（智能体）抽象）作为 chat.langchain.com 的默认模式，因为它最适合速度。

没有规划阶段，没有编排开销——只有即时的工具调用和答案。该 Agent（智能体）搜索文档，如果需要则检查知识库，如果结果不明确则优化其查询，然后返回答案。大多数文档问题可以通过 3-6 次工具调用来处理，而 Create Agent（智能体）能在几秒钟内执行这些调用。

**模型选项：**
我们为最终用户提供多种模型——Claude Haiku 4.5、GPT-4o Mini 和 GPT-4o-nano——我们发现 Haiku 4.5 在工具调用方面异常快速，同时保持很强的准确性。`createAgent` 和 Haiku 4.5 的组合为大多数查询提供低于 15 秒的响应，这正是文档问答所需要的。

**我们如何优化它：**
我们使用 LangSmith 来追踪每次对话，识别 Agent（智能体）在何处进行了不必要的工具调用，并优化我们的提示词。数据显示，如果我们教会 Agent（智能体）提出更好的后续问题，大多数问题可以通过 3-6 次工具调用来回答。LangSmith 的评估套件让我们能够对不同的提示策略进行 A/B 测试，并衡量速度和准确性的改进。

**对于使用代码回答：带有子图的 Deep Agent（智能体）**
许多问题除了利用文档、知识库和交叉引用已知问题作为资源外，还需要深入搜索我们的代码库以验证实现细节。

**架构：**
对于这些任务，我们构建了一个带有专门子图的 Deep Agent（智能体）：一个用于文档搜索，一个用于知识库搜索，一个用于代码库搜索。

每个子 Agent（智能体）独立运行，提出后续问题，筛选信息，并仅提取最相关的见解，然后将其传递给主协调 Agent（智能体）。这可以防止主 Agent（智能体）被上下文淹没，同时允许每个领域专家根据需要深入挖掘。

**代码库搜索的优势：**
代码库搜索子 Agent（智能体）特别强大。它可以使用模式匹配搜索我们的私有仓库，浏览文件结构以理解上下文，并以行号精度读取具体的实现。

**权衡：**
这种深度 Agent（智能体）架构运行时间更长——对于复杂查询有时需要 1-3 分钟——但其彻底性是值得的。当初始响应未能解决核心问题时，我们会利用 DeepAgent。

**免责声明：** 此模式在发布时仅对部分用户启用，将在几天后全面开放。

**为什么我们放弃了向量嵌入**
文档搜索的标准方法——将文档分块、生成嵌入/向量、存储在向量数据库中、通过相似性检索——对于 PDF 等非结构化内容效果很好。但对于结构化的产品文档，我们一直遇到三个问题。

**分块破坏了结构。** 当你将文档切成 500 Token 的片段时，你会丢失标题、子章节和上下文。Agent（智能体）会引用“设置 `streaming=True`”，但不解释为什么或何时设置。用户不得不翻阅页面来寻找他们需要的东西。

**持续重新索引。** 我们的文档每天更新多次。每次更改都意味着重新分块、重新生成嵌入/向量和重新上传。这拖慢了我们的速度。

**模糊的引用。** 用户无法验证答案或追踪信息来源。

突破在于我们意识到我们解决了错误的问题。文档已经是组织好的。知识库已经是分类好的。代码库已经是可导航的。

我们不需要更智能的检索——我们需要让Agent（智能体）直接访问现有的结构。

**更好的方法：直接API访问与智能提示词**

我们没有进行分块和嵌入/向量化，而是让Agent直接访问真实内容。对于文档，我们使用Mintlify的API
，它返回完整的页面，包含所有标题、子章节和完整的代码示例。对于知识库，我们首先按标题查询支持文章，然后完整阅读最有希望的那些。对于代码库搜索，我们将代码库上传到LangGraph Cloud部署中，并使用ripgrep
进行模式匹配、目录遍历以理解结构，以及文件读取以提取具体实现。

Agent不基于相似度分数进行检索。它像人类一样搜索——使用关键词、优化和后续问题。

**这就是魔法发生的地方。** 我们不只是告诉Agent搜索一次并返回它找到的任何内容。我们通过提示词让它批判性地思考是否拥有足够的信息。如果结果模糊或不完整，Agent会优化其查询并再次搜索。如果文档提到了一个概念但没有解释，Agent会专门搜索那个概念。如果存在多种可能的解释，Agent会缩小到最相关的一个。

**工具设计：为人类工作流程构建**

我们设计工具是为了反映人类实际的搜索方式，而不是检索算法的工作方式。

**文档搜索：完整页面，而非片段**

文档搜索工具查询Mintlify的API
并返回完整页面。当有人询问关于流式传输的问题时，Agent得到的不是来自不同章节的三个互不关联的段落——它得到的是整个流式传输文档页面，其结构完全像人类阅读时看到的那样。

@tool
def SearchDocsByLangChain(query: str, page_size: int = 5, language: Optional[str] = None) -> str:
    """通过Mintlify API搜索LangChain文档"""
    params = {"query": query, "page_size": page_size}
    if language:
        params["language"] = language
    response = requests.get(MINTLIFY_API_URL, params=params)
    return _format_search_results(response.json())

但我们不止于此。我们提示Agent评估初始结果是否真正回答了问题。这是正确的章节吗？是否有需要澄清的相关概念？更具体的搜索词会更好吗？

Agent有4-6次工具调用的预算，我们鼓励它在回答前策略性地使用这些调用来完善其理解。

**实际过程如下：**

用户问："如何为我的Agent添加记忆？"

Agent搜索"记忆"
，得到涵盖检查点、对话历史和Store API的结果。Agent没有随机选择一个，而是意识到这个问题是模糊的——"记忆"可能意味着在单个线程内持久化对话状态，也可能意味着跨多个对话存储事实。

它再次搜索"检查点"
以缩小到线程级持久化的范围，获取支持文章"如何在LangGraph中配置检查点？"，并认识到它没有涵盖跨线程记忆。

因此，它搜索"store API"
来填补空白。

最终答案涵盖了用于对话历史的检查点和用于长期记忆的Store API，并精确引用了所使用的支持文章和文档。

这种迭代搜索过程在Create Agent中几秒钟内完成，但它从根本上改变了回答的质量。Agent不仅仅是在检索——它还在推理用户实际需要什么。

**知识库搜索：先扫描，再阅读**

我们将知识库（由Pylon驱动）搜索构建为一个两步过程，因为这就是人类使用知识库的方式。

首先，Agent检索文章标题——有时是几十个——并扫描它们以确定哪些看起来相关。然后它只完整阅读那些文章。

@tool
def search_support_articles(collections: str = "all", limit: int = 50) -> str:
    """步骤1：获取文章标题进行扫描"""
    articles = pylon_client.list_articles(collections=collections, limit=limit)
    return json.dumps([{
        "id": a["id"],
        "title": a["title"],
        "url": a["url"]
    } for a in articles])

@tool
def get_article_content(article_ids: List[str]) -> str:
    """步骤2：阅读最相关的文章"""
    articles = pylon_client.get_articles(article_ids)
    return "\\n\\n---\\n\\n".join([
        f"# {a['title']}\\n\\n{a['content']}\\n\\nSource: {a['url']}"
        for a in articles
    ])

**为什么这有效：**

这防止了Agent被信息淹没。Agent不是将30篇完整文章塞进上下文窗口，而是筛选出真正重要的2-3篇，彻底阅读它们，并提取关键见解。

提示词强化了这一点：注重质量而非数量，必要时缩小搜索范围，并且只返回直接回答问题的信息。

**代码库搜索：搜索、导航、验证**

**这就是我们的Deep Agent
大放异彩的地方。**

我们给了Agent三个工具，它们反映了开篇提到的工作流程——与我们的工程师使用Claude Code
时遵循的模式相同：

@tool
def search_public_code(pattern: str, path: Optional[str] = None) -> str:
    """步骤1：查找匹配模式的代码"""
    cmd = ["rg", pattern, str(path or search_path)]
    return subprocess.run(cmd, capture_output=True, text=True).stdout

@tool
def list_public_directory(path: str, max_depth: int = 2) -> str:
    """步骤2：理解文件结构"""
    cmd = ["tree", "-L", str(max_depth), str(path)]

@tool
def read_public_file(file_path: str, start_line: int = 1, num_lines: int = 100) -> str:
    """步骤3：阅读实际实现"""
    with open(file_path, "r") as f:
        lines = f.readlines()
    return "\\n".join(lines[start_line-1:start_line-1+num_lines])

**工作原理：**

首先，它使用ripgrep
在代码库中搜索模式。然后它列出目录结构以理解文件是如何组织的。最后，它读取特定文件，专注于相关部分，并返回带有行号的实现。

**真实示例：**

用户报告生产环境中流式传输Token会挂起。文档子Agent发现流式传输配置涉及缓冲区设置。知识库子Agent浮现出一篇关于升级后Token流式传输问题的支持文章。

但代码库子Agent才是找到实际实现的那个——它搜索"streaming buffer"
，导航到callbacks/streaming.py
，并返回第47-83行，其中硬编码了默认缓冲区大小。

**这就是解决实际问题的深度调查。**

**区别何在？** Deep Agent
可以并行处理所有三个领域，并将中期发现汇总成一个连贯的答案。

**Deep Agent和子图如何解决上下文过载**

当我们最初将深度Agent构建为一个可以访问所有三个工具的单一系统时，它会返回它找到的所有东西。主Agent会一次性收到五个文档页面、十二篇知识库文章和二十个代码片段。

上下文窗口会爆炸，最终的回答要么充斥着无关的细节，要么完全错过了关键见解。

**正是在那时，我们用专门的子图重构了它。**

**工作原理：**

每个子Agent独立运行。它搜索自己的领域，提出后续问题以澄清模糊性，筛选结果，并只提取黄金数据：回答问题所需的基本事实、引用和上下文。

主协调器Agent永远不会看到原始的搜索结果。它只接收来自每个领域专家的精炼见解。查看完整追踪记录及提示词**此处。

**为什么这很重要：**

文档子Agent可能阅读了五个完整页面，但只返回两个关键段落。知识库子Agent可能扫描了二十篇文章标题，但只返回三个相关的摘要。

代码库子智能体可能会搜索五十个文件，但只返回带有行号的具体实现。
主智能体获得的是经过筛选的、干净的信息，可以将其综合成全面的答案。

**实现生产就绪**
即使设计优雅的智能体，也需要生产基础设施来应对真实用户的实际使用。我们构建了模块化中间件来处理运营层面的问题，否则这些问题会污染我们的提示词。

```python
middleware = [
    guardrails_middleware, # 过滤无关查询
    model_retry_middleware, # API失败时重试
    model_fallback_middleware, # 必要时切换模型
    anthropic_cache_middleware # 缓存高成本调用
]
```

每一层的作用：
*   **护栏**过滤掉无关查询，使智能体专注于LangChain相关问题。
*   **重试中间件**优雅地处理临时的API故障，用户永远不会看到晦涩的错误信息。
*   **回退中间件**在模型不可用时，在Haiku、GPT-4o Mini和Gemini Nano之间切换。
*   **缓存**通过复用相同查询的结果来降低成本。

这些层对用户不可见，但对于可靠性至关重要。它们让智能体专注于推理，而基础设施则处理故障模式、成本优化和质量控制。

**将智能体交付给用户**
构建一个优秀的智能体只是成功了一半。另一半呢？以一种感觉快速且智能的方式将其交付给用户。

我们使用LangGraph SDK来处理流式传输和状态管理的所有复杂性。

**加载用户对话线程：**
当有人打开Chat LangChain时，我们使用LangGraph SDK获取他们的对话历史：

```typescript
const userThreads = await client.threads.search({
    metadata: { user_id: userId },
    limit: THREAD_FETCH_LIMIT,
})
```

每个线程都将用户ID存储在元数据中，因此对话在不同会话间保持私密性和持久性。LangGraph SDK会自动处理过滤。

**实时流式传输响应：**
当用户发送消息时，LangGraph SDK会在生成时流式传输响应：

```typescript
const streamResponse = client.runs.stream(threadId, "docs_agent", {
    input: { messages: [{ role: "user", content: userMessage }] },
    streamMode: ["values", "updates", "messages"],
    streamSubgraphs: true,
})

for await (const chunk of streamResponse) {
    if (chunk.event === "messages/partial") {
        setMessages(prev => updateWithPartialContent(chunk.data.content))
    }
}
```

用户看到的是：
三种流模式展示了智能体的整个思考过程：
*   `messages` — Token随着智能体书写逐步出现
*   `updates` — 工具调用揭示了智能体正在搜索什么
*   `values` — 处理后的最终完整状态

用户可以看到智能体思考、搜索文档、检查知识库，并逐个Token构建响应的过程。没有加载旋转图标。

**对话记忆**
在不同消息间传递相同的`thread_id`，LangGraph的检查点机制会处理其余一切。它存储对话历史，为每一轮检索上下文，并跨会话维护状态。我们设置了7天的TTL。仅此而已。

**成果**
自新系统上线以来，我们看到了显著的改进。

对于公开的Chat LangChain，用户能在15秒内获得带有精确引用的响应。他们可以立即验证答案，因为我们直接链接到相关的文档页面或知识库文章。我们也不再需要花费数小时重新索引——文档会自动更新。

在内部，我们的支持工程师使用**深度智能体**来处理最复杂的工单。它会搜索文档、交叉引用已知问题，并深入我们的私有代码库，以找到真正解释问题所在的实现细节。这个智能体并没有取代我们的工程师——而是增强了他们的能力，它负责研究，让工程师可以专注于解决问题。

**关键要点**
*   **遵循用户的工作流程**：不要重新发明轮子；自动化你最佳用户（或内部专家）已经在使用的成功工作流程。对于LangChain，这意味着复制检查文档、知识库和代码库的三步流程。
*   **评估向量嵌入是否合适**：对于产品文档和代码这类结构化内容，使用向量嵌入可能会破坏文档结构，导致引用模糊，并且需要不断重新索引。向量嵌入对于非结构化内容、较短文本块或聚类用例来说非常出色。
*   **赋予智能体直接访问结构的能力**：这种方法允许智能体通过API直接访问内容的现有结构。这使得智能体能够像人类一样，使用关键词和细化条件进行搜索。
*   **优先考虑推理而非检索**：设计工具以反映人类工作流程：先浏览文章标题再阅读内容，对于代码则使用模式匹配和目录导航。提示智能体在初始结果不明确时提出后续问题并优化其查询，确保最终答案覆盖用户的真实需求。
*   **使用深度智能体和子图来管理上下文**：对于复杂的多领域问题，使用具有专门子图的深度智能体可以防止主编排智能体被原始搜索结果淹没。每个子智能体在向上传递精炼的见解之前，仅从其领域中过滤和提取“黄金数据”。
*   **生产中间件的必要性**：即使是一个设计优雅的智能体，也需要健壮的基础设施才能可靠。为实现生产级的可靠性、成本优化和质量控制，实施用于护栏（过滤无关查询）、重试（API失败时）、回退（切换模型）和缓存的模块化中间件至关重要。

**下一步计划**
*   **公开代码库搜索（未来几天内推出）** — 当文档和知识库不足时，智能体将搜索我们的公共代码库以验证实现并引用确切的行号。

**亲自尝试**
Chat LangChain已在 chat.langchain.com 上线。使用 Claude Haiku 4.5 可获得最快的响应，或尝试 GPT-5 Mini 和 GPT-5 Nano 以查看不同模型的表现。

**加入讨论**
构建兼顾速度与深度的智能体很困难，我们仍在学习。如果你正在处理类似的问题，我们很乐意听听你的发现。
请在我们的论坛上加入LangChain社区或在Twitter上关注我们。
订阅我们的新闻通讯，获取团队和社区的最新动态。


> 本文由AI自动翻译，原文链接：[Why We Rebuilt LangChain’s Chatbot and What We Learned](https://blog.langchain.com/rebuilding-chat-langchain/)
> 
> 翻译时间：2026-01-06 01:14
