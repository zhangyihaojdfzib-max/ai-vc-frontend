---
title: 多图像推理准备好了吗？Visual Haystacks基准测试揭示LMM重大挑战
title_original: 'Are We Ready for Multi-Image Reasoning? Launching VHs: The Visual
  Haystacks Benchmark!'
date: '2024-07-20'
source: Berkeley AI Research (BAIR)
source_url: http://bair.berkeley.edu/blog/2024/07/20/visual-haystacks/
author: Ritwik Gupta
summary: 本文介绍了首个专注于视觉长上下文推理的基准测试Visual Haystacks（VHs），用于评估大型多模态模型在多图像问答任务中的能力。该基准包含单针和多针两种挑战，要求模型从海量图像中检索并整合视觉信息。实验发现，现有模型在面临大量视觉干扰项时性能显著下降，且难以进行跨图像推理，揭示了当前LMM在处理复杂视觉场景时的局限性。
categories:
- AI研究
tags:
- 多模态模型
- 视觉问答
- 基准测试
- 长上下文推理
- 人工智能评估
draft: false
translated_at: '2026-01-05T17:19:04.033Z'
---

人类擅长处理海量视觉信息，这项技能对于实现通用人工智能（AGI）至关重要。数十年来，人工智能研究者们开发了视觉问答（VQA）系统来解读单张图像中的场景并回答相关问题。尽管基础模型的最新进展显著缩小了人类与机器视觉处理能力之间的差距，但传统VQA系统一直被限制在每次只能对单张图像进行推理，而无法处理整个视觉数据集合。

这一局限在更复杂的场景中带来了挑战。例如，在医学影像集合中识别模式、通过卫星图像监测森林砍伐、利用自主导航数据绘制城市变化图、分析大型艺术收藏的主题元素，或是从零售监控录像中理解消费者行为。这些场景不仅需要对成百上千张图像进行视觉处理，还需要对这些发现进行跨图像的整合处理。为弥补这一不足，本项目聚焦于"多图像问答"（MIQA）任务，该任务超越了传统VQA系统的能力范围。

**Visual Haystacks**：首个"以视觉为中心"的"大海捞针"（NIAH）基准测试，旨在严格评估大型多模态模型（LMMs）处理长上下文视觉信息的能力。

**如何对VQA模型进行MIQA基准测试？**
"大海捞针"（NIAH）挑战最近已成为评估LLM处理包含"长上下文"（即大量输入数据，如长文档、视频或数百张图像）输入能力最流行的范式之一。在此任务中，包含特定问题答案的关键信息（"针"）被嵌入海量数据（"干草堆"）中。系统必须检索相关信息并正确回答问题。

首个用于视觉推理的NIAH基准测试由谷歌在Gemini-v1.5技术报告中提出。在该报告中，他们要求模型从长视频的单个帧中检索叠加的文本。事实证明，现有模型在此任务上表现相当出色——主要归功于其强大的OCR检索能力。但如果我们提出更多视觉问题呢？模型是否仍能保持良好表现？

**什么是Visual Haystacks（VHs）基准测试？**
为评估"以视觉为中心"的长上下文推理能力，我们推出了"Visual Haystacks（VHs）"基准测试。这一新基准旨在评估大型多模态模型（LMMs）在大型非相关图像集合中进行视觉检索和推理的能力。VHs包含约1,000个二元问答对，每个集合包含1到10,000张不等的图像。与之前专注于文本检索和推理的基准不同，VHs的问题核心在于识别特定视觉内容（如物体）的存在，使用的是COCO数据集中的图像和标注。

VHs基准测试分为两大挑战，每个挑战都旨在测试模型在回答查询前准确定位和分析相关图像的能力。我们精心设计了数据集，以确保猜测或依赖常识推理而不查看图像不会获得任何优势（即在二元QA任务中准确率仅为50%）。

**单针挑战**：在图像干草堆中仅存在一张针图像。问题形式为："对于包含锚点物体的图像，是否存在目标物体？"
**多针挑战**：在图像干草堆中存在二到五张针图像。问题形式为："对于所有包含锚点物体的图像，它们是否都包含目标物体？"或"对于所有包含锚点物体的图像，其中是否有任何一张包含目标物体？"

**VHs揭示的三个重要发现**
Visual Haystacks（VHs）基准测试揭示了当前大型多模态模型（LMMs）在处理大量视觉输入时面临的重大挑战。在我们针对单针和多针模式的实验中，我们评估了包括LLaVA-v1.5、GPT-4o、Claude-3 Opus和Gemini-v1.5-pro在内的多种开源和专有方法。此外，我们还引入了一个"图像描述"基线方法，采用两阶段流程：首先使用LLaVA为图像生成描述，然后使用Llama3根据描述文本内容回答问题。以下是三个关键发现：

**难以应对视觉干扰项**
在单针设置中，尽管保持了较高的理想准确率（oracle accuracy），但随着图像数量增加，模型性能显著下降——这一现象在之前基于文本的Gemini式基准测试中并未出现。这表明现有模型可能主要受困于视觉检索，尤其是在存在挑战性视觉干扰项的情况下。此外，必须强调开源LMMs（如LLaVA）的限制，由于其2K上下文长度限制，最多只能处理三张图像。另一方面，专有模型如Gemini-v1.5和GPT-4o，尽管声称具备扩展上下文能力，但当图像数量超过1K时，常因API调用的有效载荷大小限制而无法处理请求。

*（图表说明：单针问题在VHs上的性能表现。随着干草堆大小（N）增加，所有模型性能均显著下降，表明它们都无法有效抵抗视觉干扰项。E：超出上下文长度。）*

**跨多图像推理困难**
有趣的是，与将图像描述模型（LLaVA）与LLM聚合器（Llama3）串联的基础方法相比，所有基于LMM的方法在单图像QA（5+图像）和所有多针设置中均表现不佳。这种差异表明，虽然LLMs能够有效整合长上下文描述，但现有的基于LMM的解决方案不足以处理和整合跨多个图像的信息。值得注意的是，在多图像场景中性能急剧恶化，Claude-3 Opus即使在仅使用理想图像时也表现较弱，而Gemini-1.5/GPT-4o在更大的50张图像集合中准确率降至50%（如同随机猜测）。

*（图表说明：多针问题在VHs上的结果。所有具备视觉感知的模型表现均不佳，表明模型在隐式整合视觉信息方面面临挑战。）*

**视觉领域的特有现象**
最后，我们发现LMMs的准确率极大程度上受针图像在输入序列中位置的影响。例如，当针图像紧接在问题之前时，LLaVA表现更好，否则性能下降高达26.5%。相反，专有模型通常在图像位于开头时表现更好，否则性能下降高达28.5%。这种模式呼应了自然语言处理（NLP）领域中观察到的"迷失在中间"现象，即位于上下文开头或结尾的关键信息会影响模型性能。这一问题在先前仅需文本检索和推理的Gemini式NIAH评估中并不明显，凸显了我们的VHs基准测试带来的独特挑战。

*（图表说明：针图像位置与不同图像设置下VHs性能的关系。当针图像未处于理想位置时，现有LMMs性能下降高达41%。灰色框：超出上下文长度。）*

**MIRAGE：基于RAG的解决方案以提升VHs性能**
基于以上实验结果，可以清楚地看到现有解决方案在MIQA中的核心挑战在于：（1）能够从大量可能不相关的图像池中准确检索相关图像，且不受位置偏差影响；（2）整合来自这些图像的相关视觉信息以正确回答问题。

为解决这些问题，我们引入了一种开源且简单的单阶段训练范式——“MIRAGE”（多图像检索增强生成），该范式扩展了LLaVA模型以处理多图像问答任务。下图展示了我们的模型架构。
我们提出的范式包含多个组件，每个组件都旨在缓解多图像问答任务中的关键问题：
- **压缩现有编码**：MIRAGE范式利用查询感知压缩模型，将视觉编码器的Token减少到一个更小的子集（缩小10倍），从而允许在相同上下文长度内容纳更多图像。
- **采用检索器过滤无关信息**：MIRAGE使用一个与LLM微调同步训练的检索器，来预测图像是否相关，并动态丢弃不相关的图像。
- **多图像训练数据**：MIRAGE利用多图像推理数据以及合成的多图像推理数据，对现有的单图像指令微调数据进行增强。

**结果**
我们使用MIRAGE重新评估了VHs基准。除了能够处理1K或10K张图像外，尽管其单图像问答骨干模型较弱（每张图像仅使用32个Token），MIRAGE在大多数单针任务上仍实现了最先进的性能。
我们还在各种视觉问答任务上对MIRAGE和其他基于LMM的模型进行了基准测试。在多图像任务中，MIRAGE展现出强大的召回和精确度能力，显著优于GPT-4、Gemini-v1.5和大型世界模型等强劲竞争对手。此外，它也显示出具有竞争力的单图像问答性能。
最后，我们将MIRAGE协同训练的检索器与CLIP进行了比较。我们的检索器在保持效率的同时，性能显著优于CLIP。这表明，虽然CLIP模型可以是开放词汇图像检索的良好检索器，但在处理类似问题的文本时可能效果不佳！

**最终评述**
在这项工作中，我们开发了Visual Haystacks（VHs）基准，并识别出现有大型多模态模型的三个普遍缺陷：
- **难以处理视觉干扰项**：在单针任务中，随着图像数量增加，LMMs的性能急剧下降，这表明在过滤无关视觉信息方面存在重大挑战。
- **难以跨多图像进行推理**：在多针场景中，像先生成描述再进行基于语言的问答这类简单方法，其表现优于所有现有的LMMs，突显了LMMs在处理跨多图像信息方面的能力不足。
- **视觉领域中的现象**：无论是专有模型还是开源模型，都对针信息在图像序列中的位置表现出敏感性，在视觉领域呈现出一种“中间损失”现象。
为此，我们提出了MIRAGE，一个开创性的视觉检索增强生成器框架。MIRAGE通过创新的视觉Token压缩器、协同训练的检索器以及增强的多图像指令调优数据来应对这些挑战。
在阅读完这篇博文后，我们鼓励所有未来的LMM项目在部署前，使用Visual Haystacks框架对其模型进行基准测试，以识别并纠正潜在的缺陷。我们也敦促社区探索多图像问答，以此作为推进真正通用人工智能前沿的手段。
最后但同样重要的是，请查看我们的项目页面和arXiv论文，并点击我们GitHub仓库的星标按钮！

@article{wu2024visual,
title={Visual Haystacks: Answering Harder Questions About Sets of Images},
author={Wu, Tsung-Han and Biamby, Giscard and and Quenum, Jerome and Gupta, Ritwik and Gonzalez, Joseph E and Darrell, Trevor and Chan, David M},
journal={arXiv preprint arXiv:2407.13766},
year={2024}
}.

> 本文由AI自动翻译，原文链接：[Are We Ready for Multi-Image Reasoning? Launching VHs: The Visual Haystacks Benchmark!](http://bair.berkeley.edu/blog/2024/07/20/visual-haystacks/)
> 
> 翻译时间：2026-01-05 17:19
