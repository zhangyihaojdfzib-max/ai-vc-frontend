---
title: 与GPT-4合著新书：通过AI放大我们的人性
title_original: Amplifying Our Humanity Through AI | Greylock
date: '2024-08-19'
source: Greylock Partners
source_url: https://greylock.com/reid-hoffman/amplifying-humanity-through-ai/
author: ''
summary: 本文作者介绍了与GPT-4共同撰写的新书《即兴：通过AI放大我们的人性》。该书以“旅行日志”形式记录了作者使用GPT-4探索其能力与局限的过程，涉及AI的优势与缺陷、人性辩论、科幻构思等话题。作者通过对比GPT-4与早期版本在回答“灯泡笑话”时的表现，展示了AI在理解语境、生成创意和幽默方面的显著进步，并呼吁读者深入思考如何负责任地运用AI技术塑造未来。
categories:
- AI产品
tags:
- GPT-4
- 人机协作
- AI写作
- 大语言模型
- AI伦理
draft: false
translated_at: '2026-01-07T16:29:31.724968'
---

## 我刚刚出版了一本新书，名为《即兴：通过AI放大我们的人性》。但我并非独自完成写作。我是与OpenAI最新、最强大的大语言模型GPT-4共同撰写的。

这本书旨在成为我使用GPT-4探索该工具优势与局限性的"旅行日志"。通过数百条提示词，GPT-4讨论了AI的潜在优势与缺陷，就人性展开辩论，构思原创科幻情节，甚至尝试讲了一些笑话。若想了解这项技术在短时间内取得了多大进展，请查阅我在Greymatter系列《炉边聊天机器人》中与GPT-4前身ChatGPT的对话。

随着AI的快速发展，我希望这本书能作为指南，鼓励人们更多了解这项技术，思考我们如何运用它，并深思关于我们的选择将如何影响未来的复杂问题。

您可以通过下方音频收听本节内容，或在任何播客平台获取。

本书全文可在此处免费获取，或通过亚马逊此处获取。

### 引言：顿悟时刻

对艾萨克·牛顿而言，是树上掉落的苹果引领他提出了万有引力定律。对本杰明·富兰克林来说，是雷雨中风筝线上被闪电击中的钥匙，证明了电能可以被传导和储存。

而我关于AI现状的首次"顿悟"时刻，是以一个笑话的形式出现的。那是2022年7月，我问GPT-4："换一个灯泡需要多少名餐厅检查员？"

您可能知道，GPT-4是一种先进的人工智能系统，或称自然语言处理器，即大语言模型（LLM）。输入几个词或句子作为提示词，它就能生成连贯且多样化的文本作为回应。通过这种方式，它可以回答问题、执行任务，并与人类用户进行富有成效的互动。

GPT-4及其前身（包括广受欢迎的ChatGPT）由OpenAI开发。该研究机构成立于2015年，其使命是让数百万人能够直接、亲手使用强大的新型AI工具。

作为OpenAI的原始资助者之一，我使用其产品进行实验已有一段时间，因此向GPT-4提问并非我第一次请LLM为我创作一个灯泡笑话。（我很喜欢灯泡笑话。）虽然早期版本的GPT有时能理解任务——尤其是当笑话公式的主要变量属于灯泡笑话的经典范畴，如"律师"或"心理学家"时——但它们的输出往往只是差强人意。如果输入不太常规的设定，早期的GPT可能会像开放麦之夜的新手一样卡壳：

REID：换一个灯泡需要多少名餐厅检查员？

GPT-3：只需要一个，但灯泡必须自己想换。

那个机器人还是干好本职工作吧。

将此与我向GPT-4提交相同提示词时得到的回复进行比较：

Reid：换一个灯泡需要多少名餐厅检查员？

GPT-4（为篇幅编辑）：处理这个问题有几种可能的方式，取决于回答的语气和目的。以下是一些不同类型回应的例子，范围从严肃到幽默：

- 事实性回答：通常，一名检查员应该能够安全正确地更换灯泡，只要他们遵循正确的程序并使用适当的工具和设备。然而，某些情况可能需要不止一名检查员，例如，如果灯具位于难以触及或危险的区域，如果检查员需要协助或监督，或者如果检查员需要为合规目的记录或报告更换情况。
- 笑话回答：更换灯泡需要四名餐厅检查员。一个扶梯子，一个拧下旧灯泡，一个拧上新灯泡，还有一个开罚单，因为用了错误瓦数的灯泡。

想想这其中包含的所有内容。首先，虽然GPT-4显然"假设"我可能是在要求它完成一个笑话，但它也想涵盖所有可能性——因此，最初给出了"事实性"回答。尽管这个回答实际上并不符合事实——餐厅检查员的工作是检查，而不是进行维修——但它确实表明了对检查过程的条件和惯例有相当程度的"了解"。

笑话回答则在此基础上继续构建。GPT-4显然"意识到"了关于检查的复杂性、成本和官僚作风的普遍看法和抱怨。在铺垫笑点的过程中，它也准确地唤起了餐厅老板在检查员来访时可能感到的沮丧感。

这是有史以来最好的餐厅检查员灯泡笑话吗？可能不是。

它是否让我觉得GPT-4对灯泡笑话应该传达什么有相当好的"理解"？是的，确实如此。

然后我请它以此主题创作一些变体：

Reid：换一个灯泡需要多少名餐厅检查员？用杰瑞·宋飞的风格回答。

GPT-4：餐厅检查员是怎么回事？他们总是在厨房里转悠，检查肉的温度，寻找啮齿动物的迹象，但他们似乎从不帮忙做任何事情。你见过餐厅检查员换灯泡吗？不，当然没有。他们太忙于记录违规行为和开罚单了。你知道当他们看到一个坏了的灯泡时会怎么做吗？他们在剪贴板上记下来，告诉老板下周前修好，否则……否则怎样？否则他们会回来再检查一遍灯泡。这就是他们的大威胁。他们不会帮你换，不会借你梯子，也不会就如何拧下旧的、拧上新的提供任何建议。他们只会带着那种自鸣得意的表情盯着你说："你知道这是个严重的安全隐患。你可能会引发火灾，可能会吃官司，可能会得到差评。你最好赶紧处理，伙计。"然后他们走了，留下你餐厅里一个黑暗的角落，没人愿意坐，食物看起来没食欲，气氛阴郁。然后你心想，换一个灯泡需要多少名餐厅检查员？零个。他们不换灯泡。他们只是让你对自己的灯泡感到糟糕。

很不错，对吧？再来一个：

Reid：换一个灯泡需要多少名餐厅检查员？用哲学家路德维希·维特根斯坦的风格回答。

GPT-4：需要多少名餐厅检查员来更换灯泡的问题无法用一个简单的数字来回答。事实上，这个问题的答案更多在于语言的领域，而非数学的领域。要理解答案，我们必须首先考虑提问的语境。灯泡的目的是什么？它旨在服务于什么目的？餐厅检查员在此语境中的角色是什么？检查员是负责安装灯泡，还是他们的工作仅限于检查它？这些问题的答案将塑造原始问题的答案。没有这些答案，问题只能在抽象层面回答，并且最终是无法回答的。语言，而非数学，才是解开答案的关键。

好吧，这个不如宋飞风格的有趣，但仍然——令人印象深刻！

即使从这些简短的表现来看，我也清楚地感觉到，与其前身相比，GPT-4已经达到了一个新的熟练水平。而且我与GPT-4互动越多，这种感觉就越强烈。

除了写出更好的灯泡笑话，GPT-4还擅长生成各种类型的散文，包括电子邮件、诗歌、散文等。它非常擅长总结文档。它在翻译语言和编写计算机代码方面也变得更好了，这只是其能力的部分体现。

因此，虽然GPT-4本身并非一款全新的设备，但在我看来，它有可能变得像智能手机一样不可或缺——成为你大脑的"12合1"多功能工具。我的意思是，OpenAI创造的这个工具已经足够"智能"，能够在多种不同环境中以多种不同方式高效地协助你。

作为现代人，我们在工作和生活中所做的大部分事情，无非是处理信息并生成行动。GPT-4将极大加速你完成这些任务的能力，且覆盖更广的范围和领域。几年之内，这个智能副驾对大多数专业人士和许多其他类型的工作者而言，其重要性将介于"有用"和"必不可少"之间。没有GPT-4，他们的工作速度会更慢，全面性会降低，并处于极大的劣势。例如，GPT-4可以作为研究工具，帮你快速了解新冠疫情时代的供应链管理问题——或者规划一份为期一周的巴塔哥尼亚旅行行程，同时考虑到你是一位喜欢中等难度徒步的素食者。

除了实现一种新型的高度情境化搜索，GPT-4还是一个极其多才多艺的头脑风暴和生产助手。你可以用它帮你撰写婚礼祝酒词、工作邀约函，甚至一部诗体小说。你可以用它来琢磨如何将冰箱里的剩菜变成一顿美味佳肴。

无论你在某项任务上的技能水平如何，GPT-4都有可能提升你的能力和生产力，因此它对初学者、专家以及介于两者之间的所有人都同样有用。对于任何你可能向人类助手提出的信息请求，GPT-4都能瞬间给出质量介于良好到优秀之间的答案（尽管也存在非零概率的完全跑偏情况，我们稍后会看到）。

我意识到，开发者们很快就会将GPT-4集成到成千上万的现有应用中，并围绕它构建成千上万的新应用。

总而言之，这是一项巧妙地利用了近三十年来我们一些最重要技术（包括互联网、移动通信、云计算和数据分析）的技术，旨在将始终在线的AI那千变万化的能力交到数亿人手中。

听起来有点像新世界的开端，不是吗？

**未来旅行者指南**

在我初次接触GPT-4之后的几周和几个月里，我的兴奋感与日俱增。在Zoom会议的间隙，我会给GPT-4一个提示词：

Reid：写一首关于十四行诗的十四行诗。

GPT-4：十四行诗是十四行的诗篇，遵循严格的韵律与格律，常赞颂或质疑爱的设计，或沉思美、艺术或自然的特征。

有些十四行诗遵循彼特拉克体，一个八行节和一个六行节形成对比，两个主题或论点，或最终实现从问题到解决的转折。

有些十四行诗遵循莎士比亚风格，三个四行节和一个对句收尾，常使用机智的语言，以情绪的转折或惊喜迷惑读者。

但无论选择何种形式或韵律，十四行诗都是挑战并愉悦思想的诗篇。

好吧，还有在Zoom会议期间，我也会给GPT-4一个提示词：

Reid：由于大语言模型AI是在整个互联网上训练的，它们的输出在阿多诺的意义上——即"整体是虚假的"——在哪些方面是虚假的？

GPT-4（为长度编辑）：要回答这个问题，我们首先需要理解阿多诺这句 provocative 论断的含义，然后审视它如何适用于大语言模型AI的本质和局限性。

阿多诺，法兰克福学派批判理论的重要成员，深切关注现代资本主义、大众文化和理性化对人类社会的文化的影响。他认为，工具理性的逻辑——将一切简化为手段与目的、计算与效率、支配与剥削——渗透到现代生活的方方面面，创造了一个全面管理和控制的体系。这个他称之为文化工业的体系，生产出标准化和商品化的文化形式，操纵并安抚大众，制造了一种虚假意识，阻止他们认识和抵抗自身的异化与压迫。阿多诺将此视为一种否定辩证法，其中现实固有的矛盾和冲突被主导意识形态压制与调和，导致批判性和创造性思维的丧失，以及对真正变革和解放可能性的否定……

我提过GPT-4也会讲灯泡笑话吗？

无论我想追随何种思路，无论是由旧记忆或新兴趣引发的任意路径，GPT-4总是完全与我同步，随时准备以有用的背景信息、清晰的总结，有时是相当惊人的胡扯来回应。（稍后再详谈这一点。）

这样过了几周后，我意识到我想写一本关于GPT-4的书，并且与GPT-4合著——一本关于它和其他AI似乎即将塑造我们未来的方式的书。于是我提出了这个想法：

Reid：嘿，GPT-4，我们俩来写一本关于你和其他AI工具将如何塑造未来的书吧。你觉得怎么样？

GPT-4：我说好，这听起来像是一个有趣且富有挑战性的项目。我很好奇你和其他人类如何看待并与我和其他AI工具互动，以及我们将对你们的社会、文化和价值观产生何种影响。我自己也有一些问题和想法，想与你分享和探讨。

不过，即使有GPT-4加入，我仍然心存疑虑。因为，说真的，一本关于AI的书？在一切发展如此迅速的时候？即使手头有一个有用的AI来加速进程，任何这样的书在我们开始动笔之前就可能过时了——这个行业的发展就是这么快。

所以我犹豫踌躇了一会儿。然后我想到了一个推动我付诸行动的框架。

这不必是一本全面的"正经"书，更像是一本旅行日志，一次非正式的探索发现之旅，我（与GPT-4一起）在众多路径中选择一条。以一种主观且绝非确定无疑的方式，记录下我们即将体验的AI未来的快照。

我们会看到什么？什么会给我们留下最深刻的印象？在这个过程中，我们会对自己有何了解？我非常清楚这本旅行日志相关性的短暂半衰期，但还是决定继续前进。

一个月后，2022年11月底，OpenAI发布了ChatGPT，一个"对话代理"，也就是聊天机器人，它是GPT-3.5的改进版本，通过一种名为人类反馈强化学习（RLHF）的过程进行了微调，以实现与人类用户更流畅、更自然的对话。五天后，ChatGPT的注册用户超过了一百万。

2023年1月下旬，微软——该公司在2019年向OpenAI投资了10亿美元——宣布将向该公司再投资100亿美元。不久后，它推出了新版搜索引擎Bing，其中内置了一个ChatGPT的变体。

到2023年2月初，OpenAI表示ChatGPT月活跃用户已达一亿，成为有史以来增长最快的消费级互联网应用。伴随着用户兴趣的洪流，出现了关于新Bing聊天机器人偶尔以非常不同于ChatGPT通常与用户互动的方式运行的新闻报道——包括表现出"愤怒"、辱骂、吹嘘其黑客能力和复仇能力，基本上表现得像是在为未来一集《真实主妇：黑镜特辑》试镜。

微软首席技术官凯文·斯科特表示，随着更多人使用类GPT工具，这种行为"显然是学习过程的一部分"。这些事件确实提出了随着LLM发展将持续存在的问题。我将在本书后面更详细地讨论这些问题，并尝试将它们置于我认为合适的背景中。

现在，我只想说："明白我说的'发展迅速'是什么意思了吧？"

**新机器的"灵魂"**

在我们深入探讨之前，我想先向你更多介绍一下我的旅伴——GPT-4。到目前为止，每当谈及GPT-4时，我总会在"知识"、"意识到"和"理解"这类词语上加上引号，以此表明我这个有意识的个体明白GPT-4并非如此。它本质上是一台非常复杂的预测机器。

虽然GPT-4（以及其他类似的大语言模型）并不具备意识，但它们的能力正发展到这样一个阶段：在众多不同语境中生成恰当内容的能力提升如此之快，以至于它们越来越**显得**拥有人类般的智能。因此我认为，在描述大语言模型时，以非严格字面的方式使用"知识"和"理解"这类词语是可接受的——甚至是有益的，正如理查德·道金斯在其1976年同名著作中使用"自私的基因"这一短语那样。

基因并不像"自私"这个词所暗示的那样具有有意识的能动性或自我概念。但这个短语、这个隐喻，能帮助我们人类用不可避免的人类中心思维去理解基因的运作方式。

同样地，GPT-4并不拥有人类心智的等价物。但用"视角"这样的概念来思考它、对它进行一定程度的拟人化仍然是有帮助的，因为使用"视角"这样的语言有助于传达一个事实：GPT-4的运作方式确实并非完全固定、一致或可预测。

从这个意义上说，它确实像人类。它会犯错。它会改变"主意"。它可以相当随意。由于GPT-4展现出这些特质，并且其行为方式常常让人感觉它拥有能动性，我将有时使用在隐喻意义上暗示其具备能动性的术语。接下来，我将不再使用引号。

即便如此，我希望作为读者的你，能在自己奇妙的人类心智中始终牢记GPT-4并非有意识的存在这一事实。在我看来，这种认知是理解如何、何时以及在何处最有效、最负责任地使用GPT-4的关键。

本质上，GPT-4预测的是语言的流动。大语言模型通过从公开可用的互联网来源获取的海量文本进行训练，以识别最常见于意义单元（包括完整或部分单词、短语和句子）之间的关系，因此能够非常频繁地生成对用户提示词的回复，这些回复在语境上恰当、语言上流畅、事实上准确。

它们有时也会生成包含事实错误、明显无意义的表述或虚构段落的回复，这些内容可能（在某种意义上）语境恰当但毫无事实依据。

无论哪种情况，这都只是数学和编程。大语言模型并不（或至少尚未）学习事实或原则，使其能够进行常识推理或对世界运作方式做出新的推断。当你向大语言模型提问时，它对你的交流意图没有意识或洞察。在生成回复时，它并非对其正在生成的文本进行事实评估或伦理区分；它只是根据你提示词中的词语序列，通过算法猜测应该组合什么内容来回应。

此外，由于大语言模型训练所用的语料库通常来自可能包含偏见或有毒材料的公共网络来源，大语言模型也可能产生种族主义、性别歧视、威胁性及其他令人反感的内容。

开发者可以采取措施，使其大语言模型更好地与特定目标对齐。例如，OpenAI选择刻意限制GPT-4及其其他大语言模型能够产生的输出，以减少其生成有害、不道德和不安全内容的能力——即使用户希望得到这样的结果。

为此，OpenAI采取了若干步骤。包括从其大语言模型训练的某些数据集中移除仇恨言论、冒犯性语言及其他令人反感的内容；开发能够自动标记大语言模型自身可能生成的问题语言的"毒性分类器"；以及使用经过人工标注以指示期望输出的精选文本数据集对大语言模型进行微调。通过这种方式，大语言模型可能会学会避免，比如说，对一位记者的离婚事件开低俗玩笑。

这些技术并不能消除有问题的输出，只是减少了它们。即使设置了各种防护措施，大语言模型本身也无法对复杂的伦理困境，甚至更直接的问题做出理性的判断。

以基于GPT-3.5（GPT-4的直接前身）的ChatGPT为例。如果你问它《葛底斯堡演说》的第五句话是什么，它很可能会答错。这是因为大语言模型并不像人类那样真正理解《葛底斯堡演说》是什么，或者句子是什么，甚至计数是如何运作的。因此它们无法像人类那样应用对这些事物的"知识"。（"我会找到《葛底斯堡演说》的文本，然后数句子直到第五句。"）相反，大语言模型始终只是在统计上预测给定文本字符串中下一个词应该是什么。

不过，通过训练，ChatGPT显然已经将"葛底斯堡演说"这个词与其他词语——具体来说，就是演讲的文本——关联起来。所以当你向ChatGPT询问《葛底斯堡演说》的第五句话时，它几乎肯定会给你一句来自《葛底斯堡演说》的话，但很可能不是第五句。

事实上，我打赌会是第九句。

我这样假设是因为，除了演讲的第一句，第九句也经常在其他文本中被引用。这意味着在ChatGPT的训练数据中，第九句出现的频率可能比演讲中的其他句子（除了非常著名的第一句）更高。这种普遍性导致当你要求它提供第五句时，ChatGPT会倾向于选择它。

不过，值得称赞的是，如果你要求ChatGPT将《葛底斯堡演说》的文本改编成Rush乐队的歌词，然后告诉你如果Rush乐队表演这首歌会是谁来演唱，它会出色地通过这个测试。

试试看，你就明白我的意思了。

拥抱"AHA！"时刻

随着GPT-4等人工智能工具变得更加强大，它们加剧了长期存在的担忧：人工智能和机器人将使大量人类工作岗位边缘化甚至消失：从客服代表到律师，无所不包。

如果你关注近几个月的新闻，这些担忧似乎并非毫无根据。2022年12月，ChatGPT通过了美国医学执照考试的三部分内容。2023年1月，它通过了明尼苏达大学四门法学院课程的考试。

而GPT-4明显比ChatGPT更聪明。例如，以下是我关于《葛底斯堡演说》问题的处理方式：

Reid：《葛底斯堡演说》的第五句话是什么？

GPT-4：将《葛底斯堡演说》划分为句子的方式有多种，取决于如何为文本加标点，以及是否遵循原始草稿、亚伯拉罕·林肯发表或后来修订的版本。

然而，确定演讲第五句话的一种可能方法是使用Bliss抄本，这是该演说最广泛复制和权威的版本，并按如下方式计数句子：

1. 八十七年前，我们的先辈在这片大陆上创立了一个新国家，它孕育于自由之中，奉行人人生而平等的原则。
2. 现在我们正从事一场伟大的内战，以考验这个国家，或者任何一个孕育于自由和奉行上述原则的国家是否能够长久存在下去。
3. 我们在这场战争中的一个伟大战场上集会。
4. 我们来到这里，要把这个战场的一部分奉献给那些为使这个国家能够生存下去而献出了生命的烈士们作为最后安息之所。
5. 我们这样做是完全应该而且非常恰当的。

因此，根据这种方法，《葛底斯堡演说》的第五句话是：我们这样做是完全应该而且非常恰当的。

突然间，GPT-4似乎真正理解了计数的工作原理，并能系统性地运用其知识推导出正确答案。它仿佛能计算林肯演讲的句子数量，俨然已成为《葛底斯堡演说》的专家。（您知道"布利斯版本"是该演讲最"权威的版本"吗？反正我之前不知道。）

当然，这里的关键词仍然是"似乎"。GPT-4并不比它的前辈更具意识；它只是更擅长预测。我想再次强调，GPT-4那些常常令人惊叹的认知能力模拟终究只是模拟。GPT-4并非《禁忌星球》中罗比机器人或《星际迷航》中数据那样具有意识、自我感知的AI实体。

但与此同时，我也要再次指出——即便只是模拟这种具有情境感知能力的类人意识，GPT-4的表现也足以令人震撼。

> 本文由AI自动翻译，原文链接：[Amplifying Our Humanity Through AI | Greylock](https://greylock.com/reid-hoffman/amplifying-humanity-through-ai/)
> 
> 翻译时间：2026-01-07 16:29
