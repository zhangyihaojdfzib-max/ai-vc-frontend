---
title: 更好的模型未必更好：AI的“对错”困境与未来
title_original: Are better models better? — Benedict Evans
date: '2025-01-22'
source: Benedict Evans
source_url: https://www.ben-evans.com/benedictevans/2025/1/the-problem-with-better-models
author: Benedict Evans
summary: 本文探讨了当前AI模型评估的复杂性。作者指出，虽然基准测试和主观任务能体现模型“更好”，但许多实际任务（如数据查询）只有对错之分。当用户无法验证答案时，概率性模型提供的“近似正确”可能毫无价值。文章分析了生成式AI在软件开发、营销等容错领域的成功，以及在需要确定性答案场景中的局限。最后，作者提出核心问题：模型改进能否解决根本的“理解”缺陷，以及未来应通过工程控制模型，还是让模型控制工具链。
categories:
- AI研究
tags:
- 大语言模型
- 模型评估
- AI局限性
- 确定性系统
- 生成式AI
draft: false
---

更好的模型就更好吗？
每周都有新模型、新方法、新事物涌现。每周都有人问我：“你试过o1 Pro吗？Phi 4呢？Midjourney 6.1用了吗？”我一直在想：到底该怎么判断？

当然，一种方法是看基准测试。但暂且不论这些测试的意义有多大，它们并不能告诉我现在能做什么以前做不到或做不好的事。你也可以准备一个装满精心设计逻辑谜题的文本文件来测试，但这本质上只是自建基准——同样地，这又能说明什么呢？

更实际的做法是用自己的工作流来测试。这个模型表现更好吗？然而这里会遇到一个问题：有些任务中，更好的模型确实能产出更优质、更准确的结果；但另一些任务里，不存在所谓“更好”或“更准确”，只有对与错。

有些问题本就没有“错误”答案：输出质量是主观的，“更好”是一个光谱。这是同一提示词在Midjourney 3、4、5和6.1版本上的效果。确实更好了！

同样，有些任务的错误很容易发现和修正。如果你让ChatGPT起草邮件或提供烹饪灵感，它可能出错，但你能看出来并改正。

因此，生成式AI早期展现出明确且强劲产品市场契合的两个领域是软件开发和市场营销：错误通常易于发现（或测试），且不一定存在错误答案。如果我需要几百字的新产品或品牌文案，可能没有“错误”答案；如果是你自己的产品，你还能发现错误——这仍然极具价值。我过去常将上一波机器学习比作“无限实习生”。如果有100个实习生，你可以分配一堆工作，虽然需要检查结果且部分结果可能不佳，但这仍远比自己从零完成所有工作要好。

然而，还有一大类我们渴望自动化的工作：枯燥耗时、传统软件无法处理，且结果质量非百分比而是二元判断的任务。对某些任务而言，答案没有好坏之分，只有对错之别。

如果我需要处理那些答案可能在重要层面绝对错误的任务，而我又非该领域专家，或无法记诵所有基础数据，必须亲自重复所有工作来验证——那么目前我完全无法使用LLM处理此类任务。

举个我常遇到、希望自动化的实际例子：我问ChatGPT 4o“1980年美国有多少电梯操作员？”美国人口普查局收集并公布了该数据：答案是21,982人（PDF第17页）。

首先，我直接提问，得到了具体但无来源的错误答案。接着我提供原始资料辅助，却得到另一个错误答案，还附带了来源列表（确实是美国人口普查局资料），第一个链接指向正确的PDF……但数字仍是错的。嗯，试试直接上传PDF？不行。明确告知查找位置？不行。让它联网搜索？还是不行。

这里的问题不仅是数字错误，更在于我若不亲自完成所有工作就无从判断。答案可能正确，换种提问方式可能更接近正确，付费升级Pro版或许正确率更高——但我需要的不是“可能更正确”的答案，尤其在我无法判断时。我需要的是正确的答案。

当然，这些模型不追求“正确”。它们是概率性统计系统，告诉你一个“好答案”可能长什么样；而非确定性系统，直接告诉你答案是什么。它们不“知道”也不“理解”——只是近似。“更好”的模型近似程度更高，可能在某类问题上表现突飞猛进（尽管我们未必知道原因甚至问题类别）。但这仍不等于提供“正确”答案，不等于模型“知道”或“理解”应该查找标有“1980”的列和“电梯操作员”的行。

这种情况会如何改变、是否改变，是今年乃至十年内核心争论的一部分：关于这些模型是否会持续扩展，甚至关于AGI。唯一能确定的是，我们尚无理论框架能给出答案。我们不知道。也许“理解”会随模型规模扩大自发涌现；也许如芝诺悖论所示，模型永远无法抵达终点，但能收敛到99.99%的正确率，那么“理解”与否并不重要；也许需要其他未知的理论突破；也许OpenAI的o3“推理”能力是解决路径，也许不是。众说纷纭，但迄今我们仍不知道。目前，“错误率”（如果这真是合适的思考角度）并非通过简单工程优化就能弥补的差距，不像iPhone添加复制粘贴功能或拨号上网被宽带取代那样。就我们所知，这是该技术的根本属性。

这引发了几类问题：

具体而言，当前大多数用生成式AI创业、希望自动化大公司枯燥后台流程的人，正将生成式AI模型作为API调用封装在传统确定性软件中。他们通过工具、流程、控制、用户体验以及预处理和后处理来管理错误率（以及聊天机器人自身的用户体验差距，我在其他文章多次探讨）。他们给马套上挽具、戴上眼罩、握紧缰绳，因为这是获得可预测结果的唯一方式。

但随着模型改进，它们可能走向技术栈顶端。LLM告诉SAP该执行哪些查询，用户或许能查看并验证过程，但现在你用概率系统控制确定性系统。这是理解“智能体”系统（可能成为下一个风口，也可能半年后被遗忘）的一种视角——LLM将其他一切转化为API调用。哪种方式更好？是在可预测框架内控制LLM，还是给LLM提供可预测的工具？

这引出了第二类问题。对我“电梯操作员”问题的有效批评，不在于我提示词有误或用错了模型版本，而在于我本质上试图用非确定性系统处理确定性任务。我像使用SQL那样使用LLM：但LLM不是SQL，也不擅长此事。如果你在Claude上尝试我的电梯问题，它会直白指出这属于特定信息检索问题，很可能产生幻觉，并拒绝尝试。这是化劣势为优势：LLM极不擅长判断自己是否错误（确定性问题），但非常擅长判断自己可能出错（概率性问题）。

“颠覆性创新”概念的部分内涵在于：重要的新技术往往不擅长上一代技术关注的重点，但它们能实现其他重要价值。追问LLM能否进行非常具体精确的信息检索，可能就像问Apple II能否达到大型机的运行稳定性，或能否在Netscape里构建Photoshop。它们确实做不到，但重点不在此处，也不意味着它们毫无用处。

它们还做了别的事情，而那个"别的事情"更为重要，并吸引了所有的投资、创新和公司创建。也许20年后，它们也能做旧的事情——也许最终你可以在个人电脑上运营银行，在浏览器里构建图形软件——但这在起步阶段并不重要。它们解锁了别的东西。

然而，对于生成式AI来说，那个"别的东西"是什么？你如何从概念上思考那些错误率是特性而非缺陷的领域？

机器学习最初以图像识别的形式发挥作用，但它远不止于此，并且人们花了一段时间才想明白，正确的理解方式应该是将其视为模式识别。关于个人电脑、网络或移动设备的本质究竟是什么"正确理解方式"，你可以进行长时间的哲学思辨。那么生成式AI的本质是什么？我认为还没有人真正想明白，但将其用作传统软件模式中的一组新API调用，感觉就像是用新事物去做旧事情。

与此同时，有一个古老的英国笑话，讲的是一个法国人说："这在实践中都很好，但它在理论上行得通吗？" 你可能会花太多时间对"这到底意味着什么"进行哲学思考，而没有足够的时间去实际构建和使用这些东西。这张图表恰恰说明了这一点——硅谷的每个人都在用AI构建产品。其中一些会是错的，很多会是无趣的，但其中一些人将会发现那个新事物。

然而，所有这些公司仍然是在赌一种哲学观点是正确的：它们赌的是生成式AI不会完全通用化，因为如果它真的完全通用了，我们就不需要所有这些独立的产品了。

这类难题也让我想起了2005年2月——现在差不多正好20年前——在戛纳MWC移动通信大会上，我与一位摩托罗拉副总裁的一次会面。当时iPod是热门产品，所有手机原始设备制造商都想与之匹敌，但苹果使用的微型硬盘在你掉落设备时非常容易损坏。那位摩托罗拉的人指出，这在一定程度上是期望和认知的问题：如果你掉了iPod并且它坏了，你会责怪自己；但如果你掉了手机并且它坏了，你会责怪手机制造商，即使它使用的是相同的硬件。

六个月后，苹果在Nano上从硬盘转向了闪存，而闪存在掉落时不会损坏。但两年后，苹果开始销售iPhone，现在你的手机掉落时确实会坏，但你很可能责怪自己。无论如何，我们接受了一个掉落会损坏、电池只能续航一天而非一周的设备，以换取随之而来的新功能。我们改变了我们的期望。这种期望和认知的问题，似乎现在正适用于生成式AI。经过50年的消费计算发展，我们已经被训练得期望计算机是"正确的"——是可预测的、确定性的系统。这正是我电梯测试的前提。但如果你翻转这种期望，你能得到什么回报呢？

---

> 本文由AI自动翻译，原文链接：[Are better models better? — Benedict Evans](https://www.ben-evans.com/benedictevans/2025/1/the-problem-with-better-models)
> 
> 翻译时间：2026-01-05 17:03
