---
title: 基于Core ML与dots.ocr实现设备端SOTA OCR
title_original: SOTA OCR with Core ML and dots.ocr
date: '2025-10-02'
source: Hugging Face Blog
source_url: https://huggingface.co/blog/dots-ocr-ne
author: ''
summary: 本文介绍了如何将拥有30亿参数、在OmniDocBench基准测试中超越Gemini 2.5 Pro的SOTA OCR模型dots.ocr转换为可在苹果设备端高效运行的过程。文章重点阐述了利用苹果Neural
  Engine的高能效特性，通过Core ML和MLX框架将PyTorch模型进行转换与优化的关键技术步骤，包括模型简化、精度调整以及针对设备端单图像处理的适配，旨在帮助开发者在无需网络连接和API密钥的情况下实现高性能、低功耗的OCR应用。
categories:
- AI基础设施
tags:
- OCR
- Core ML
- 设备端AI
- 模型转换
- 苹果Neural Engine
draft: false
translated_at: '2026-01-29T04:06:05.829774'
---

# 基于 Core ML 与 dots.ocr 的 SOTA OCR 技术

每年，我们的硬件都变得更强大一些，模型也因每个参数而变得更智能一些。到了2025年，在设备端运行真正具有竞争力的模型比以往任何时候都更加可行。RedNote 推出的 dots.ocr 是一个拥有 30 亿参数的 OCR 模型，它在 OmniDocBench 基准测试中超越了 Gemini 2.5 Pro，使得 OCR 成为一个真正无需妥协的设备端应用场景。在设备端运行模型对开发者来说无疑极具吸引力：无需隐藏 API 密钥、零成本且无需网络连接。然而，如果我们希望这些模型能在设备端运行，就必须考虑到有限的计算能力和功耗预算。

这就引出了 Neural Engine，这是苹果自 2017 年起为每款苹果设备配备的定制 AI 加速器。该加速器旨在实现高性能的同时，仅消耗极少的电池电量。我们的一些测试发现，Neural Engine 的能效比 CPU 高 12 倍，比 GPU 高 4 倍。

![计算单元能耗](/images/posts/e0b717614c05.png)

尽管这一切听起来非常诱人，但遗憾的是，Neural Engine 只能通过苹果的闭源机器学习框架 Core ML 来访问。此外，仅仅将模型从 PyTorch 转换到 Core ML 就可能带来一些挑战，如果没有预转换的模型或对其"锋利边缘"（潜在问题）的了解，对开发者来说可能是一项艰巨的任务。幸运的是，苹果还提供了 MLX，这是一个更现代、更灵活的 ML 框架，主要面向 GPU（而非 Neural Engine），并且可以与 Core ML 结合使用。

在这个由三部分组成的系列文章中，我们将提供一个推理追踪，展示我们如何结合使用 CoreML 和 MLX 将 dots.ocr 转换为能在设备端运行。这个过程应该适用于许多其他模型，我们希望这能帮助那些希望在自己设备上运行模型的开发者，了解所需的关键思路和工具。

要跟随操作，请克隆该仓库。你需要安装 uv 和 hf 来运行设置命令：

```bash
./boostrap.sh

```

如果你只想跳过步骤直接使用转换后的模型，可以在这里下载。

## 转换过程

从 PyTorch 转换到 CoreML 是一个两步过程：

1.  捕获你的 PyTorch 执行图（通过 `torch.jit.trace` 或更现代的方法 `torch.export`）。
2.  使用 `coremltools` 将这个转换后的图编译成 `.mlpackage` 文件。

虽然我们可以在第 2 步中调整一些参数，但我们的大部分控制权在第 1 步，即我们提供给 `coremltools` 的图。

遵循程序员"先让它跑起来，再让它正确，最后让它变快"的信条，我们将首先专注于在 GPU 上、使用 FLOAT32 精度和静态形状让转换工作起来。一旦这一步成功，我们就可以降低精度并尝试迁移到 Neural Engine。

## Dots.OCR

Dots.OCR 由两个关键组件组成：一个基于 NaViT 架构从头开始训练的 12 亿参数视觉编码器，以及一个 Qwen2.5-1.5B 骨干网络。我们将使用 CoreML 来运行视觉编码器，使用 MLX 来运行语言模型骨干网络。

## 第 0 步：理解并简化模型

为了转换一个模型，最好在开始前先理解其结构和功能。查看原始的视觉建模文件，我们可以看到视觉编码器与 QwenVL 系列相似。与许多视觉编码器一样，dots 的视觉编码器基于图像块工作，在这个案例中是 14x14 的块。dots 的视觉编码器能够处理视频和图像批次。这给了我们一个简化的机会，即每次只处理单张图像。这在设备端应用中很常见，我们转换一个提供基本功能的模型，如果需要处理多张图像再迭代。

在启动转换过程时，最好从一个最小可行模型开始。这意味着移除任何对模型功能并非严格必要的花哨功能。在我们的案例中，dots 为视觉编码器和语言模型骨干网络提供了许多不同的注意力机制实现。CoreML 围绕 `scaled_dot_product_attention` 算子构建了大量基础设施，该算子在 iOS 18 中引入。我们可以通过移除所有其他注意力实现，暂时只专注于简单的 `sdpa`（非内存高效变体）来简化模型，提交记录在此。

完成这一步后，我们在加载模型时看到一个令人担忧的警告信息：

```bash
Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.

```

模型运行并不需要滑动窗口注意力，所以我们可以放心地继续。

## 第 1 步：一个简单的封装

使用 `torch.jit.trace` 仍然是转换模型到 CoreML 最成熟的方法。我们通常将其封装在一个简单的工具中，允许你修改使用的计算单元和选择的精度。

你可以在这里查看初始的封装工具。如果我们在原始代码实现上运行以下命令：

```bash
uv run convert.py --precision FLOAT32 --compute_units CPU_AND_GPU

```

我们应该会遇到第一个（众多问题中的）问题。

## 第 2 步：排查错误

模型第一次转换就成功的情况很少见。通常，你需要逐步深入执行图进行修改，直到到达最终节点。

我们的第一个问题是以下错误：

```bash
ERROR - converting 'outer' op (located at: 'vision_tower/rotary_pos_emb/192'):
In op "matmul", when x and y are both non-const, their dtype need to match, but got x as int32 and y as fp32

```

幸运的是，这个错误给了我们很多信息。我们可以查看 `VisionRotaryEmbedding` 层，看到以下代码：

```python
def forward(self, seqlen: int) -> torch.Tensor:
    seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)
    freqs = torch.outer(seq, self.inv_freq)
    return freqs

```

尽管 `torch.arange` 有一个 `dtype` 参数，但 `coremltools` 会忽略 `arange` 的这个参数并总是输出 `int32`。
我们只需在 `arange` 后添加一个类型转换即可解决这个问题，提交记录在此。

修复此问题后，再次运行转换，我们在 `repeat_interleave` 处遇到了下一个问题：

```bash
ERROR - converting 'repeat_interleave' op (located at: 'vision_tower/204'):
Cannot add const [None]

```

虽然这个错误信息量较少，但我们的视觉编码器中只有一个地方调用了 `repeat_interleave`：

```python
cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(
    dim=0,
    dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,
)

```

`cu_seqlens` 用于在 `flash_attention_2` 中对可变长度序列进行掩码。它派生自 `grid_thw` 张量，该张量代表时间、高度和宽度。由于我们只处理单张图像，我们可以直接移除这个调用，提交记录在此。

继续下一个！这次，我们遇到了一个更隐晦的错误：

```bash
ERROR - converting '_internal_op_tensor_inplace_fill_' op (located at: 'vision_tower/0/attn/301_internal_tensor_assign_1'):
_internal_op_tensor_inplace_fill does not support dynamic index

```

这同样是由于处理可变长度序列的掩码逻辑。由于我们只处理单张图像（不是视频或图像批次），我们根本不需要注意力掩码！因此，我们可以直接使用一个全为 `True` 的掩码。为了给 Neural Engine 转换做准备，我们也从使用布尔掩码切换为全零的浮点掩码，因为 Neural Engine 不支持 `bool` 张量，提交记录在此。

完成所有这些后，模型现在应该能成功转换到 CoreML 了！然而，当我们运行模型时，我们得到以下错误：

```bash
error: 'mps.reshape' op the result shape is not compatible with the input shape

```

这个 reshape 操作可能出现在多个地方！幸运的是，我们可以利用之前的一个警告信息来帮助我们定位问题：

```bash
TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).
  for t, h, w in grid_thw:

```

大多数机器学习编译器都不喜欢动态控制流。幸运的是，由于我们只处理单张图像，我们可以直接移除循环并处理单个 `h, w` 对，相关提交在此处。

就这样完成了！如果我们再次运行转换，应该会看到模型成功转换并匹配原始 PyTorch 的精度：

```bash
Max difference: 0.006000518798828125, Mean difference: 1.100682402466191e-05

```

## 步骤 3：性能基准测试

现在模型已经可以运行，让我们评估其大小和性能。好消息是模型能正常工作，坏消息是它超过了 5GB！这对于设备端部署来说是完全不可行的！
要基准测试计算时间，我们可以使用 XCode 内置工具，通过调用：

```
open DotsOCR_FLOAT32.mlpackage

```

这将启动 XCode 的模型检查器。点击 **+ 性能报告** 并在所有计算设备上启动报告后，您应该会看到类似以下内容：

![GPU 性能报告](/images/posts/f4e1e1ed7e18.png)

视觉编码器的单次前向传播耗时超过一秒！我们还有很多工作要做。

在本系列的第二部分，我们将致力于 CoreML 与 MLX 之间的集成，以在设备端运行完整模型。在第三部分，我们将深入探讨在 Neural Engine 上运行此模型所需的优化，包括量化和动态形状处理。

---

> 本文由AI自动翻译，原文链接：[SOTA OCR with Core ML and dots.ocr](https://huggingface.co/blog/dots-ocr-ne)
> 
> 翻译时间：2026-01-29 04:06
