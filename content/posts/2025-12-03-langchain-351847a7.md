---
title: 评估深度智能体：五种核心测试模式与经验总结
title_original: 'Evaluating Deep Agents: Our Learnings'
date: '2025-12-03'
source: LangChain Blog
source_url: https://www.blog.langchain.com/evaluating-deep-agents-our-learnings/
author: ''
summary: 本文基于LangChain团队开发四个深度智能体应用的经验，总结了评估深度智能体的五种核心模式。文章指出，深度智能体的评估与传统大语言模型不同，需要为每个数据点定制测试逻辑，并详细介绍了单步推理、完整轮次、多轮交互等不同运行方式的适用场景。同时强调了环境设置的重要性，以及如何对智能体的轨迹、最终响应和其他状态进行针对性断言，为构建可靠的深度智能体提供了实用的评估框架。
categories:
- AI产品
tags:
- 深度智能体
- AI评估
- LangChain
- 智能体测试
- AI开发
draft: false
translated_at: '2026-01-21T04:40:13.529973'
---

# 评估深度智能体：我们的经验总结

过去一个月，我们在 LangChain 基于深度智能体框架开发并发布了四个应用：

- DeepAgents CLI：一个编码智能体
- LangSmith Assist：一个应用内智能体，用于协助处理 LangSmith 中的各种事务
- 个人邮件助手：一个通过用户交互进行学习的邮件助手
- Agent Builder：一个由元深度智能体驱动的无代码智能体构建平台

构建并发布这些智能体意味着需要为每个应用添加评估，我们在此过程中收获颇丰！本文将深入探讨评估深度智能体的以下几种模式。

1.  深度智能体需要为每个数据点定制测试逻辑——每个测试用例都有其自身的成功标准。
2.  运行深度智能体的单步推理非常适合验证特定场景下的决策（还能节省 Token！）。
3.  完整的智能体轮次非常适合测试关于智能体“最终状态”的断言。
4.  多轮智能体交互能模拟真实的用户互动，但需要保持在预设轨道上。
5.  环境设置很重要——深度智能体需要干净、可复现的测试环境。

在深入探讨之前，我们先定义一下本文中使用的几个术语。

运行智能体的方式：

- **单步推理**：将核心智能体循环限制为仅运行一轮，确定智能体将采取的下一步行动。
- **完整轮次**：在单个输入上完整运行智能体，这可能包含多次工具调用迭代。
- **多轮交互**：完整运行智能体多次。通常用于模拟智能体与用户之间包含多次来回互动的“多轮”对话。

- **轨迹**：智能体调用的工具序列，以及智能体生成的特定工具参数。
- **最终响应**：智能体返回给用户的最终响应。
- **其他状态**：智能体在运行过程中生成的其他值（例如文件、其他产物）。

## #1：深度智能体需要为每个数据点定制更多测试逻辑（代码）

传统的大语言模型评估是直截了当的：

1) 构建示例数据集
2) 编写评估器
3) 在数据集上运行你的应用以产生输出，并用你的评估器对这些输出进行评分

每个数据点都被同等对待——通过相同的应用逻辑运行，由相同的评估器评分。

深度智能体打破了这一假设。你希望测试的不仅仅是最终消息。“成功标准”也可能更针对每个数据点，并且可能涉及对智能体轨迹和状态的特定断言。

考虑以下示例：

我们有一个日历调度深度智能体，它能够记住用户偏好。用户要求其智能体“记住永远不要在上午9点前安排会议”。我们希望断言日历调度智能体在其文件系统中更新了自己的记忆以记住此信息。

为了测试这一点，我们可能需要编写断言来验证：

1) 智能体在 `memories.md` 文件路径上调用了 `edit_file` 工具
2) 智能体在其最终消息中向用户传达了记忆更新
3) `memories.md` 文件确实包含了关于不安排早期会议的信息。你可以：
    - 使用正则表达式查找提及“9am”的内容
    - 或者使用 LLM 作为评判者，并设定特定的成功标准，对文件更新进行更全面的分析

LangSmith 的 Pytest 和 Vitest 集成支持这种类型的定制测试。你可以为每个测试用例对智能体的轨迹、最终消息和状态做出不同的断言。

```
# 标记为 LangSmith 测试用例
@pytest.mark.langsmith
def test_remember_no_early_meetings() -> None:
    user_input = "I don't want any meetings scheduled before 9 AM ET"
    # 我们可以将智能体的输入记录到 LangSmith
    t.log_inputs({"question": user_input})
    
    response = run_agent(user_input)
    # 我们可以将智能体的输出记录到 LangSmith
    t.log_outputs({"outputs": response})
    
    agent_tool_calls = get_agent_tool_calls(response)
    
    # 我们断言智能体调用了 edit_file 工具来更新其记忆
    assert any([tc["name"] == "edit_file" and tc["args"]["path"] == "memories.md" for tc in agent_tool_calls])
    
		# 我们记录来自 LLM 作为评判者的反馈，即最终消息确认了记忆更新
		communicated_to_user = llm_as_judge_A(response)
    t.log_feedback(key="communicated_to_user", score=communicated_to_user)
    
    # 我们记录来自 LLM 作为评判者的反馈，即记忆文件现在包含了正确的信息
    memory_updated = llm_as_judge_B(response)
    t.log_feedback(key="memory_updated", score=memory_updated)
```

关于如何使用 Pytest 的通用代码片段，请查看这些文档：

此 LangSmith 集成会自动将所有测试用例记录到一个实验中，因此你可以查看失败测试用例的轨迹（以调试出错原因）并随时间跟踪结果。

## #2：单步评估很有价值且高效

在为深度智能体运行评估时，我们大约一半的测试用例看起来像是单步评估，即在大语言模型接收到一系列特定的输入消息后，它立即决定做什么？

这对于验证智能体在特定场景下是否以正确的参数调用了正确的工具特别有用。常见的测试用例包括：

- 它是否调用了正确的工具来搜索会议时间？
- 它是否检查了正确的目录内容？
- 它是否更新了其记忆？

回归问题通常发生在单个决策点，而不是整个执行序列中。如果使用 LangGraph，其流式功能允许你在单个工具调用后中断智能体以检查输出——这样你就可以在无需完整智能体序列开销的情况下及早发现问题。

在下面的代码片段中，我们在工具节点之前手动引入了一个断点，使我们能够轻松地运行智能体的单步推理。然后我们可以检查该单步之后的状态并做出断言。

```
def test_single_step() -> None:
	state_before_tool_execution = await agent.ainvoke(
	    inputs,
	    # interrupt_before 指定在哪些节点之前停止
	    # 在工具节点之前中断允许我们检查工具调用参数
	    interrupt_before=["tools"]
	)
	# 我们可以看到智能体的消息历史，包括最新的工具调用
	print(state_before_tool_execution["messages"])
```

## #3：完整的智能体轮次让你获得完整视图

可以将单步评估视为你的“单元测试”，确保智能体在特定场景下采取预期的行动。同时，完整的智能体轮次也很有价值——它们向你展示了智能体端到端行动的完整视图。

完整的智能体轮次让你可以通过多种方式测试智能体行为：

1) **轨迹**：评估完整轨迹的一种非常常见的方法是确保在行动过程中的某个时刻调用了特定的工具，但具体何时调用并不重要。在我们的日历调度器示例中，调度器可能需要多次工具调用来找到一个适合所有参与方的合适时间段。

2) **最终响应**：在某些情况下，最终输出的质量比智能体采取的具体路径更重要。我们发现对于编码和研究等更开放式的任务来说确实如此。

3) 其他状态：评估其他状态与评估Agent（智能体）的最终响应非常相似。有些Agent会创建产物，而非以聊天格式响应用户。通过检查LangGraph中的Agent状态，可以轻松检查和测试这些产物。

1. 对于编码Agent → 读取并测试Agent编写的文件。
2. 对于研究Agent → 确认Agent找到了正确的链接或来源。

完整的Agent轮次能让你全面了解Agent的执行情况。LangSmith让查看完整的Agent轮次轨迹变得非常简单，你可以看到延迟和Token使用量等高级指标，同时还能向下分析到每个模型调用或工具调用的具体步骤。

## #4：跨多轮次运行Agent可模拟完整的用户交互

某些场景需要在包含多个连续用户输入的多轮对话中测试Agent。挑战在于，如果你简单地硬编码一系列输入，而Agent偏离了预期路径，那么后续硬编码的用户输入可能就不合理了。

我们通过在Pytest和Vitest测试中添加条件逻辑来解决这个问题。例如，我们会：

- 运行第一轮，然后检查Agent输出。如果输出符合预期，则运行下一轮。如果不符合预期，则提前终止测试。（这是可行的，因为我们有灵活性可以在每个步骤后添加检查。）
- 如果输出符合预期，则运行下一轮。
- 如果不符合预期，则提前终止测试。（这是可行的，因为我们有灵活性可以在每个步骤后添加检查。）

这种方法让我们能够运行多轮评估，而无需为每个可能的Agent分支路径建模。如果我们想单独测试第二轮或第三轮，只需从该点开始设置测试，并配以适当的初始状态即可。

## #5：设置正确的评估环境很重要

深度Agent是有状态的，旨在处理复杂、长期运行的任务——这通常需要更复杂的环境来进行评估。

与更简单的LLM（大语言模型）评估不同（其环境通常仅限于少数几个无状态工具），深度Agent每次评估运行都需要一个全新、干净的环境，以确保结果可复现。

编码Agent清楚地说明了这一点。Harbor为TerminalBench提供了一个在专用Docker容器或沙箱内运行的评估环境。对于DeepAgents CLI，我们采用了一种更轻量级的方法：为每个测试用例创建一个临时目录，并在其中运行Agent。

更广泛的要点是：深度Agent评估需要每个测试都能重置的环境——否则你的评估会变得不稳定且难以复现。

提示：模拟你的API请求

LangSmith Assist需要连接到真实的LangSmith API。针对实时服务运行评估可能速度慢且成本高。相反，可以将HTTP请求记录到文件系统中，并在测试执行期间重放它们。对于Python，`vcr`效果很好；对于JS，我们通过Hono应用代理`fetch`请求。

模拟或重放API请求使深度Agent评估更快、更易于调试，尤其是当Agent严重依赖外部系统状态时。

## 使用LangSmith评估深度Agent

以上技术是我们在为深度Agent驱动的应用程序编写自己的测试套件时看到的常见模式。对于你的具体应用，可能只需要上述模式的一个子集——因此，你的评估框架具备灵活性非常重要。如果你正在构建深度Agent并开始进行评估，请查看LangSmith的测试集成！

来自LangChain团队和社区的更新

正在处理您的申请...

成功！请检查您的收件箱并点击链接确认订阅。

抱歉，出错了。请重试。

## 在LangSmith中引入端到端OpenTelemetry支持

## LangChain 2024年人工智能现状报告

## 为LangSmith引入OpenTelemetry支持

## 使用LangSmith SDK v0.2进行更轻松的评估

## LangGraph平台测试版：用于可扩展Agent基础设施的新部署选项

## 通过少样本提示改进工具调用性能

> 本文由AI自动翻译，原文链接：[Evaluating Deep Agents: Our Learnings](https://www.blog.langchain.com/evaluating-deep-agents-our-learnings/)
> 
> 翻译时间：2026-01-21 04:40
