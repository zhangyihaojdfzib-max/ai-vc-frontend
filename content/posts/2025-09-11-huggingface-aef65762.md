---
title: OpenAI GPT-OSSæ–°æŠ€å·§ï¼šTransformersåº“å…¨é¢å‡çº§æŒ‡å—
title_original: Tricks from OpenAI gpt-oss YOU ðŸ«µ can use with transformers
date: '2025-09-11'
source: Hugging Face Blog
source_url: https://huggingface.co/blog/faster-transformers
author: ''
summary: æœ¬æ–‡ä»‹ç»äº†OpenAIå‘å¸ƒGPT-OSSç³»åˆ—æ¨¡åž‹åŽï¼ŒTransformersåº“è¿›è¡Œçš„ä¸€ç³»åˆ—é‡å¤§å‡çº§ã€‚è¿™äº›å‡çº§åŒ…æ‹¬é›¶æž„å»ºå†…æ ¸ä¸‹è½½ã€MXFP4é‡åŒ–ã€å¼ é‡å¹¶è¡Œã€ä¸“å®¶å¹¶è¡Œã€åŠ¨æ€æ»‘åŠ¨çª—å£ç¼“å­˜ã€è¿žç»­æ‰¹å¤„ç†ä¸Žåˆ†é¡µæ³¨æ„åŠ›ç­‰æŠ€æœ¯ï¼Œæ—¨åœ¨æå‡æ¨¡åž‹åŠ è½½ã€è¿è¡Œå’Œå¾®è°ƒçš„æ•ˆçŽ‡ã€‚æ–‡ç« é‡ç‚¹è§£é‡Šäº†å¦‚ä½•é€šè¿‡Hubä¸‹è½½é¢„æž„å»ºå†…æ ¸æ¥ä¼˜åŒ–æ€§èƒ½ï¼Œå¹¶æŒ‡å‡ºè¿™äº›ç‰¹æ€§å°†æƒ åŠTransformersåº“ä¸­çš„å…¶ä»–çŽ°æœ‰åŠæœªæ¥æ¨¡åž‹ï¼ŒæŽ¨åŠ¨ç¤¾åŒºå¿«é€Ÿé‡‡ç”¨æ–°æŠ€æœ¯ã€‚
categories:
- AIåŸºç¡€è®¾æ–½
tags:
- Transformersåº“
- GPT-OSS
- æ¨¡åž‹ä¼˜åŒ–
- å†…æ ¸æŠ€æœ¯
- å¹¶è¡Œè®¡ç®—
draft: false
translated_at: '2026-02-10T04:35:09.642261'
---

# æ¥è‡ª OpenAI GPT-OSS çš„ã€ä½  ðŸ«µ ä¹Ÿèƒ½åœ¨ Transformers ä¸­ä½¿ç”¨çš„æŠ€å·§

OpenAI æœ€è¿‘å‘å¸ƒäº†ä»–ä»¬çš„ GPT-OSS ç³»åˆ—æ¨¡åž‹ã€‚è¿™äº›æ¨¡åž‹é‡‡ç”¨äº†ä¸€äº›æ–°é¢–çš„æŠ€æœ¯ï¼Œå¦‚ MXFP4 é‡åŒ–ã€é«˜æ•ˆå†…æ ¸ã€å…¨æ–°çš„èŠå¤©æ ¼å¼ç­‰ã€‚ä¸ºäº†é€šè¿‡ Transformers åº“å‘å¸ƒ GPT-OSSï¼Œæˆ‘ä»¬å¯¹åº“è¿›è¡Œäº†å¤§å¹…å‡çº§ã€‚è¿™äº›æ›´æ–°ä½¿å¾—åŠ è½½ã€è¿è¡Œå’Œå¾®è°ƒæ¨¡åž‹å˜å¾—éžå¸¸é«˜æ•ˆã€‚

åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æŽ¢è®¨æ‰€æœ‰çš„å‡çº§å†…å®¹ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•æˆä¸º Transformers å·¥å…·åŒ…çš„ä¸€éƒ¨åˆ†ï¼Œä»¥ä¾¿å…¶ä»–æ¨¡åž‹ï¼ˆå½“å‰å’Œæœªæ¥çš„ï¼‰éƒ½èƒ½ä»Žä¸­å—ç›Šã€‚åœ¨ Transformers ä¸­æä¾›æ–°æ–¹æ³•çš„æ¸…æ™°å®žçŽ°ï¼Œä¹Ÿä½¿å¾—ç¤¾åŒºèƒ½å¤Ÿå¿«é€Ÿç†è§£å¹¶é‡‡ç”¨å®ƒä»¬ã€‚åƒ MLXã€llama.cpp æˆ– vLLM è¿™æ ·çš„æ¡†æž¶å¯ä»¥ä½¿ç”¨ Transformers çš„ä»£ç ä½œä¸ºå‚è€ƒæ¥æž„å»ºè‡ªå·±çš„å®žçŽ°ã€‚

é’ˆå¯¹è¿™æ¬¡å‘å¸ƒï¼Œæˆ‘ä»¬ä¸»è¦è‡´åŠ›äºŽï¼š

-   é›¶æž„å»ºå†…æ ¸ï¼Œå¯ä»Ž Hub ä¸‹è½½
-   MXFP4 é‡åŒ–
-   å¼ é‡å¹¶è¡Œ
-   ä¸“å®¶å¹¶è¡Œ
-   åŠ¨æ€æ»‘åŠ¨çª—å£å±‚ä¸Žç¼“å­˜
-   è¿žç»­æ‰¹å¤„ç†ä¸Žåˆ†é¡µæ³¨æ„åŠ›
-   æ›´å¿«åœ°åŠ è½½æ›´å¤§æ¨¡åž‹

æœ€æ£’çš„éƒ¨åˆ†æ˜¯ï¼šè¿™äº›ç‰¹æ€§ä¸­çš„å¤§éƒ¨åˆ†åº”è¯¥èƒ½åœ¨ Transformers å†…çš„æ‰€æœ‰ä¸»è¦æ¨¡åž‹ä¸Šè¿è¡Œï¼

## é›¶æž„å»ºå†…æ ¸ï¼Œå¯ä»Ž Hub ä¸‹è½½

å†…æ ¸æ˜¯ä¸€ä¸ªåœ¨åŠ é€Ÿå™¨ä¸Šè¿è¡Œçš„ã€ä¸“é—¨ä¸”ç´§å‡‘çš„ç¨‹åºï¼Œç”¨äºŽæ‰§è¡ŒçŸ©é˜µä¹˜æ³•ã€æ¿€æ´»æˆ–å½’ä¸€åŒ–ç­‰ä»»åŠ¡ã€‚åœ¨ PyTorch çš„å³æ—¶æ‰§è¡Œæ¨¡å¼ä¸‹ï¼Œæ“ä½œä¼šé¡ºåºè§¦å‘å•ä¸ªå†…æ ¸ï¼Œè¿™ç§æ–¹å¼ç®€å•ç›´æŽ¥ï¼Œä½†å¯èƒ½å¯¼è‡´é¢å¤–çš„å†…å­˜ä¼ è¾“å’Œå¯åŠ¨å¼€é”€ã€‚PyTorch 2.0 çš„ `torch.compile` é…åˆ TorchInductor ç­‰åŽç«¯ï¼Œé€šè¿‡è‡ªåŠ¨èžåˆå’Œä¼˜åŒ–å†…æ ¸æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¸¦æ¥äº† 2-10 å€çš„æ€§èƒ½æå‡ã€‚

æ­¤å¤–ï¼Œç¤¾åŒºè¿˜ä¸ºé¢‘ç¹å‡ºçŽ°çš„æ“ä½œç»„åˆåˆ›å»ºäº†è‡ªå®šä¹‰å†…æ ¸ï¼Œè€Œä¸ä»…ä»…æ˜¯é’ˆå¯¹åƒçŸ©é˜µä¹˜æ³•è¿™æ ·çš„å•ä¸ª PyTorch æ“ä½œã€‚ä¾‹å¦‚ï¼ŒFlash Attention çš„åˆ›å»ºå°±æ˜¯ä¸ºäº†ä¼˜åŒ–å®šä¹‰ Transformer æž¶æž„çš„å…³é”®æ³¨æ„åŠ›æ¨¡å—ï¼Œè¯¥æ¨¡å—å­˜åœ¨äºŽåŒ…æ‹¬å¤§å¤šæ•° LLM åœ¨å†…çš„è®¸å¤šæ¨¡åž‹ä¸­ã€‚é€šè¿‡å°†æ³¨æ„åŠ›æ“ä½œçš„æ‰€æœ‰æ­¥éª¤ç²¾å¿ƒç»„åˆåœ¨å•ä¸ªå†…æ ¸ä¸­ï¼Œå¯ä»¥æœ€å¤§é™åº¦åœ°å‡å°‘å†…å­˜ä¼ è¾“ã€é™ä½Žå†…å­˜ä½¿ç”¨å¹¶å®žçŽ°åŠ é€Ÿã€‚

é—®é¢˜æ˜¯ï¼Œæ‰€æœ‰è¿™äº›ä¸åŒçš„å†…æ ¸éƒ½åˆ†æ•£åœ¨ä¸åŒçš„åº“ä¸­ï¼Œå¦‚æžœå°†å®ƒä»¬æ·»åŠ åˆ° Transformers åº“ä¸­ï¼Œä¼šé€ æˆä¾èµ–è†¨èƒ€ã€‚æ­¤å¤–ï¼Œè¿™äº›å†…æ ¸ä¸ä»…ä»…æ˜¯ Python ä»£ç ï¼Œå®ƒä»¬ç”±åº•å±‚ CUDA ä»£ç ç»„æˆï¼Œç”¨ C++ ç²˜åˆåœ¨ä¸€èµ·ï¼Œå¹¶é€šè¿‡ Python å±‚æš´éœ²ã€‚è¿™æ„å‘³ç€å®ƒä»¬å¿…é¡»åœ¨ç›®æ ‡ç³»ç»Ÿä¸­ç¼–è¯‘ï¼Œè€Œè¿™åˆéœ€è¦æ¯ä¸ªå†…æ ¸åº“æ‰€è¦æ±‚çš„ä»»ä½•æž„å»ºç³»ç»Ÿã€‚

`kernels` åŒ…é€šè¿‡ä»Ž Hub ä¸‹è½½å—æ”¯æŒå†…æ ¸çš„é¢„æž„å»ºäºŒè¿›åˆ¶æ–‡ä»¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ä½ åªéœ€æŒ‡å®šæƒ³è¦ä½¿ç”¨çš„å†…æ ¸ï¼Œ`kernels` å°±ä¼šå¯»æ‰¾ä¸Žä½ çš„ç³»ç»Ÿå…¼å®¹çš„ç‰ˆæœ¬å¹¶åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶ä¸‹è½½å®ƒã€‚

### GPT-OSS çš„è‡ªå®šä¹‰å†…æ ¸

GPT-OSS æ˜¯ä¸€ä¸ªä¸“å®¶æ··åˆæ¨¡åž‹ï¼Œæ˜¯ Hub å†…æ ¸çš„ä¸»è¦ä½¿ç”¨è€…ã€‚å®ƒåˆ©ç”¨äº†å¤šä¸ªè‡ªå®šä¹‰å†…æ ¸ï¼š

1.  Liger RMSNormï¼Œä½¿ç”¨æ–¹å¼ä¸º `@use_kernel_forward_from_hub("RMSNorm")`
2.  Megablocks MoE å†…æ ¸ï¼š`@use_kernel_forward_from_hub("MegaBlocksMoeMLP")`
3.  æ”¯æŒæ³¨æ„åŠ›æ±‡èšçš„ Flash Attention 3ã€‚
4.  MXFP4 Triton å†…æ ¸ï¼ˆç¨åŽä»‹ç»ï¼‰

è®©æˆ‘ä»¬çœ‹çœ‹å‰ä¸¤ä¸ªã€‚

åœ¨å¹•åŽï¼Œè£…é¥°å™¨ï¼ˆ1 å’Œ 2ï¼‰åªæ˜¯æŒ‡å‘ç¤¾åŒºè´¡çŒ®çš„å†…æ ¸ã€‚ä¾‹å¦‚ï¼Œ`RMSNorm` æ¥è‡ª `liger_kernels`ï¼Œè€Œ `MegaBlocksMoeMLP` å†…æ ¸æ¥è‡ª `megablocks`ã€‚æ ¹æ®ä½ çš„è®¾å¤‡ï¼ˆCUDA æˆ– ROCmï¼‰ä»¥åŠä½ æ˜¯åœ¨è®­ç»ƒè¿˜æ˜¯è¿è¡ŒæŽ¨ç†ï¼Œæ­£ç¡®çš„å†…æ ¸ä¼šè¢«è‡ªåŠ¨å¼•å…¥ã€‚

è¿™ç§è®¾è®¡æ—¢å…·ä½“åˆé€šç”¨ï¼šLiger RMSNorm å†…æ ¸å·²ç»åœ¨å¤šä¸ªæ¨¡åž‹ä¸­å¤ç”¨ï¼Œè€Œ MoE å†…æ ¸ä¹Ÿå¯ä»¥åº”ç”¨äºŽæœªæ¥çš„ MoE æ¨¡åž‹ã€‚

å› ä¸º `kernels` ä»Ž Hub æ‹‰å–ä»£ç ï¼Œä½ å¿…é¡»é€šè¿‡åœ¨æ¨¡åž‹å®žä¾‹åŒ–æ—¶ä¼ é€’ `use_kernels=True` æ¥é€‰æ‹©å¯ç”¨æ­¤åŠŸèƒ½ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚æˆ‘ä»¬åœ¨ç¤ºä¾‹ä¸­å¯ç”¨äº† `INFO` æ—¥å¿—è®°å½•ï¼Œä»¥ä¾¿ä½ å¯ä»¥è½»æ¾éªŒè¯æ˜¯å¦æ­£åœ¨ä½¿ç”¨å¯ä¸‹è½½çš„å†…æ ¸ã€‚

è¿™äº›å†…æ ¸ä¸Ž `mxfp4` ä¸å…¼å®¹ï¼Œå› æ­¤å¦‚æžœä½ ä½¿ç”¨å®ƒä»¬ï¼ŒæŽ¨ç†å°†ä»¥ `bfloat16` ç²¾åº¦è¿›è¡Œã€‚è¯·æ ¹æ®ä½ çš„é¡¹ç›®éœ€æ±‚ï¼Œå¯¹ä½ çš„ç³»ç»Ÿè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä»¥æ‰¾åˆ°å†…å­˜å’Œåžåé‡çš„æœ€ä½³ç»„åˆï¼

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

import logging
logging.basicConfig(level=logging.INFO)

model_id = "openai/gpt-oss-20b"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    dtype="auto",
    device_map="auto",
    use_kernels=True,
)

```

è¿è¡Œå¿«é€Ÿç”Ÿæˆä¼šäº§ç”Ÿå¦‚ä¸‹æ—¥å¿—ä¿¡æ¯ï¼š

```shell
INFO:root:Using layer `LigerRMSNorm` from repo `kernels-community/liger_kernels`
INFO:root:Using layer `MegaBlocksMoeMLP` from repo `kernels-community/megablocks`

```

å›¾ 1 æ˜¾ç¤ºï¼Œåœ¨æˆ‘ä»¬æµ‹è¯•çš„ç³»ç»Ÿä¸­ï¼Œè¿™äº›å†…æ ¸åœ¨è¾ƒå¤§çš„æ‰¹å¤„ç†å¤§å°æ—¶æ•ˆæžœæœ€ä½³ã€‚æˆ‘ä»¬å§‹ç»ˆå»ºè®®å°½å¯èƒ½åœ¨æŽ¥è¿‘ç”Ÿäº§çŽ¯å¢ƒçš„æ¡ä»¶ä¸‹å¯¹ä»»ä½•ä¸Žæ€§èƒ½ç›¸å…³çš„æ›´æ”¹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚

![ä½¿ç”¨å†…æ ¸ä¸Žä¸ä½¿ç”¨å†…æ ¸çš„åŸºå‡†æµ‹è¯•](/images/posts/eed2ac2cb139.png)

ä½ å¯ä»¥åœ¨æ­¤å¤„æŽ¢ç´¢å’Œä½¿ç”¨åŸºå‡†æµ‹è¯•è„šæœ¬ã€‚

### Flash Attention 3

OpenAI GPT-OSS æ¨¡åž‹ä½¿ç”¨äº†æ³¨æ„åŠ›æ±‡èšï¼Œè¿™æé«˜äº†è´¨é‡å¹¶ä¾¿äºŽä½¿ç”¨æ›´é•¿çš„ä¸Šä¸‹æ–‡ã€‚vLLM å›¢é˜Ÿå°†æ­¤åŠŸèƒ½æ·»åŠ åˆ°äº†æœ€æ–°ç‰ˆæœ¬çš„ Flash Attention ä¸­ï¼Œç”±æ­¤äº§ç”Ÿçš„è‡ªå®šä¹‰å†…æ ¸å¯åœ¨ Hub ä¸ŠèŽ·å–ã€‚ç›®å‰ï¼Œè¯¥å†…æ ¸ä¸Ž Hopper æž¶æž„å…¼å®¹ã€‚å¦‚æžœä½ æœ‰è¯¥æž¶æž„çš„ç¡¬ä»¶ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯ç”¨å®ƒï¼š

```diff
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    dtype="auto",
    device_map="auto",
+    # å¸¦æ±‡èšçš„ Flash Attention
+    attn_implementation="kernels-community/vllm-flash-attn3",
)

```

## MXFP4 é‡åŒ–

å¤§è¯­è¨€æ¨¡åž‹éžå¸¸æ¶ˆè€—å†…å­˜ã€‚é‡åŒ–é€šè¿‡ä»¥è¾ƒä½Žç²¾åº¦æ ¼å¼å­˜å‚¨æƒé‡ï¼ˆæœ‰æ—¶æ˜¯æ¿€æ´»å€¼ï¼‰æ¥å‡å°‘å†…å­˜å ç”¨ã€‚ä½œä¸ºå‚è€ƒï¼Œ`FP32` æ¯ä¸ªæ•°å­—ä½¿ç”¨ 32 ä½ï¼Œ`BF16` ä½¿ç”¨ 16 ä½ã€‚é€šè¿‡å‡å°‘ä½å®½ï¼Œæˆ‘ä»¬ç‰ºç‰²ä¸€äº›ç²¾åº¦æ¥æ¢å–æ›´å°çš„æ¨¡åž‹å’Œæ›´å¿«çš„å†…å­˜ç§»åŠ¨ã€‚

å¦‚æžœä½ æƒ³äº†è§£é‡åŒ–æƒè¡¡çš„ç›´è§‚å…¥é—¨çŸ¥è¯†ï¼ŒMaarten Grootendorst çš„æ–‡ç« éžå¸¸å‡ºè‰²ï¼šã€Šé‡åŒ–å¯è§†åŒ–æŒ‡å—ã€‹ã€‚

### ä»€ä¹ˆæ˜¯ MXFP4

![mxfp4 æ ¼å¼è¯´æ˜Ž](/images/posts/408576fa1dbb.png)

`MXFP4` æ˜¯ä¸€ç§ 4 ä½æµ®ç‚¹æ ¼å¼ï¼Œé‡‡ç”¨ E2M1 å¸ƒå±€ï¼š1 ä¸ªç¬¦å·ä½ã€2 ä¸ªæŒ‡æ•°ä½å’Œ 1 ä¸ªå°¾æ•°ä½ï¼Œå¦‚å›¾ 2 æ‰€ç¤ºã€‚å°±å…¶æœ¬èº«è€Œè¨€ï¼ŒE2M1 éžå¸¸ç²—ç³™ã€‚MXFP4 é€šè¿‡åˆ†å—ç¼©æ”¾è¿›è¡Œè¡¥å¿ï¼š

-   å‘é‡è¢«åˆ†ç»„ä¸º 32 ä¸ªå…ƒç´ çš„å—ã€‚
-   æ¯ä¸ªå—å­˜å‚¨ä¸€ä¸ªå…±äº«çš„ç¼©æ”¾å› å­ï¼Œç”¨äºŽåœ¨åé‡åŒ–æ—¶æ¢å¤åŠ¨æ€èŒƒå›´ã€‚
-   åœ¨æ¯ä¸ªå—å†…éƒ¨ï¼Œ4 ä½å€¼è¡¨ç¤ºç›¸å¯¹äºŽè¯¥ç¼©æ”¾å› å­çš„æ•°å­—ã€‚

è¿™ç§åˆ†å—æ–¹æ¡ˆè®© `MXFP4` åœ¨ä½¿ç”¨æžå°‘ä½æ•°çš„æƒ…å†µä¸‹ä¿æŒèŒƒå›´ã€‚å®žé™…ä¸Šï¼Œå½“å¯ç”¨ `MXFP4` æ—¶ï¼ŒGPT-OSS 20B å¤§çº¦å ç”¨ 16 GB çš„ VRAMï¼ŒGPT-OSS 120B å¤§çº¦å ç”¨ 80 GBï¼Œè¿™å†³å®šäº†æ¨¡åž‹æ˜¯â€œæ— æ³•åŠ è½½â€è¿˜æ˜¯â€œå¯ä»¥åœ¨å•ä¸ª GPU ä¸Šè¿è¡Œâ€ã€‚å…³é”®åœ¨äºŽï¼ŒçŸ©é˜µä¹˜æ³•çŽ°åœ¨å¿…é¡»è€ƒè™‘å—ç¼©æ”¾å› å­ã€‚è¦å¤§è§„æ¨¡é«˜æ•ˆåœ°åšåˆ°è¿™ä¸€ç‚¹ï¼Œéœ€è¦ä¸“ç”¨çš„å†…æ ¸ã€‚

### Transformers ä¸­çš„ MXFP4

Transformers çŽ°åœ¨åŒ…å«å¯¹ MXFP4 çš„åŽŸç”Ÿæ”¯æŒï¼Œåˆ©ç”¨ä¼˜åŒ–çš„ Triton å†…æ ¸æ¥æå‡æ€§èƒ½ã€‚è¿™å»ºç«‹åœ¨ä¹‹å‰è®¨è®ºçš„ç¤¾åŒºé©±åŠ¨çš„å†…æ ¸åˆ†å‘åŸºç¡€ä¸Šï¼Œåˆ©ç”¨ Hub ä¸Šçš„é¢„ç¼–è¯‘å†…æ ¸æ¥ç®€åŒ–éƒ¨ç½²ã€‚

å…³é”®å®žçŽ°ç»†èŠ‚ï¼š

-   é‡åŒ–å™¨é€»è¾‘ï¼šä½äºŽ MXFP4 é‡åŒ–å™¨æ–‡ä»¶ä¸­ï¼Œå¤„ç† MXFP4 çš„æ ¸å¿ƒé‡åŒ–è¿‡ç¨‹ã€‚
-   é›†æˆé’©å­ï¼šMXFP4 é›†æˆæ–‡ä»¶ä½¿å¾—åœ¨ Transformers æ¡†æž¶å†…æ— ç¼ä½¿ç”¨ MXFP4 æˆä¸ºå¯èƒ½ã€‚

è¦æ£€æŸ¥æ¨¡åž‹æ˜¯å¦æ”¯æŒ `MXFP4`ï¼Œè¯·æŸ¥çœ‹å…¶é…ç½®ï¼š

```py
from transformers import GptOssConfig

model_id = "openai/gpt-oss-120b"
cfg = GptOssConfig.from_pretrained(model_id)
print(cfg.quantization_config)












```

å¦‚æžœå­˜åœ¨ `'quant_method': 'mxfp4'`ï¼Œåˆ™åœ¨å—æ”¯æŒçš„æƒ…å†µä¸‹ï¼Œæ¨¡åž‹å°†è‡ªåŠ¨ä½¿ç”¨å¸¦æœ‰ Triton å†…æ ¸çš„ MXFP4 è·¯å¾„ã€‚

å¾—ç›ŠäºŽè¿™ä¸ªpull requestï¼Œæ‚¨çŽ°åœ¨å¯ä»¥å¾®è°ƒgpt-ossæ¨¡åž‹ï¼Œå¹¶ä»¥MXFP4æ ¼å¼ç›´æŽ¥ä¿å­˜åˆ°Hubï¼Œä»Žè€Œé€šè¿‡ä¼˜åŒ–æ€§èƒ½ç®€åŒ–éƒ¨ç½²æµç¨‹ã€‚

### è¦æ±‚ä¸Žå›žé€€æ–¹æ¡ˆ

è¦åœ¨GPUä¸Šè¿è¡ŒMXFP4ï¼Œæ‚¨éœ€è¦ï¼š

1. å®‰è£…accelerateã€kernelsä»¥åŠtriton>=3.4ã€‚è¯·æ³¨æ„ï¼ŒPytorch 2.8å·²è‡ªå¸¦triton 3.4ï¼Œå› æ­¤åªæœ‰åœ¨ä½¿ç”¨Pytorch 2.7æ—¶æ‰éœ€è¦æ‰‹åŠ¨å®‰è£…tritonã€‚
2. è®¡ç®—èƒ½åŠ›â‰¥ 7.5çš„NVIDIA GPUã€‚è¿™å¯ä»¥è¿½æº¯åˆ°Teslaç³»åˆ—ï¼Œå› æ­¤æ‚¨å¯ä»¥åœ¨Google Colabå’ŒKaggleçš„å…è´¹å±‚çº§ä»¥åŠè®¸å¤šæ¶ˆè´¹çº§GPUä¸Šè¿è¡Œgpt-oss-20bã€‚

å¦‚æžœæ— æ³•æ»¡è¶³è¿™äº›æ¡ä»¶ï¼Œtransformerså°†å›žé€€åˆ°æ›´é«˜ç²¾åº¦çš„è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨bfloat16ï¼‰ï¼Œè¿™å¤§çº¦éœ€è¦MXFP4 4å€çš„å†…å­˜ã€‚

ä»¥ä¸‹ä»£ç ç‰‡æ®µåœ¨CUDAä¸ŠåŠ è½½äº†ä¸¤æ¬¡GPT-OSSï¼šä¸€æ¬¡ä½¿ç”¨`Mxfp4Config(dequantize=True)`ï¼ˆå†…å­˜å¯†é›†åž‹ï¼‰ï¼Œå¦ä¸€æ¬¡ä½¿ç”¨é»˜è®¤çš„é‡åŒ–è·¯å¾„ï¼ˆå†…å­˜é«˜æ•ˆï¼‰ã€‚å›¾3æ˜¾ç¤ºäº†æ¯æ¬¡åŠ è½½åŽä½¿ç”¨çš„VRAMé‡ï¼Œä»¥ä¾¿æ‚¨ç›´è§‚åœ°äº†è§£èŠ‚çœçš„å†…å­˜ã€‚

![é‡åŒ–æ¨¡åž‹ä¸Žåé‡åŒ–æ¨¡åž‹çš„å†…å­˜ä½¿ç”¨å¯¹æ¯”](/images/posts/076d2fe8c3c3.png)

### MXFP4çš„å†…æ ¸

é«˜æ•ˆçš„MXFP4éœ€è¦å†…æ ¸èƒ½å¤Ÿåœ¨GEMMå’Œèžåˆæ“ä½œæœŸé—´ç†è§£32å…ƒç´ å—åŠå…¶ç¼©æ”¾å› å­ã€‚è¿™æ­£æ˜¯Hubä¸­çš„Kernelså†æ¬¡å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚å½“æ‚¨åŠ è½½éœ€è¦MXFP4å†…æ ¸çš„æ¨¡åž‹æ—¶ï¼Œtransformersä¼šè‡ªåŠ¨ä»Žç¤¾åŒºä»“åº“æ‹‰å–æ”¯æŒMXFP4çš„Tritonå†…æ ¸ã€‚è¯¥ä»“åº“å°†å‡ºçŽ°åœ¨æ‚¨çš„æœ¬åœ°ç¼“å­˜ä¸­ï¼Œå¹¶åœ¨å‰å‘ä¼ æ’­æœŸé—´ä½¿ç”¨ã€‚å¯¹äºŽMXFP4å†…æ ¸ï¼Œæ‚¨æ— éœ€åƒä»¥å‰é‚£æ ·ä½¿ç”¨`use_kernels=True`å‚æ•°ï¼Œå®ƒåœ¨transformersä¸­å·²é»˜è®¤å¯ç”¨ã€‚

åœ¨å…¼å®¹triton MXFP4å†…æ ¸çš„GPUä¸Šè¿è¡Œgpt-oss-20båŽï¼Œä½¿ç”¨Hugging Faceç¼“å­˜CLIè¿›è¡Œå¿«é€Ÿå®Œæ•´æ€§æ£€æŸ¥ï¼š

```shell
hf cache scan

```

ç¤ºä¾‹è¾“å‡ºï¼š

```shell
REPO ID                          REPO TYPE SIZE ON DISK
-------------------------------- --------- ------------
kernels-community/triton_kernels model           536.2K
openai/gpt-oss-20b               model            13.8G

```

è¿™è¡¨æ˜ŽMXFP4å†…æ ¸å·²è¢«èŽ·å–å¹¶å¯ç”¨äºŽæ‰§è¡Œã€‚

è®©æˆ‘ä»¬è¿è¡Œä¸€äº›åŸºå‡†æµ‹è¯•ï¼Œçœ‹çœ‹MXFP4å†…æ ¸çš„è¡¨çŽ°å¦‚ä½•ã€‚åœ¨å›¾4ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å¯¹äºŽè¾ƒå¤§çš„æ‰¹æ¬¡ï¼ŒMXFP4å†…æ ¸ç”šè‡³æ¯”è‡ªå®šä¹‰çš„MoEå’ŒRMSNormå†…æ ¸è¡¨çŽ°æ›´å¥½ã€‚

![MXFP4å†…æ ¸åŸºå‡†æµ‹è¯•](/images/posts/c43ccaa748fc.png)

## å¼ é‡å¹¶è¡Œ

![è§£é‡Šå¼ é‡å¹¶è¡Œ](/images/posts/2434150b9bb1.png)

å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰å°†å±‚å†…çš„å¼ é‡åˆ†å‰²åˆ°å¤šä¸ªGPUä¸Šï¼ˆå¦‚å›¾5æ‰€ç¤ºï¼‰ã€‚æ¯ä¸ªGPUå¹¶è¡Œè®¡ç®—å…¶åˆ†ç‰‡ï¼Œç„¶åŽä½¿ç”¨all-gatheræˆ–all-reduceæ“ä½œæ”¶é›†éƒ¨åˆ†ç»“æžœã€‚
è¿™å‡å°‘äº†æ¯ä¸ªGPUçš„å†…å­˜å ç”¨ï¼Œå¹¶ä½¿æ‰€æœ‰GPUåœ¨åŒä¸€å±‚ä¸Šå·¥ä½œï¼Œä»Žè€Œéšç€åºåˆ—é•¿åº¦æˆ–æ‰¹æ¬¡å¤§å°çš„å¢žé•¿æé«˜åžåé‡ã€‚TPæ˜¯é€šä¿¡å¯†é›†åž‹çš„ï¼Œé€šå¸¸åœ¨å…·æœ‰å¿«é€ŸèŠ‚ç‚¹å†…é“¾è·¯çš„å•å°æœºå™¨ä¸Šæ•ˆæžœæœ€ä½³ã€‚

### è¿™åœ¨transformersä¸­å®žçŽ°äº†ä»€ä¹ˆ

transformersç›´æŽ¥åœ¨`from_pretrained`ä¸­å®žçŽ°äº†TPã€‚æ‚¨å¯ä»¥ä»Žé¢„å®šä¹‰çš„è®¡åˆ’å¼€å§‹ï¼š

```python

import torch
from transformers import PreTrainedTokenizerFast, GptOssForCausalLM

model_id = "openai/gpt-oss-120b"
tokenizer = PreTrainedTokenizerFast.from_pretrained(model_id)
model = GptOssForCausalLM.from_pretrained(
    model_id,
    tp_plan="auto", 
    dtype="auto",
).eval()

messages = [
    {"role": "system", "content": "Be concise."},
    {"role": "user", "content": "Explain KV caching briefly."},
]
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt",
    return_dict=True,
    reasoning_effort="low",
).to(model.device)

with torch.inference_mode():
    generations = model.generate(**inputs, max_new_tokens=128)

print(tokenizer.decode(generations[0][inputs["input_ids"].shape[-1]:]))

```

å¦‚æžœæ‚¨æ²¡æœ‰è¿è¡Œä¸Šè¿°ä»£ç çš„åŸºç¡€è®¾æ–½ï¼Œå¯ä»¥ä½¿ç”¨Hugging Face Jobsåœ¨æˆ‘ä»¬çš„GPUä¸Šå¯åŠ¨ä¸€ä¸ªè¿›ç¨‹ï¼

```bash
hf jobs run --detach --flavor l4x4 ghcr.io/astral-sh/uv:debian /bin/bash -c \
  "uv venv .venv --python 3.12 && \
  source .venv/bin/activate && \
  uv pip install --upgrade torch numpy transformers accelerate triton kernels && \
  wget https://huggingface.co/datasets/ariG23498/distributed/raw/main/tp_gpt_oss.py && \
  torchrun --nproc-per-node=4 tp_gpt_oss.py"

```

hf jobså¯¹æ‰€æœ‰Hugging Face PROå’Œä¼ä¸šç”¨æˆ·å¼€æ”¾ã€‚

åœ¨åº•å±‚ï¼Œ`tp_plan="auto"`ä¸ºæ¯ä¸€å±‚é€‰æ‹©é¢„å®šä¹‰çš„åˆ†ç‰‡æ–¹æ¡ˆï¼Œå¹¶è¿žæŽ¥å¿…è¦çš„é›†åˆé€šä¿¡æ“ä½œã€‚å¦‚æžœæ‚¨æƒ³éªŒè¯æ­£åœ¨åˆ†ç‰‡çš„å†…å®¹ï¼Œå¯ä»¥ä½¿ç”¨`print(model._tp_plan)`æ£€æŸ¥æ´»åŠ¨è®¡åˆ’ã€‚

### ä½•æ—¶ä½¿ç”¨TP

å½“æ¨¡åž‹å¯¹äºŽå•ä¸ªGPUæ¥è¯´å¤ªå¤§ï¼Œå¹¶ä¸”æ‚¨å¸Œæœ›è¿›è¡Œå¹¶è¡Œè®¡ç®—è€Œä¸ä»…ä»…æ˜¯å†…å­˜æ”¾ç½®æ—¶ï¼Œè¯·ä½¿ç”¨TPã€‚TPå€¾å‘äºŽéšç€GPUæ•°é‡çš„å¢žåŠ è€Œæ‰©å±•åžåé‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºŽé•¿åºåˆ—æˆ–å¤§æ‰¹æ¬¡ã€‚

å¦‚æžœæ‚¨å¥½å¥‡TPä¸Ž`device_map="auto"`ï¼ˆå†…å­˜æ”¾ç½®ï¼‰æœ‰ä½•ä¸åŒï¼Œè¿™ä¸ªç®€çŸ­çš„Stack Overflowå›žç­”è§£é‡Šäº†ä¸¤è€…çš„åŒºåˆ«ä»¥åŠå„è‡ªçš„é€‚ç”¨åœºæ™¯ã€‚

è¦äº†è§£æ›´å¤šå…³äºŽTPçš„ä¿¡æ¯ï¼Œä»¥ä¸‹æ˜¯ä¸¤ä¸ªå¿…è¯»èµ„æºï¼š

- transformersæŒ‡å—ï¼šå¼ é‡å¹¶è¡Œã€æ”¯æŒçš„æ¨¡åž‹ã€è®¡åˆ’å’Œæ‰©å±•ç‚¹ã€‚
- è¶…å¤§è§„æ¨¡éƒ¨ç½²æ‰‹å†Œï¼šTPçš„èƒŒæ™¯åŠå…¶ä¸Žå…¶ä»–å¹¶è¡Œæ¨¡å¼çš„å…³ç³»ã€‚

## ä¸“å®¶å¹¶è¡Œ

ä¸“å®¶å¹¶è¡Œï¼ˆEPï¼‰å°†MoEå±‚å†…çš„ä¸“å®¶åˆ†ç‰‡åˆ°å¤šä¸ªGPUä¸Šã€‚æ¯ä¸ªTokenè¢«è·¯ç”±åˆ°ä¸€ä¸ªæˆ–å‡ ä¸ªä¸“å®¶ï¼Œå› æ­¤åªæœ‰è¿™äº›ä¸“å®¶è¿è¡Œå…¶å‰é¦ˆä¼ æ’­ã€‚ç”±äºŽä¸“å®¶æ˜¯ç‹¬ç«‹çš„MLPï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸åŒçš„ä¸“å®¶æ”¾ç½®åœ¨ä¸åŒçš„GPUä¸Šï¼Œå¹¶ä¸”åªä¸ºè¢«è·¯ç”±çš„Tokenäº¤æ¢éšè—çŠ¶æ€ã€‚è¿™ä½¿å¾—æ¯ä¸ªGPUä¸Šçš„çŸ©é˜µä¹˜æ³•ä¿æŒå®Œæ•´ï¼Œå¹¶ç”¨è·¯ç”±å’Œé›†åˆé€šä¿¡æ“ä½œæ›¿ä»£äº†å¼ é‡åˆ‡ç‰‡ã€‚

ä½¿ç”¨`torchrun`è¿è¡Œå¤šä¸ªè¿›ç¨‹ã€‚EPé€šè¿‡åˆ†å¸ƒå¼é…ç½®å¯ç”¨ï¼Œå¹¶åœ¨transformersä¸­å¼€ç®±å³ç”¨åœ°ä¸ŽGPT-OSS MoEå±‚ååŒå·¥ä½œã€‚

```python

import torch
from transformers import PreTrainedTokenizerFast, GptOssForCausalLM
from transformers.distributed import DistributedConfig

model_id = "openai/gpt-oss-120b"
tokenizer = PreTrainedTokenizerFast.from_pretrained(model_id)
model = GptOssForCausalLM.from_pretrained(
    model_id,
    distributed_config=DistributedConfig(enable_expert_parallel=True), 
    dtype="auto",
).eval()

messages = [
    {"role": "system", "content": "Be concise."},
    {"role": "user", "content": "Explain KV caching briefly."},
]
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt",
    return_dict=True,
    reasoning_effort="low",
).to(model.device)

with torch.inference_mode():
    generations = model.generate(**inputs, max_new_tokens=128)

print(tokenizer.decode(generations[0][inputs["input_ids"].shape[-1]:]))

```

ä»¥ä¸‹æ˜¯ä½¿ç”¨hf jobsè¿è¡Œçš„æ–¹å¼ï¼š

```bash
hf jobs run --detach --flavor l4x4 ghcr.io/astral-sh/uv:debian /bin/bash -c \
  "uv venv .venv --python 3.12 && \
  source .venv/bin/activate && \
  uv pip install --upgrade torch numpy transformers accelerate triton kernels && \
  wget https://huggingface.co/datasets/ariG23498/distributed/raw/main/ep_gpt_oss.py && \
  torchrun --nproc-per-node=4 ep_gpt_oss.py"

```

å½“æ‚¨å¯ç”¨ä¸“å®¶å¹¶è¡Œæ—¶ï¼Œå¼ é‡å¹¶è¡Œä¹Ÿä¼šè¢«æ¿€æ´»ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥åŒæ—¶äº«å—ä¸¤è€…çš„ä¼˜åŠ¿ï¼

## åŠ¨æ€æ»‘åŠ¨çª—å£å±‚ä¸Žç¼“å­˜

è®¸å¤šæœ€è¿‘çš„LLMä½¿ç”¨æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼Œæˆ–æ»‘åŠ¨ä¸Žå…¨å±€æ³¨æ„åŠ›å±‚çš„ç»„åˆï¼Œä½œä¸ºèŠ‚çœå†…å­˜å’Œå‡å°‘é‚£äº›éšåºåˆ—é•¿åº¦å¢žé•¿çš„æ˜‚è´µäºŒæ¬¡çŸ©é˜µä¹˜æ³•çš„ä¸€ç§æ‰‹æ®µã€‚ç„¶è€Œï¼Œtransformersä¸­åŠ¨æ€KVç¼“å­˜çš„å®žçŽ°è¿‡åŽ»å¸¸å¸¸æ ¹æ®åºåˆ—é•¿åº¦ç»§ç»­åˆ†é…ç©ºé—´ï¼Œè€Œä¸è€ƒè™‘å„ä¸ªæ³¨æ„åŠ›å±‚ã€‚æ‚¨å§‹ç»ˆå¯ä»¥ä½¿ç”¨ç¼–è¯‘ï¼ˆå³å›ºå®šå½¢çŠ¶ï¼‰æ¥ä¼˜åŒ–å†…å­˜ï¼Œä½†è¿™å®Œå…¨æ˜¯å¦ä¸€ç§æƒ…å†µã€‚

transformers çŽ°å·²å…·å¤‡åŠ¨æ€æ»‘åŠ¨çª—å£å±‚å’Œé…ç½®æ„ŸçŸ¥çš„åŠ¨æ€ç¼“å­˜ã€‚å¦‚æžœæ¨¡åž‹é…ç½®å£°æ˜Žäº†æ»‘åŠ¨çª—å£æˆ–æ··åˆæ³¨æ„åŠ›ï¼ˆåŒæ—¶ä½¿ç”¨æ»‘åŠ¨å’Œå…¨å±€æ³¨æ„åŠ›å±‚ï¼‰ï¼Œåˆ™æ»‘åŠ¨å±‚çš„ç¼“å­˜å°†åœ¨è¶…è¿‡çª—å£å¤§å°åŽåœæ­¢å¢žé•¿ã€‚å¦‚æžœä¸ä¼ é€’é…ç½®ï¼Œè¡Œä¸ºå°†ä¿æŒåŽŸæ ·ï¼ˆéšç€åºåˆ—é•¿åº¦å¢žé•¿ï¼ŒKV ç¼“å­˜æŒç»­çº¿æ€§å¢žé•¿ï¼‰ã€‚

å¯¹äºŽä»…ä½¿ç”¨æ»‘åŠ¨çª—å£å±‚çš„æ¨¡åž‹ï¼Œä¾‹å¦‚ Mistral 7Bï¼Œå½“åºåˆ—è¾¾åˆ°çª—å£å¤§å°ï¼ˆæœ¬ä¾‹ä¸­ä¸º 4096ï¼‰æ—¶ï¼Œç¼“å­˜å†…å­˜å°†åœæ­¢å¢žé•¿ã€‚è¿™æ˜¯åˆç†çš„ï¼Œå› ä¸ºæ»‘åŠ¨å±‚æ— è®ºå¦‚ä½•éƒ½æ— æ³•çœ‹åˆ°è¶…è¿‡å‰ 4K ä¸ª Token çš„å†…å®¹ã€‚

![mistral cache behaviour comparison](/images/posts/fde83db639c3.png)

OpenAI çš„ GPT-OSS åœ¨æ»‘åŠ¨å’Œå…¨å±€æ³¨æ„åŠ›å±‚ä¹‹é—´äº¤æ›¿ä½¿ç”¨ï¼Œè¿™å°†å¯¼è‡´ KV ç¼“å­˜æ€»å†…å­˜éšç€åºåˆ—é•¿åº¦å¢žåŠ è€Œå‡åŠï¼Œæˆ‘ä»¬ç¨åŽä¼šçœ‹åˆ°è¿™ä¸€ç‚¹ã€‚
è¿™ä¸ºæˆ‘ä»¬å¸¦æ¥äº†ä»¥ä¸‹ä¼˜åŠ¿ï¼š

- **å¤§å¹…é™ä½Ž KV ç¼“å­˜å†…å­˜**ï¼šå¯¹äºŽå…·æœ‰æ»‘åŠ¨æˆ–æ··åˆæ³¨æ„åŠ›çš„æ¨¡åž‹ï¼ˆä¾‹å¦‚ GPT-OSSï¼‰ï¼Œä¸€æ—¦è¾¾åˆ°çª—å£å¤§å°ï¼ˆä¾‹å¦‚ï¼ŒMistral ä¸º 4Kï¼›GPT-OSS æ»‘åŠ¨å±‚ä¸º 128ï¼‰ï¼Œç¼“å­˜å¢žé•¿å°±ä¼šè¶‹äºŽå¹³ç¨³ï¼Œè€Œä¸æ˜¯ä¸Žç”Ÿæˆçš„ Token æ€»æ•°æˆçº¿æ€§æ¯”ä¾‹å¢žé•¿ã€‚ï¼ˆGitHub, Transformersï¼‰
- **åœ¨é•¿æç¤ºè¯/é•¿ç”Ÿæˆåœºæ™¯ä¸‹èŽ·å¾—é€Ÿåº¦/å»¶è¿Ÿä¼˜åŠ¿**ï¼šæ›´å°çš„ KV å¼ é‡æ„å‘³ç€æ›´è½»é‡çš„æ³¨æ„åŠ›è¯»å†™æ“ä½œå’Œæ›´å°‘çš„å†…å­˜å¸¦å®½åŽ‹åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è¾¾åˆ°çª—å£å¤§å°ä¹‹åŽã€‚ï¼ˆè¿™æ˜¯æ»‘åŠ¨çª—å£/æ··åˆ LLM èƒŒåŽçš„æ ¸å¿ƒåŠ¨æœºã€‚ï¼‰ï¼ˆAI21, vLLM Blogï¼‰

### ä½¿ç”¨æ–¹æ³•

ä¼˜åŒ–åŽçš„ç¼“å­˜å·²é»˜è®¤å¯ç”¨ï¼Œè¿™æ„å‘³ç€**æ‚¨æ— éœ€å¯¹çŽ°æœ‰ä»£ç è¿›è¡Œä»»ä½•æ›´æ”¹**ã€‚å¦‚æžœæ‚¨æƒ³æ˜¾å¼åˆ›å»º `DynamicCache`ï¼Œå¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼æ“ä½œï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache

model_id = "openai/gpt-oss-20b"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    dtype="auto",
    device_map="auto",
).eval()

messages = [
    {"role": "system", "content": "Always respond in riddles"},
    {"role": "user", "content": "What is the weather like in Madrid?"},
]

inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt",
    return_dict=True,
    reasoning_effort="low",
).to(model.device)

cache = DynamicCache(config=model.config) 

generated = model.generate(
    **inputs,
    max_new_tokens=500,
    past_key_values=cache
)
print(tokenizer.decode(generated[0][inputs["input_ids"].shape[-1]:]))

```

å›¾ 6 å±•ç¤ºäº†ä½¿ç”¨åŠ¨æ€ KV ç¼“å­˜é…åˆæ»‘åŠ¨çª—å£æ³¨æ„åŠ›èƒ½å¸¦æ¥å¤šå¤§çš„æ€§èƒ½å·®å¼‚ã€‚

![sliding window cache](/images/posts/23e94acb132d.png)

## è¿žç»­æ‰¹å¤„ç†ä¸Žåˆ†é¡µæ³¨æ„åŠ›

å…¸åž‹çš„è‡ªå›žå½’ç”Ÿæˆè¿‡ç¨‹å¦‚å›¾ 7 æ‰€ç¤ºã€‚æ‚¨è¾“å…¥é¢„å¡«å…… Tokenï¼Œæ¨¡åž‹é€ä¸ªé¢„æµ‹æ¯ä¸ªæ–° Tokenï¼Œç›´åˆ°é¢„æµ‹å‡º EOSï¼ˆåºåˆ—ç»“æŸï¼‰Tokenã€‚

![prefilling](/images/posts/0cdbc147053b.png)

è®©æˆ‘ä»¬çœ‹çœ‹å½“ä¼ é€’ä¸€æ‰¹è¾“å…¥æ—¶ï¼Œç”Ÿæˆè¿‡ç¨‹æ˜¯æ€Žæ ·çš„ã€‚åœ¨å›¾ 8 ä¸­ï¼Œæ‚¨ä¼šæ³¨æ„åˆ°æœ‰äº›ç”Ÿæˆæ¯”å…¶ä»–ç”Ÿæˆæ›´æ—©ç»“æŸã€‚è¿™ç§é•¿åº¦ä¸åŒ¹é…å¯¼è‡´ GPU åˆ©ç”¨çŽ‡ä¸è¶³ã€‚

![static batching](/images/posts/c81b3859c552.png)

è¿™ç§æ‰¹å¤„ç†åºåˆ—çš„æ–¹å¼ç§°ä¸º**é™æ€æ‰¹å¤„ç†**ã€‚è™½ç„¶ç®€å•æ˜“æ‡‚ï¼Œä½†å®ƒæœ¬èº«å­˜åœ¨æ•ˆçŽ‡ä½Žä¸‹çš„é—®é¢˜ã€‚åªæœ‰åœ¨æ¯ä¸ªå¥å­å®Œå…¨ç”ŸæˆåŽï¼Œæˆ‘ä»¬æ‰èƒ½å¤„ç†ä¸‹ä¸€æ‰¹ã€‚

ä¸ºäº†ç»•è¿‡è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨**åŠ¨æ€æ‰¹å¤„ç†**ï¼ˆä¹Ÿç§°ä¸º**è¿žç»­æ‰¹å¤„ç†**ï¼‰ã€‚æˆ‘ä»¬ä¸æ˜¯ç­‰å¾…æ‰€æœ‰ç”Ÿæˆå®Œæˆï¼Œè€Œæ˜¯å°†ä¼ å…¥çš„è¯·æ±‚è°ƒåº¦ç»™å·²å®Œæˆçš„ç”Ÿæˆã€‚è¿™æ ·ï¼Œä¸€æ—¦æ‰¹æ¬¡ä¸­çš„æŸä¸ªç”Ÿæˆå®Œæˆï¼Œæˆ‘ä»¬å°±ç”¨ä¸‹ä¸€ä¸ªè¯·æ±‚é¢„å¡«å……è¯¥æ‰¹æ¬¡ã€‚è¯¥è¿‡ç¨‹å¦‚å›¾ 9 æ‰€ç¤ºã€‚

![continuous batching](/images/posts/7959fe29438d.png)

Transformers é€šè¿‡ `generate_batch` API æ”¯æŒè¿žç»­æ‰¹å¤„ç†ã€‚è¿™å¹¶éžç”¨äºŽç”Ÿäº§çº§æ¨¡åž‹æœåŠ¡â€”â€”åƒ vLLM å’Œ SGLang è¿™æ ·çš„æ¡†æž¶åœ¨è¿™æ–¹é¢è¡¨çŽ°å‡ºè‰²â€”â€”ä½†å¯¹äºŽè¯„ä¼°å’Œå®žéªŒéžå¸¸æœ‰å¸®åŠ©ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªåœ¨ `Qwen/Qwen3-4B-Instruct-2507` ä¸Šç«¯åˆ°ç«¯è¿è¡Œè¿žç»­æ‰¹å¤„ç†çš„ç¤ºä¾‹è„šæœ¬ã€‚

æˆ‘ä»¬è¿˜å¯¹è¿žç»­æ‰¹å¤„ç†å’Œé™æ€æ‰¹å¤„ç†è¿›è¡Œäº† 100 ä¸ªæ ·æœ¬çš„åŸºå‡†æµ‹è¯•ã€‚åœ¨å›¾ 9 ä¸­ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°è¿žç»­æ‰¹å¤„ç†æ¯”é™æ€æ‰¹å¤„ç†å¿«å¾—å¤šã€‚

![](/images/posts/94359c1cc503.png)

æ‚¨å¯ä»¥åœ¨æ­¤å¤„è¯•ç”¨è¯¥åŸºå‡†æµ‹è¯•ï¼šSB, CB

## æ›´å¿«åœ°åŠ è½½å¤§åž‹æ¨¡åž‹

å½“æ‚¨å°†å¤§åž‹æ¨¡åž‹åŠ è½½åˆ° GPU æ—¶ï¼ŒPyTorch éœ€è¦ä¸ºæ¯ä¸€å±‚çš„æƒé‡é¢„ç•™ GPU å†…å­˜ã€‚æ¯ä¸ªè¿™æ ·çš„è¯·æ±‚ï¼ˆæ¯å±‚ï¼‰éƒ½éœ€è¦æ—¶é—´ï¼Œå¯¹äºŽæ•°åäº¿å‚æ•°çš„æ¨¡åž‹æ¥è¯´ï¼Œè¿™å¯èƒ½æ„å‘³ç€**æ•°åƒæ¬¡å¾®å°çš„å†…å­˜åˆ†é…**ï¼Œå¯¼è‡´æ¨¡åž‹å‡†å¤‡å°±ç»ªå‰éœ€è¦ç­‰å¾…å¾ˆé•¿æ—¶é—´ã€‚ä¸Žå…¶æ¯æ¬¡éƒ½å‘ GPU è¯·æ±‚æ–°å†…å­˜ï¼Œä¸å¦‚**ä¸€æ¬¡æ€§æŒæœ‰ä¸€å¤§å—å†…å­˜**ï¼Œç„¶åŽä»Žä¸­å¿«é€Ÿåˆ†é…åˆ‡ç‰‡ã€‚

PyTorch åˆ†é…å™¨å®Œå…¨å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚å…³é”®åœ¨äºŽï¼Œåˆ†é…å™¨åªæœ‰åœ¨æ‚¨ç»™å®ƒä¸€äº›å†…å­˜å¤„ç†åŽæ‰ä¼šå˜å¿«ã€‚å¦‚æžœæ‚¨ä¸å…ˆâ€œå‚¨å¤‡åº“å­˜â€ï¼Œæ‚¨ä»ç„¶éœ€è¦è¿›è¡Œè®¸å¤šç¼“æ…¢çš„â€œå¸‚åœºé‡‡è´­â€ã€‚è¿™ä¸ª PRï¼ˆðŸŽ‰#36380ï¼‰æ•™ä¼šäº† transformers åœ¨å¼€å§‹å¤åˆ¶æ¨¡åž‹æƒé‡ä¹‹å‰**é¢„å…ˆå‚¨å¤‡åº“å­˜**ã€‚

- æŸ¥çœ‹ `device_map`ï¼ˆæ¯å±‚å°†å­˜æ”¾çš„ä½ç½®ï¼‰ã€‚
- åœ¨æ¯ä¸ª GPU ä¸Šé¢„åˆ†é…è¶³å¤Ÿå¤§çš„å†…å­˜å—ã€‚
- ç„¶åŽï¼Œéšç€å„å±‚è¢«å¤åˆ¶è¿›æ¥ï¼Œå®ƒä»¬åªéœ€æ•´é½åœ°æ”¾å…¥è¿™ä¸ªé¢„å…ˆé¢„ç•™çš„ç©ºé—´ã€‚

æ‚¨æ— éœ€å¯¹çŽ°æœ‰ä»£ç è¿›è¡Œä»»ä½•æ›´æ”¹ï¼Œå› ä¸ºè¿™æ˜¯ transformers çš„é»˜è®¤è¡Œä¸ºã€‚å¦‚æžœæ‚¨ä½¿ç”¨ `device_map="auto"` æˆ–æä¾›è‡ªå·±çš„è®¾å¤‡æ˜ å°„ï¼Œæ‚¨çš„æ¨¡åž‹çŽ°åœ¨å°†è‡ªåŠ¨åŠ è½½å¾—æ›´å¿«ã€‚å¦‚æžœæ‚¨ä½¿ç”¨**å¼ é‡å¹¶è¡Œ**ï¼ˆ`tp_plan="auto"`ï¼‰å’Œ `torchrun`ï¼Œæ‚¨è¿˜å¯ä»¥å—ç›ŠäºŽä½¿å¤š GPU åŠ è½½æ›´æ™ºèƒ½çš„é…å¥—æ›´æ”¹ã€‚

## ç»“è®º

transformers å‘å±•è¿…é€Ÿï¼Œå¹¶ä¸”ä»¥ç¤¾åŒºä¸ºå…ˆã€‚è¯¥åº“ä»¥è¯¥é¢†åŸŸçš„å‘å±•é€Ÿåº¦æ¼”è¿›ï¼Œå› ä¸ºè´¡çŒ®è€…åœ¨å¼€æ”¾çŽ¯å¢ƒä¸­å¡‘é€ å®ƒã€‚ä¸ºæ–°æ¨¡åž‹æ·»åŠ çš„ç»„ä»¶æˆä¸ºå·¥å…·åŒ…çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶åœ¨æœªæ¥çš„é›†æˆä¸­é‡å¤ä½¿ç”¨ã€‚

è¿™ç§é€Ÿåº¦ä½¿å¾—åƒ GPT-OSS ç³»åˆ—è¿™æ ·çš„é›¶æ—¥é›†æˆæˆä¸ºå¯èƒ½ã€‚éšç€æŠ€æœ¯æ ˆæ—¥ç›Š**ä»¥ PyTorch ä¸ºå…ˆ**ï¼Œå®ƒç²¾ç®€äº†å†—ä½™ï¼Œå¹¶åŠ å€æŠ•å…¥å®žè·µä¸­é‡è¦çš„ PyTorch è·¯å¾„ã€‚å…¶ç»“æžœæ˜¯å½¢æˆäº†ä¸€ä¸ªæ›´ç®€æ´çš„æ ¸å¿ƒï¼Œé€šè¿‡ç¤¾åŒºå†…æ ¸ã€é‡åŒ–å’Œå¹¶è¡Œè®¡åˆ’è§£é”äº†æ–°åŠŸèƒ½ï¼ŒåŒæ—¶ä¹Ÿ**æ ‡å‡†åŒ–äº†æ¨¡åž‹å®šä¹‰**ï¼Œä½¿å¾— transformers æ”¯æŒçš„æž¶æž„æˆä¸ºå‚è€ƒæ ‡å‡†ï¼Œå¹¶æ‰©å±•åˆ°æ›´å¹¿æ³›çš„ç”Ÿæ€ç³»ç»Ÿä¸­ã€‚

è¿™ç¯‡æ–‡ç« æ˜¯æˆ‘ä»¬åå¤è¿­ä»£ã€æœç€åŒä¸€æ–¹å‘å‰è¿›çš„è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªå¿«ç…§ï¼šæœåŠ¡äºŽç¤¾åŒºçš„éœ€æ±‚ã€‚è¦äº†è§£ transformers çš„æœ€æ–°æ·»åŠ å†…å®¹ï¼Œè¯·æŸ¥çœ‹æ–‡æ¡£å’Œå‘å¸ƒè¯´æ˜Žã€‚å¹¶ä¸”ï¼Œè¯·ç»§ç»­åˆ†äº«æ‚¨çš„åé¦ˆï¼Œå¹¶åœ¨ transformers ä¸­å‘å¸ƒæ‚¨çš„æ¨¡åž‹ï¼Œä¾›ç¤¾åŒºäº«ç”¨ ðŸ¤—

å¦‚æžœæ‚¨æƒ³æ·±å…¥äº†è§£ç‰¹å®šä¸»é¢˜ï¼Œä»¥ä¸‹æ˜¯å€¼å¾—è®¿é—®çš„é“¾æŽ¥åˆ—è¡¨ï¼š

1. Hugging Face GPT-OSS Recipes Repository
2. Welcome GPT OSS: OpenAI's New Open-Source Model Family
3. OpenAI Cookbook: GPT-OSS Topic
4. Transformers Documentation: Distributed Inference on Multiple GPUs
5. Matthew Carrigan's X Thread on GPT OSS Innovations
6. YouTube Video: OpenAI GPT OSS Announcement
7. Transformers PR #36380: Faster Model Loading on Accelerators
8. Transformers PR #36335: Update from_pretrained for Tensor Parallelism
9. Transformers PR #40039: New Dynamic Sliding Window Layer and Cache
10. HAN Lab Blog: How Attention Sinks Keep Language Models Stable

---

> æœ¬æ–‡ç”±AIè‡ªåŠ¨ç¿»è¯‘ï¼ŒåŽŸæ–‡é“¾æŽ¥ï¼š[Tricks from OpenAI gpt-oss YOU ðŸ«µ can use with transformers](https://huggingface.co/blog/faster-transformers)
> 
> ç¿»è¯‘æ—¶é—´ï¼š2026-02-10 04:35
