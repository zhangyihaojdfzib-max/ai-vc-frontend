---
title: Vercel AI Gateway如何借助Fluid计算实现高效运行
title_original: How AI Gateway runs on Fluid compute - Vercel
date: '2025-11-06'
source: Vercel Blog
source_url: https://vercel.com/blog/how-ai-gateway-runs-on-fluid-compute
author: ''
summary: 本文介绍了Vercel的AI Gateway服务如何利用其Fluid计算模型实现大规模、低成本运行。AI Gateway作为统一的Node.js接口，每天处理数十亿Token，连接数百个AI模型。关键创新在于Fluid的Active
  CPU定价模式：仅在CPU实际处理请求时按CPU费率计费，而在等待AI提供商响应时仅支付较低的内存费率。这使得AI Gateway仅需为不到8%的运行时支付CPU费用，而非传统无服务器模式的100%。文章还阐述了其基于Vercel全球交付网络和标准Next.js项目的技术架构，实现了低延迟的全球流量处理。
categories:
- AI基础设施
tags:
- Vercel
- AI Gateway
- 无服务器计算
- 成本优化
- 全球网络
draft: false
translated_at: '2026-02-02T04:24:19.701100'
---

AI Gateway 是一项 Node.js 服务，可通过单一接口连接数百个 AI 模型。它每天处理数十亿 Token。支撑如此大规模运行的秘诀是 Fluid。

![Vercel AI Gateway 是一个标准项目，以与我们客户相同的方式使用 Vercel，每天处理数十亿 Token。](/images/posts/699236c9a943.jpg)

![Vercel AI Gateway 是一个标准项目，以与我们客户相同的方式使用 Vercel，每天处理数十亿 Token。](/images/posts/48bb0b4e33bf.jpg)

![Vercel AI Gateway 是一个标准项目，以与我们客户相同的方式使用 Vercel，每天处理数十亿 Token。](/images/posts/05b058fd2ba6.jpg)

![Vercel AI Gateway 是一个标准项目，以与我们客户相同的方式使用 Vercel，每天处理数十亿 Token。](/images/posts/64112affe45c.jpg)

当我们宣布其全面上市时，我们重点介绍了 AI Gateway 如何高效扩展、安全地路由请求以及简化与多个 AI 提供商的连接。

我们查看了其上线第一个月的数据。AI Gateway 总共处理了大约 16,000 个运行时小时，但其中只有 1,200 小时涉及实际的 CPU 工作（处理请求、路由逻辑、流式响应）。剩余的 14,800 小时都花在等待 AI 提供商响应上。

传统的无服务器平台按挂钟时间计费。您的函数存活的每一毫秒，您都需要付费。而通过 Fluid 和 Active CPU 定价，您只需在 CPU 实际运行时支付 CPU 费率。其余时间（当 AI Gateway 在等待 OpenAI 或 Anthropic 时）您只需支付较低的内存费率。对于 AI Gateway 来说，这意味着只需为不到 8% 的运行时支付 CPU 费率，而不是 100%。

通过将 AI Gateway 的模型抽象与 Fluid 快速、经济高效的基础设施相结合，Vercel 消除了构建 AI 功能中最困难的部分。团队可以更快地交付产品，而无需担心提供商集成或底层计算问题。

## 我们如何用 Vercel 构建 Vercel

从高层次看，AI Gateway 是一个使用 Next.js 部署在 Vercel 上的 Node.js 应用，尽管您也可以使用任何提供 REST API 的后端框架构建类似的东西。

尽管架构简单，AI Gateway 却能以极低的延迟处理全球规模的流量。该应用程序运行在 Vercel 分布式基础设施上，覆盖多个 AWS 区域，使用与每位 Vercel 客户相同的优质网络、全球区域和计算模型。

当用户从 Vercel 外部连接到 AI Gateway 时，他们的请求会通过加速路径进入我们的网络。对于在 AWS 内部部署的用户，连接已经是本地的，通过云内路由为他们提供持续的低延迟。

在产品内部，AI Gateway 只是一个标准的 Vercel 项目，没有任何特殊的基础设施或特权访问。该应用程序使用与您相同的框架定义的基础设施、零配置部署、可观测性和安全性。

## 技术内幕：全球交付网络

当请求进入 AI Gateway 时，首先由 Vercel 全球交付网络接收。这是一个全球分布式系统，结合了任播路由、接入点（PoP）和私有骨干网连接，以最小化延迟。

该网络持续评估每个端点的健康状况和拥塞情况，根据邻近度和实时性能遥测数据将请求路由到最优的 PoP。从那里，流量被移交到最近的计算能力区域，请求在支持每个 Vercel 部署的同一全球基础设施内运行。

![AI Gateway 通过 Vercel 的全球交付网络路由请求，以实现更快的响应和低延迟的云内路由。](/images/posts/1f517ae253fe.jpg)

![AI Gateway 通过 Vercel 的全球交付网络路由请求，以实现更快的响应和低延迟的云内路由。](/images/posts/f8ad2098de02.jpg)

![AI Gateway 通过 Vercel 的全球交付网络路由请求，以实现更快的响应和低延迟的云内路由。](/images/posts/49717f7b5bc5.jpg)

![AI Gateway 通过 Vercel 的全球交付网络路由请求，以实现更快的响应和低延迟的云内路由。](/images/posts/7fc98d659821.jpg)

交付网络确保了用户、网络区域位置和 AI 提供商之间数据的高吞吐量、低延迟传输。从客户端入口到提供商响应的每一跳，都保持在 Vercel 管理的边界内，避免了不可预测的公共互联网路由。

这种架构在全球范围内提供了一致的性能，对于大多数客户而言，往返时间保持在个位数毫秒，同时通过边缘请求指标保持完全的可见性。

一旦路由完成，请求到达 AI Gateway 的应用层，并在运行于 Fluid 计算上的 Vercel Function 中执行。该函数使用 OIDC Token 对 Vercel 托管的应用程序进行身份验证，或使用 API 密钥进行外部集成。该函数通过区域内的 Redis 验证配额，并为目标 AI 提供商准备有效负载。转发请求后，该函数通过相同的低延迟网络路径流式传输响应。

```
1import { generateText } from 'ai';2
3const { text } = await generateText({4  model: 'anthropic/claude-sonnet-4',5  prompt: 'Explain how request routing works',6});
```

使用 AI SDK 与 AI Gateway 的示例

## 技术内幕：由 Fluid 计算驱动

AI Gateway 运行在 Fluid 计算上，这是我们为高并发、网络密集型工作负载设计的下一代运行时。Fluid 的行为类似于动态、短寿命的服务器，既保持了无服务器的弹性和部署模型，又能在多次调用间复用底层云资源，实现类似服务器的效率。

传统的无服务器模型要求每次调用都有一个单独的实例。即使进行了预热，每个传统的无服务器实例启动时都没有内存或状态。

Fluid 改变了这一点，它不仅能在实例运行结束后跨调用复用它们，还能在它们运行时通过函数内并发进行复用。这使得内存数据、打开的套接字和运行时缓存可以在实例的整个生命周期内持续存在。当一个调用暂停以等待提供商响应时，另一个调用可以立即在同一实例内执行。

这种并发模型保持了 CPU 的高利用率并降低了成本。通过 Active CPU 定价，您只需为代码主动运行的毫秒数付费。

![AI Gateway 请求运行在 Fluid 计算上，结合了无服务器的可扩展性和服务器的并发性，以减少跨调用的网络开销。](/images/posts/d3759b2c89ec.jpg)

![AI Gateway 请求运行在 Fluid 计算上，结合了无服务器的可扩展性和服务器的并发性，以减少跨调用的网络开销。](/images/posts/85e9a951fb9b.jpg)

![AI Gateway 请求运行在 Fluid 计算上，结合了无服务器的可扩展性和服务器的并发性，以减少跨调用的网络开销。](/images/posts/ccff5ee21b47.jpg)

![AI Gateway 请求运行在 Fluid 计算上，结合了无服务器的可扩展性和服务器的并发性，以减少跨调用的网络开销。](/images/posts/0a0868506301.jpg)

由于 Fluid 实例被复用，这些实例可以在内存中直接存储小型、短期的缓存，例如提供商路由、凭证或配额快照等频繁访问的数据。这减少了冗余的 Redis 查找，并最小化了热路径上的延迟。当流量激增时，Fluid 可以立即扩展。当流量平静下来时，实例会优雅地退役。基础设施保持无服务器般的弹性，但性能却像一台经过调优、始终温热的服务器。

## 状态、缓存和全局协调

AI Gateway 通过 Vercel Marketplace 配置 Redis，以实现全局一致性和配额追踪，同时结合 Fluid 进行临时内存缓存以提升本地速度。频繁访问的凭证、供应商元数据和配额计数器存储在实例内存中，以实现亚毫秒级访问。

我们在后台异步刷新 Redis 中的值，以最小化对实时流量的影响。写入和使用量增量会被批量处理，并在响应完成后写回 Redis。

每个 Vercel 区域都维护自己的 Redis 集群，以确保配额验证和使用量更新在本地进行。这种区域隔离使得即使在全局规模下，延迟也保持可预测。

## 由内而外的监控

每个 Fluid 实例持续向 AI Gateway 监控服务提供来自两个互补系统的实时指标：频繁的健康检查和内存统计信息。

健康检查持续测量核心指标，如错误率、首 Token 时间（TTFT）和每秒 Token 吞吐量。同时，每个 Fluid 实例维护自己的一组实时计数器，追踪活动调用、内存利用率和供应商延迟。这两个来源形成了一个自我修正的反馈循环。监控系统将内存中的遥测数据与全局健康检查进行比较，并利用这些数据自动调整路由、扩展实例或在区域间转移流量。

![Continuous checks are performed by both in-memory services and a global system, relaying feedback of provider and model performance to the entire network.](/images/posts/61d9a267c0a9.jpg)

![Continuous checks are performed by both in-memory services and a global system, relaying feedback of provider and model performance to the entire network.](/images/posts/c002ab5f1576.jpg)

![Continuous checks are performed by both in-memory services and a global system, relaying feedback of provider and model performance to the entire network.](/images/posts/e1f3b2b9b7f5.jpg)

![Continuous checks are performed by both in-memory services and a global system, relaying feedback of provider and model performance to the entire network.](/images/posts/62c9c921b142.jpg)

如果某个供应商区域开始返回较慢的响应，AI Gateway 会在无需人工干预或更改应用程序的情况下，将新请求重新路由到更健康的供应商。即使底层模型 API 发生波动，该系统也能确保可靠性和弹性。

AI Gateway 可以为同一模型跨多个供应商路由请求，现在甚至可以在发生错误、上下文大小不匹配或其他不兼容问题时回退到其他模型。

例如，Claude Sonnet 4 可通过 Anthropic、Amazon Bedrock 和 Google Vertex AI 使用。您可以控制使用哪些供应商及其顺序：

```
1import { streamText } from 'ai';2
3const result = streamText({4  model: 'anthropic/claude-sonnet-4',5  prompt: 'Write a technical explanation',6  providerOptions: {7    gateway: {8      order: ['vertex', 'bedrock', 'anthropic'],9    },10  },11});
```

如果主要供应商不可用，AI Gateway 会自动尝试下一个供应商。每个响应都包含详细的元数据，描述哪个供应商处理了请求、任何回退尝试、延迟和总 Token 成本。这种回退系统提高了可靠性，而无需对应用程序代码进行任何更改。

Vercel 通过 Vercel Observability 原生提供对 AI Gateway 活动的可见性，为开发者实时提供有关延迟、供应商健康状况、Token 计数和成本的详细指标。

![Vercel Observability provides native visibility into every model call, including overall request volume, spend, and performance.](/images/posts/164c245c9c8d.jpg)

![Vercel Observability provides native visibility into every model call, including overall request volume, spend, and performance.](/images/posts/c7eeb05e712d.jpg)

## 为什么 Fluid 计算是合适的选择

AI Gateway 大部分时间处于等待状态，而非计算状态。路由请求、验证凭证和检查配额只需几毫秒。等待 OpenAI 或 Anthropic 流式返回响应则需要数秒。这是一个网络密集型工作负载，而非计算密集型工作负载。

动态 CPU 定价与实际工作模式相匹配。在那些等待的秒数内，您只需为内存配置付费，而非全额 CPU 费率。对于像 AI Gateway 这样大部分时间花在等待网络响应的工作负载，这种定价模式消除了不必要的成本。

## 与 Vercel 共同构建

通过利用 Fluid 的并发模型和 Vercel 的分布式网络，AI Gateway 展示了当无服务器架构超越简单函数时，现代基础设施应有的面貌。它仍然是即时且弹性的，但现在更加智能、高效且能自我优化。为 AI Gateway 提供支持的相同架构，可供每位在 Vercel 上构建下一代 AI 驱动应用程序的开发者使用。

---

> 本文由AI自动翻译，原文链接：[How AI Gateway runs on Fluid compute - Vercel](https://vercel.com/blog/how-ai-gateway-runs-on-fluid-compute)
> 
> 翻译时间：2026-02-02 04:24
