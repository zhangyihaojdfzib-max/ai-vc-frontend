---
title: VideoPrism：谷歌推出通用视频理解基础视觉编码器
title_original: 'VideoPrism: A foundational visual encoder for video understanding'
date: '2024-02-22'
source: Google AI Blog
source_url: http://blog.research.google/2024/02/videoprism-foundational-visual-encoder.html
author: null
summary: 谷歌研究团队推出视频基础模型VideoPrism，旨在处理广泛的视频理解任务。该模型在包含3600万高质量视频-文本对及5.82亿带噪声文本视频片段的大规模数据集上进行预训练，采用两阶段训练策略，结合视频-文本对比学习和掩码视频建模。VideoPrism在33个视频理解基准测试中的30个上实现了最先进的性能，仅需对单一冻结模型进行最小适应即可应用于分类、定位、检索、描述和问答等多种任务。
categories:
- AI研究
tags:
- 视频理解
- 基础模型
- 计算机视觉
- 谷歌研究
- 多模态AI
draft: false
---

VideoPrism：用于视频理解的基础视觉编码器
2024年2月22日
作者：Google Research 高级研究科学家 Long Zhao，高级主任软件工程师 Ting Liu
快速链接
网络上存在数量惊人的视频，涵盖从人们分享的日常瞬间到历史时刻再到科学观察的各种内容，每一段视频都包含着对世界的独特记录。合适的工具可以帮助研究人员分析这些视频，从而改变我们理解周围世界的方式。
视频提供的动态视觉内容远比静态图像丰富，能够捕捉实体的运动、变化和动态关系。分析这种复杂性，以及公开视频数据的巨大多样性，需要超越传统图像理解的模型。因此，许多在视频理解上表现最佳的方法仍然依赖于为特定任务量身定制的专用模型。最近，该领域利用视频基础模型取得了令人兴奋的进展，例如 VideoCLIP、InternVideo、VideoCoCa 和 UMT。然而，构建一个能够处理视频数据巨大多样性的视频基础模型仍然是一个挑战。
为了构建一个用于通用视频理解的单一模型，我们推出了“VideoPrism：用于视频理解的基础视觉编码器”。VideoPrism 是一个视频基础模型，旨在处理广泛的视频理解任务，包括分类、定位、检索、描述和问答。我们在预训练数据和建模策略上都提出了创新。我们在一个大规模且多样化的数据集上对 VideoPrism 进行预训练：3600 万个高质量视频-文本对，以及 5.82 亿个带有噪声或机器生成并行文本的视频片段。我们的预训练方法专为这种混合数据设计，旨在同时从视频-文本对和视频本身中学习。VideoPrism 非常容易适应新的视频理解挑战，并且使用单一的冻结模型就能实现最先进的性能。
预训练数据
一个强大的视频基础模型需要一个非常庞大的视频集合来进行训练——类似于其他基础模型，例如那些用于大语言模型的基础模型。理想情况下，我们希望预训练数据能够代表世界上所有视频的样本。虽然自然情况下，这些视频大多没有完美的字幕或描述，但即使是不完美的文本也能提供有关视频语义内容的有用信息。
为了让我们的模型拥有最佳的起点，我们汇集了一个由多个公共和私有数据集组成的大规模预训练语料库，包括 YT-Temporal-180M、InternVid、VideoCC、WTS-70M 等。这包括 3600 万个精心挑选的带有高质量字幕的视频，以及额外的 5.82 亿个带有不同程度噪声文本（如自动生成的字幕）的片段。据我们所知，这是同类中规模最大、最多样化的视频训练语料库。
| 视频-文本预训练数据的统计信息。CLIP 相似度得分（越高越好）的巨大差异表明了我们预训练数据字幕质量的多样性，这是用于收集文本的各种方法的副产品。|
两阶段训练
VideoPrism 模型架构源于标准的视觉 Transformer，采用因子化设计，遵循 ViViT 的顺序编码空间和时间信息。我们的训练方法利用了上述高质量视频-文本数据和带有噪声文本的视频数据。首先，我们使用对比学习（一种最小化正视频-文本对之间的距离，同时最大化负视频-文本对之间的距离的方法）来教导我们的模型将视频与其自身的文本描述（包括不完美的描述）进行匹配。这为将语义语言内容与视觉内容相匹配奠定了基础。
在视频-文本对比训练之后，我们利用没有文本描述的视频集合。在这里，我们在掩码视频建模框架的基础上进行改进，以预测视频中的掩码图像块。我们训练模型同时预测来自第一阶段模型的视频级全局嵌入和 Token 级嵌入，以有效利用该阶段获得的知识。然后，我们随机打乱预测的 Token，以防止模型学习捷径。
VideoPrism 设置的独特之处在于，我们使用了两种互补的预训练信号：文本描述和视频内的视觉内容。文本描述通常侧重于事物的外观，而视频内容则提供有关运动和视觉动态的信息。这使得 VideoPrism 在需要同时理解外观和运动的任务中表现出色。
结果
我们在四大类视频理解任务上对 VideoPrism 进行了广泛评估，包括视频分类与定位、视频-文本检索、视频描述、问答以及科学视频理解。VideoPrism 在 33 个视频理解基准测试中的 30 个上实现了最先进的性能——所有这些都只需对一个单一的冻结模型进行最小程度的适应。
| VideoPrism 与先前性能最佳的基础模型对比。|
分类与定位
我们在现有的大规模视频理解基准测试（VideoGLUE）上评估 VideoPrism，该基准涵盖分类和定位任务。我们发现：（1）VideoPrism 优于所有其他最先进的基础模型；（2）没有其他单一模型能始终位居第二。这告诉我们，VideoPrism 已经学会将各种视频信号有效地打包到一个编码器中——从不同粒度的语义到外观和运动线索——并且它在各种视频源上都能良好工作。
| VideoPrism 在视频理解基准测试中优于最先进的方法（包括 CLIP、VATT、InternVideo 和 UMT）。在此图中，我们展示了与先前最佳模型相比的绝对分数差异，以突出 VideoPrism 的相对改进。在 Charades、ActivityNet、AVA 和 AVA-K 上，我们使用平均精度均值作为评估指标。在其他数据集上，我们报告 top-1 准确率。|
与大语言模型结合
我们进一步探索将 VideoPrism 与大语言模型结合，以释放其处理各种视频-语言任务的能力。特别是，当与文本编码器（遵循 LiT）或语言解码器（如 PaLM-2）配对时，VideoPrism 可用于视频-文本检索、视频描述和视频问答任务。我们在一个广泛且具有挑战性的视觉-语言基准测试集上比较了这些组合模型。VideoPrism 在大多数基准测试中创造了新的最佳水平。从视觉结果来看，我们发现 VideoPrism 能够理解视频中复杂的运动和外观（例如，在下面的视觉示例中，模型可以识别窗户上旋转物体的不同颜色）。这些结果表明 VideoPrism 与语言模型具有很强的兼容性。
| VideoPrism 在多个视频-文本检索（上图）以及视频描述和视频问答（下图）基准测试中，与最先进的方法（包括 VideoCoCa、UMT 和 Flamingo）相比取得了有竞争力的结果。我们还展示了与先前最佳模型相比的绝对分数差异，以突出 VideoPrism 的相对改进。我们报告了 MASRVTT、VATEX 和 ActivityNet 上的 Recall@1，MSRVTT-Cap、VATEX-Cap 和 YouCook2 上的 CIDEr 分数，MSRVTT-QA 和 MSVD-QA 上的 top-1 准确率，以及 NExT-QA 上的 WUPS 指数。|
科学应用
最后，我们在跨领域科学家使用的数据集上测试 VideoPrism，包括动物行为学、行为神经科学和生态学等领域。

这些数据集通常需要领域专业知识进行标注，为此我们利用了社区开源的科学数据集，包括Fly vs. Fly、CalMS21、ChimpACT和KABR。VideoPrism不仅表现异常出色，甚至超越了专门为这些任务设计的模型。这表明像VideoPrism这样的工具有潜力改变科学家们跨不同领域分析视频数据的方式。

**结论**
通过VideoPrism，我们引入了一个强大且通用的视频编码器，为通用视频理解设立了新标准。我们强调构建大规模、多样化的预训练数据集以及创新的建模技术，这一点已通过广泛的评估得到验证。VideoPrism不仅持续超越强大的基线模型，其独特的泛化能力也使其能很好地应对一系列现实世界的应用。鉴于其广泛的潜在用途，我们承诺将在人工智能原则的指导下，继续在这一领域进行负责任的研究。我们希望VideoPrism能为人工智能与视频分析交叉领域的未来突破铺平道路，帮助实现视频基础模型在科学发现、教育和医疗等领域的潜力。

**致谢**
这篇博文代表所有VideoPrism作者发布：Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming-Hsuan Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, and Boqing Gong。我们衷心感谢David Hendon的产品管理工作，以及Alex Siegman, Ramya Ganeshan, 和 Victor Gomes的项目与资源管理工作。我们也感谢Hassan Akbari, Sherry Ben, Yoni Ben-Meshulam, Chun-Te Chu, Sam Clearwater, Yin Cui, Ilya Figotin, Anja Hauth, Sergey Ioffe, Xuhui Jia, Yeqing Li, Lu Jiang, Zu Kim, Dan Kondratyuk, Bill Mark, Arsha Nagrani, Caroline Pantofaru, Sushant Prakash, Cordelia Schmid, Bryan Seybold, Mojtaba Seyedhosseini, Amanda Sadler, Rif A. Saurous, Rachel Stigler, Paul Voigtlaender, Pingmei Xu, Chaochao Yan, Xuan Yang, and Yukun Zhu的讨论、支持和反馈，这些对本工作贡献巨大。我们感谢Jay Yagnik, Rahul Sukthankar, 和 Tomas Izo对本项目的热情支持。最后，我们感谢Tom Small, Jennifer J. Sun, Hao Zhou, Nitesh B. Gundavarapu, Luke Friedman, 和 Mikhail Sirotenko在制作这篇博文过程中提供的巨大帮助。

---

> 本文由AI自动翻译，原文链接：[VideoPrism: A foundational visual encoder for video understanding](http://blog.research.google/2024/02/videoprism-foundational-visual-encoder.html)
> 
> 翻译时间：2026-01-05 17:15
