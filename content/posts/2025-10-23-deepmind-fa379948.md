---
title: VaultGemma：全球最强差分隐私大语言模型发布
title_original: 'VaultGemma: The world''s most capable differentially private LLM'
date: '2025-10-23'
source: Google DeepMind
source_url: https://deepmind.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/
author: ''
summary: Google Research与DeepMind合作推出VaultGemma，这是目前全球最大、能力最强的开源差分隐私大语言模型。研究团队通过建立差分隐私语言模型的扩展定律，揭示了计算、隐私与效用之间的权衡关系，并指导了模型的训练。VaultGemma基于Gemma架构，旨在以数学严谨的差分隐私技术构建以隐私为核心的人工智能，推动下一代隐私保护AI的发展。模型权重及相关技术报告已在Hugging
  Face和Kaggle平台发布。
categories:
- AI研究
tags:
- 差分隐私
- 大语言模型
- Google Research
- AI安全
- 开源模型
draft: false
translated_at: '2026-01-23T04:40:27.038895'
---

1. 首页
2. 博客

# VaultGemma：全球能力最强的差分隐私大语言模型

Amer Sinha，软件工程师，与 Ryan McKenna，研究科学家，Google Research

我们推出 VaultGemma，这是目前从头开始训练、能力最强的差分隐私模型。

- 论文
- Hugging Face
- Kaggle
- 技术报告
- 分享复制链接×

- 复制链接×

随着人工智能日益融入我们的生活，构建以隐私为核心的人工智能是该领域的一个关键前沿。**差分隐私**通过添加校准噪声来防止记忆，提供了一种数学上严谨的解决方案。然而，将差分隐私应用于大语言模型会带来权衡取舍。理解这些权衡至关重要。应用差分隐私噪声会改变传统的**扩展定律**——描述性能动态的规则——具体表现为降低训练稳定性（模型在没有经历损失峰值或发散等灾难性事件的情况下持续学习的能力），并显著增加批量大小（同时发送给模型进行处理的一批训练样本）和计算成本。

我们与 Google DeepMind 合作进行的新研究《差分隐私语言模型的扩展定律》，建立了能够精确模拟这些复杂性的定律，为计算-隐私-效用之间的权衡提供了完整的图景。在这项研究的指导下，我们很高兴地推出 VaultGemma，这是目前最大的（10亿参数）、从头开始使用差分隐私训练的开源模型。我们将在 **Hugging Face** 和 **Kaggle** 上发布模型权重，并附上一份**技术报告**，以推动下一代隐私保护人工智能的发展。

## 理解扩展定律

通过精心设计的实验方法，我们旨在量化在差分隐私训练背景下，增加模型大小、批量大小和迭代次数的益处。我们的工作需要做出一些简化假设，以克服可能尝试的指数级组合数量。我们假设模型的学习效果主要取决于“噪声-批量比”，即我们为隐私添加的随机噪声量与用于训练的数据组（批次）大小之比。这个假设之所以成立，是因为我们添加的隐私噪声远大于数据采样带来的任何自然随机性。

为了建立差分隐私扩展定律，我们进行了一系列全面的实验，以评估各种模型大小和噪声-批量比下的性能。由此产生的经验数据，加上其他变量之间已知的确定性关系，使我们能够回答各种有趣的扩展定律式问题，例如：“对于给定的计算预算、隐私预算和数据预算，实现尽可能低的训练损失的最佳训练配置是什么？”

我们的差分隐私扩展定律结构。我们确定，预测损失可以主要通过模型大小、迭代次数和噪声-批量比来准确建模，从而简化了计算、隐私和数据预算之间复杂的相互作用。

## 关键发现：强大的协同效应

在深入探讨完整的扩展定律之前，从隐私核算的角度理解计算预算、隐私预算和数据预算之间的动态关系和协同效应是很有用的——即理解这些因素如何影响固定模型大小和迭代次数下的噪声-批量比。这种分析的成本要低得多，因为它不需要任何模型训练，却能产生许多有用的见解。例如，单独增加隐私预算会导致收益递减，除非同时相应增加计算预算（浮点运算次数）或数据预算（Token 数量）。

增加隐私预算（epsilon）和计算预算（批量大小）对噪声-批量比影响的边际效益。

为了进一步探索这种协同效应，下面的可视化展示了最佳训练配置如何根据不同约束而变化。随着隐私和计算预算的变化，请注意建议如何在投资于更大的模型与使用更大的批量或更多迭代进行训练之间转换。

不同数据/隐私/计算预算设置下的预测训练损失，以及按迭代次数、批量大小和模型大小进行的进一步详细细分。图表显示了不同预算设置下可实现的最小损失，以及最佳超参数配置。

这些数据为从业者提供了大量有用的见解。虽然所有见解都在论文中有所报告，但一个关键发现是，与不使用差分隐私的情况相比，应该训练一个更小的模型，并使用更大的批量大小。对于差分隐私专家来说，鉴于大批量的重要性，这一普遍见解应该不足为奇。虽然这一普遍见解适用于许多设置，但最佳训练配置确实会随着隐私和数据预算而变化。理解确切的权衡对于确保在实际训练场景中明智地使用计算和隐私预算至关重要。上述可视化还表明，训练配置通常存在调整空间——即，如果与正确的迭代次数和/或批量大小配对，一系列模型大小可能会提供非常相似的效用。

## 应用扩展定律构建 VaultGemma

**Gemma** 模型的设计核心是责任与安全。这使其成为开发像 VaultGemma 这样的生产级、差分隐私训练模型的天然基础。

### 算法进展：大规模训练

我们上面推导出的扩展定律是朝着用差分隐私训练一个有用的 Gemma 模型迈出的重要第一步。我们使用扩展定律来确定两件事：一是我们需要多少计算资源来训练一个计算最优的、基于 Gemma 2 的 10 亿参数差分隐私模型；二是如何将该计算资源在批量大小、迭代次数和序列长度之间分配，以实现最佳效用。

扩展定律的基础研究与 VaultGemma 的实际训练之间存在的一个显著差距是我们对**泊松采样**的处理，这是 **DP-SGD** 的核心组成部分。我们最初使用了一种简单的方法，以均匀批次加载数据，但后来切换到泊松采样，以便以最少的噪声获得最佳的隐私保证。这种方法带来了两个主要挑战：它创建了不同大小的批次，并且需要特定的随机顺序来处理数据。我们通过使用我们最近关于**可扩展 DP-SGD** 的工作解决了这个问题，该工作允许我们处理固定大小的批次——通过添加额外的填充或进行修剪——同时仍然保持强大的隐私保护。

凭借我们新的扩展定律和先进的训练算法，我们构建了 VaultGemma，这是迄今为止最大的（10亿参数）完全使用差分隐私进行预训练的开源模型，其方法可以产生高实用性的模型。

从训练 VaultGemma 的过程中，我们发现我们的扩展定律非常准确。VaultGemma 的最终训练损失与我们方程预测的结果非常接近，这验证了我们的研究，并为社区未来的隐私模型开发提供了可靠的路线图。

VaultGemma 1B（差分隐私）与其非隐私对应模型（Gemma3 1B）以及一个较旧的基线模型（GPT-2 1.5B）的性能比较。结果量化了当前隐私保护所需的资源投入，并表明现代差分隐私训练产生的效用与大约五年前的非隐私模型相当。

我们还在一系列标准学术基准测试（即HellaSwag、BoolQ、PIQA、SocialIQA、TriviaQA、ARC-C、ARC-E）中，将我们模型的**下游性能**与其非隐私保护版本进行了比较。为了更全面地看待这一性能表现，并量化当前实现隐私保护所需的资源投入，我们还与一个较早的、规模相似的GPT-2模型进行了对比，后者在这些基准测试中表现相近。这一比较表明，当今的隐私保护训练方法所产生的模型，其效用大约相当于5年前的非隐私保护模型，这凸显了我们工作将帮助社区系统性地弥合的重要差距。

最后，该模型具备强大的理论和实证隐私保护能力。

### 形式化隐私保证

通常，在进行差分隐私（DP）训练时，隐私参数（ε, δ）和**隐私单元**都是重要的考量因素，因为它们共同决定了训练后的模型能学到什么。VaultGemma 的训练遵循**序列级差分隐私**保证（ε ≤ 2.0, δ ≤ 1.1e-10），其中一个序列由从异构数据源中提取的1024个连续Token组成。具体来说，我们使用了与训练Gemma 2模型相同的训练数据混合方案，其中包含许多长度不一的文档。在预处理过程中，长文档被分割并标记化为多个序列，而较短的文档则被打包成一个序列。虽然序列级隐私单元对我们的训练数据混合方案来说是一个自然的选择，但在数据与用户之间存在明确映射关系的情况下，**用户级差分隐私**会是更好的选择。

这在实践中意味着什么？通俗地说，由于我们在序列级别提供保护，如果任何（可能是私密的）事实或推断信息出现在单个序列中，那么VaultGemma基本上不知道这个事实：它对任何查询的响应，在统计意义上将与一个从未在该序列上训练过的模型的结果相似。然而，如果许多训练序列都包含与某个特定事实相关的信息，那么VaultGemma通常将能够提供该信息。

### 实证记忆测试

为了补充我们的序列级DP保证，我们对训练后模型的实证隐私属性进行了额外测试。为此，我们使用训练文档中的50个Token前缀作为**提示词**输入模型，观察它是否会生成相应的50个Token后缀。VaultGemma 1B模型未显示出任何可检测到的对其训练数据的记忆，成功证明了DP训练的有效性。

VaultGemma 标志着在构建既强大又天生具备隐私保护能力的人工智能道路上迈出了重要一步。通过开发并应用对差分隐私**扩展定律**的新的、更深入的理解，我们成功训练并发布了迄今为止最大的开源、经过DP训练的语言模型。

尽管经过DP训练的模型与未经DP训练的模型之间仍存在效用差距，但我们相信，通过对DP训练的机制设计进行更多研究，这一差距可以被系统性地缩小。我们希望VaultGemma 及我们伴随的研究能够赋能社区，为每个人构建下一代安全、负责任且保护隐私的人工智能。

我们要感谢整个Gemma和谷歌隐私团队在整个项目期间的贡献和支持，特别是Peter Kairouz、Brendan McMahan和Dan Ramage对博客文章的反馈，Mark Simborg和Kimberly Schwede在可视化方面的帮助，以及谷歌在算法设计、基础设施实现和生产维护方面提供帮助的团队。以下人员直接为本文介绍的工作做出了贡献（按字母顺序排列）：Borja Balle, Zachary Charles, Christopher A. Choquette-Choo, Lynn Chua, Prem Eruvbetine, Badih Ghazi, Steve He, Yangsibo Huang, Armand Joulin, George Kaissis, Pritish Kamath, Ravi Kumar, Daogao Liu, Ruibo Liu, Pasin Manurangsi, Thomas Mesnard, Andreas Terzis, Tris Warkentin, Da Yu, and Chiyuan Zhang。

- 生成式人工智能
- 开源模型与数据集
- 负责任的人工智能
- 安全、隐私与滥用防范

- 论文
- Hugging Face
- Kaggle
- 技术报告
- 分享复制链接×

- 复制链接×
