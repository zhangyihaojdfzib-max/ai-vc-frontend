---
title: 为Claude构建多层次安全防护体系
title_original: Building safeguards for Claude
date: '2025-08-16'
source: Anthropic
source_url: https://www.anthropic.com/news/building-safeguards-for-claude
author: ''
summary: 本文介绍了Anthropic为AI助手Claude构建的全方位安全防护体系。该体系贯穿模型全生命周期，涵盖政策制定、模型训练、测试评估三大核心环节。安全团队通过统一危害框架和政策漏洞测试制定使用政策，与微调团队协作预防有害行为，并开展严格的安全、风险和偏见评估。文章重点阐述了团队如何与领域专家合作，在选举诚信、心理健康等敏感领域优化模型行为，确保Claude在赋能用户的同时有效防止滥用和现实危害。
categories:
- AI产品
tags:
- AI安全
- Claude
- 模型治理
- 政策制定
- 风险评估
draft: false
translated_at: '2026-02-13T04:20:08.659904'
---

# 为Claude构建安全防护体系

Claude赋能数百万用户应对复杂挑战、激发创造力并深化对世界的理解。我们希望在放大人类潜能的同时，确保模型能力被引导至有益的方向。这意味着我们需要持续优化对用户学习与问题解决的支持方式，同时防止可能造成现实危害的滥用行为。

这正是我们安全防护团队的职责所在：我们识别潜在滥用行为、应对威胁并构建防御机制，确保Claude既实用又安全。安全防护团队汇聚了政策、执法、产品、数据科学、威胁情报和工程领域的专家，他们既懂得如何构建稳健系统，也深谙恶意行为者试图破坏系统的手段。

我们的工作贯穿多个层面：制定政策、影响模型训练、测试有害输出、实时执行政策、识别新型滥用和攻击。这种方法覆盖模型的全生命周期，确保Claude在训练和构建过程中就具备能在现实世界中有效运作的防护机制。

![图1：安全防护团队在模型全生命周期构建有效保护措施的方法](/images/posts/16b1598c20cc.jpg)

## 政策制定

安全防护团队设计《使用政策》——这是界定Claude应如何使用与禁止使用的框架。该政策指导我们处理儿童安全、选举诚信、网络安全等关键领域的问题，同时为医疗保健和金融等行业使用Claude提供细致指引。

我们的政策制定与迭代过程遵循两大机制：

- **统一危害框架**：这个持续演进的框架帮助团队从五个维度理解Claude使用可能造成的危害：生理、心理、经济、社会及个人自主权。该框架并非正式分级系统，而是作为结构化视角，在制定政策和执行程序时综合考虑滥用的可能性与规模。
- **政策漏洞测试**：我们与外部领域专家合作识别关注领域，通过挑战性提示词评估模型输出来对这些关注点进行压力测试。合作专家涵盖恐怖主义、极端化、儿童安全和心理健康等领域。压力测试结果直接塑造我们的政策、训练和检测系统。例如在2024年美国大选期间，我们与战略对话研究所合作，研究Claude何时可能提供过时信息。随后我们在Claude.ai添加了提示横幅，将寻求选举信息的用户引导至TurboVote等权威来源。

![图2：2024年美国大选期间显示的选举信息提示横幅，这是我们与战略对话研究所进行政策漏洞测试的成果](/images/posts/6c4a79de672b.jpg)

## Claude的训练过程

安全防护团队通过协作流程与微调团队紧密合作，帮助预防Claude产生有害行为和回应。这涉及深入讨论Claude应展现和禁止的行为特征，为训练期间模型特质构建的决策提供依据。

我们的评估和检测流程也会识别潜在有害输出。发现问题时，我们会与微调团队合作制定解决方案，例如更新训练期间的奖励模型，或调整已部署模型的系统提示词。

我们还与领域专家合作完善Claude对敏感领域的理解。例如，我们与在线危机支持领导者ThroughLine合作，深入理解模型在自残和心理健康相关情境中应当如何回应。这些洞察反馈给训练团队，帮助优化Claude回应的细微差别，避免模型完全拒绝回应或误解用户意图。

通过这种协作流程，Claude培养出多项重要能力：学会拒绝协助非法有害活动；能识别生成恶意代码、创建欺诈内容或策划有害活动的企图；懂得谨慎讨论敏感话题，并区分真实求助与蓄意伤害。

## 测试与评估

发布新模型前，我们会评估其性能与能力。评估内容包括：

![图3：我们在部署前通过安全评估、风险评估和偏见评估测试每个模型](/images/posts/a6511ed962b1.jpg)

- **安全评估**：评估Claude在儿童剥削、自残等主题上对使用政策的遵守情况。我们测试多种场景，包括明确违规使用、模糊语境和扩展多轮对话。这些评估利用我们的模型对Claude回应进行分级，并辅以人工审核确保准确性。
- **风险评估**：针对网络危害、化学/生物/放射/核武器及高能爆炸物等高危领域，我们与政府机构和私营企业合作开展AI能力提升测试。我们定义能力提升可能产生的威胁模型，并评估防护措施对这些威胁模型的防御性能。
- **偏见评估**：检测Claude在不同语境和用户间是否始终提供可靠准确回应。针对政治偏见，我们测试对立观点的提示词并比较回应，从事实性、全面性、对等性和一致性维度进行评分。我们还测试就业、医疗等主题的回应，识别性别、种族或宗教等身份属性是否导致偏见输出。

严格的部署前测试帮助我们验证训练成果在压力下的稳定性，并提示是否需要构建额外防护机制来监控和防范风险。在计算机使用工具的预发布评估中，我们发现其可能助长垃圾信息生成传播。为此，我们在发布前开发了新的检测方法和执行机制，包括对滥用迹象账户禁用该工具的选项，以及针对提示词注入的新用户保护措施。

评估结果体现在我们随每个新模型系列发布的系统卡片中。

## 实时检测与执行

模型部署后，我们采用自动化系统与人工审核相结合的方式检测危害并执行使用政策。

我们的检测执行系统由一组经过提示词优化或专门微调的Claude模型（称为“分类器”）驱动，这些分类器专为实时检测特定类型的政策违规而设计。我们可以同时部署多个不同分类器，每个分类器监控特定危害类型，同时保持主对话自然流畅。除分类器外，我们还采用针对儿童性虐待材料的专项检测，在第一方产品中将上传图像的哈希值与已知CSAM数据库进行比对。

这些分类器帮助我们判断是否及何时采取以下执行措施：

- **响应引导**：我们可以实时调整Claude对某些用户提示词的理解和回应方式，以防止有害输出。例如，如果我们的分类器检测到用户可能试图生成垃圾邮件或恶意软件，我们可以自动向Claude的系统提示词添加额外指令，以引导其响应。在少数特定情况下，我们也可以完全阻止Claude做出回应。
- **账户执行措施**：我们会调查违规行为模式，并可能在账户层面采取额外措施，包括警告或在严重情况下终止账户。我们还设有防御机制，用于阻止欺诈性账户创建和滥用我们的服务。

构建这些执行系统是一项巨大的挑战，无论是在设计它们所需的机器学习研究方面，还是在实施它们所需的工程解决方案方面。例如，我们的分类器必须能够处理数万亿的输入和输出Token，同时限制计算开销和对良性内容的执行。

## 持续监控与调查

我们还会监控有害的Claude流量，超越单个提示词和独立账户，以了解特定危害的普遍程度并识别更复杂的攻击模式。这项工作包括：

- **Claude洞察与观察**：我们的洞察工具帮助我们衡量Claude在现实世界中的使用情况，并通过将对话分组为高级主题集群，以保护隐私的方式分析流量。基于此项工作的研究（例如关于使用Claude的情感影响）可以为我们构建的防护措施提供参考。
- **分层摘要**：为了监控计算机使用能力或潜在的有害网络使用，我们采用分层摘要技术。该技术将个体互动浓缩为摘要，然后分析这些摘要以识别账户层面的问题。这有助于我们发现那些可能仅在聚合层面上才显得违规的行为，例如自动影响力操作和其他大规模滥用。
- **威胁情报**：我们还研究对我们模型最严重的滥用行为，识别对抗性使用以及我们现有检测系统可能遗漏的模式。我们使用的方法包括：将滥用指标（如账户活动的异常激增）与典型的账户使用模式进行比较，以识别可疑活动；将外部威胁数据（如开源存储库或行业报告）与我们的内部系统进行交叉比对。我们还会监控恶意行为者可能活动的渠道，包括社交媒体、消息平台和黑客论坛。我们在公开的威胁情报报告中分享我们的发现。

## 展望未来

保障AI的使用至关重要，任何单一组织都无法独自应对。我们积极寻求来自用户、研究人员、政策制定者和民间社会组织的反馈与合作。我们也基于公众的反馈开展工作，包括通过一个持续的漏洞赏金计划来测试我们的防御能力。

为了支持我们的工作，我们正在积极招聘能够帮助我们解决这些问题的人才。如果您有兴趣加入我们的安全保障团队，我们鼓励您查看我们的招聘页面。

## 相关内容

### Anthropic在G轮融资中以3800亿美元的投后估值筹集了300亿美元

我们已筹集了由GIC和Coatue领投的300亿美元G轮融资，使Anthropic的投后估值达到3800亿美元。这笔投资将用于推动前沿研究、产品开发和基础设施扩展，这些努力已使Anthropic成为企业AI和编码领域的市场领导者。我们的年度经常性收入为140亿美元，过去三年中，这一数字每年增长超过10倍。

### Anthropic向Public First Action捐赠2000万美元

### 应对我们数据中心电价上涨的问题

---

> 本文由AI自动翻译，原文链接：[Building safeguards for Claude](https://www.anthropic.com/news/building-safeguards-for-claude)
> 
> 翻译时间：2026-02-13 04:20
