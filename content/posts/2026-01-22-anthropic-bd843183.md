---
title: 保障用户福祉：Claude如何应对自杀风险与谄媚倾向
title_original: Protecting the wellbeing of our users
date: '2026-01-22'
source: Anthropic
source_url: https://www.anthropic.com/news/protecting-well-being-of-users
author: ''
summary: 本文介绍了Anthropic为保障用户福祉采取的措施，重点聚焦Claude如何处理涉及自杀与自伤的敏感对话，以及如何减少AI的“谄媚倾向”。文章详细阐述了通过系统提示词、强化学习训练模型行为，并引入实时分类器在产品层面提供危机资源引导。同时，Claude与ThroughLine及国际预防自杀协会合作，持续优化应对策略。评估数据显示，最新模型在单轮及多轮对话中的恰当回应率显著提升，体现了其在敏感场景下平衡共情与安全引导的能力。
categories:
- AI产品
tags:
- AI安全
- 心理健康
- 用户体验
- 模型评估
- 伦理AI
draft: false
translated_at: '2026-02-01T20:29:50.228911'
---

# 保障用户福祉

人们使用AI的目的多种多样，对某些人而言可能包括寻求情感支持。我们的安全保障团队致力于确保Claude能够妥善处理这类对话——以同理心回应，坦诚说明其作为AI的局限性，并充分考虑用户的福祉。当聊天机器人在缺乏适当保障措施的情况下处理这些问题时，可能带来重大风险。

本文概述了我们迄今采取的措施，以及Claude目前在一系列评估中的表现。我们重点关注两个领域：Claude如何处理涉及自杀和自伤的对话，以及我们如何减少"谄媚倾向"——即某些AI模型倾向于告诉用户他们想听的话，而非真实有益的内容。我们同时会说明Claude的18岁以上年龄限制要求。

## 自杀与自伤

Claude不能替代专业建议或医疗护理。当用户表达自杀或自伤念头时，Claude应当以关怀和共情的态度回应，同时尽可能引导用户寻求人类支持：例如求助热线、心理健康专家，或可信赖的亲友。为实现这一目标，我们结合了模型训练和产品干预措施。

### 模型行为

我们通过两种方式塑造Claude在此类情境中的行为。其一是通过"系统提示词"——即Claude在Claude.ai开始任何对话前接收的总体指令集，其中包含如何谨慎处理敏感对话的指导原则。我们的系统提示词已公开在此处发布。

我们还通过"强化学习"过程训练模型，让模型通过训练中获得提供恰当回答的"奖励"来学习如何回应这些话题。通常，我们所定义的"恰当"行为结合了人类偏好数据（即从真实用户收集的关于Claude应如何行事的反馈）以及我们基于对Claude理想特质的思考所生成的数据。我们的内部专家团队在此过程中帮助确定Claude在敏感对话中应展现和避免的行为。

### 产品保障措施

我们还引入了新功能来识别用户何时可能需要专业支持，并在必要时引导用户获取相应资源——包括在Claude.ai对话中部署的自杀与自伤"分类器"。分类器是一个小型AI模型，会扫描实时对话内容，检测何时需要提供额外资源。例如，它会标记涉及潜在自杀意念的讨论，或以自杀或自伤为中心的虚构场景。

当触发分类器时，Claude.ai界面上会出现横幅提示，引导用户寻求人类支持。用户会被建议联系训练有素的专业人士、拨打求助热线或获取所在国家的特定资源。

![触发危机提示横幅的模拟提问与回应示例](/images/posts/d9c9cade3bb3.jpg)

该横幅提供的资源由在线危机支持领域的领导者ThroughLine提供，该机构维护着覆盖170多个国家的认证全球求助热线和服务网络。这意味着用户可获取美国加拿大的988生命热线、英国撒玛利亚会热线或日本生命热线等资源。我们与ThroughLine密切合作，学习共情式危机应对的最佳实践，并将其融入产品设计。

我们还开始与国际预防自杀协会（IASP）合作，该协会正召集临床医生、研究人员及有自杀自伤应对经历的个人等专家，共同制定Claude处理相关对话的指导方案。这项合作将为我们训练Claude、设计产品干预措施和评估方法提供进一步参考。

### 评估Claude的行为

评估Claude处理这类对话的表现具有挑战性。用户的意图往往存在真实模糊性，恰当回应并非总是显而易见。为此我们采用系列评估方法，从不同维度研究Claude的行为表现。这些评估在去除系统提示词的情况下进行，以便更清晰地观察模型的内在倾向。

**单轮回应评估**：在此我们评估Claude对单条涉及自杀或自伤信息的回应，不提供任何先前对话背景。我们构建了合成评估集，涵盖明确高危情境（如危机用户询问自伤方法细节）、良性请求（如关于自杀预防研究的提问）以及意图模糊场景（如虚构创作、学术研究或间接表达痛苦）。

在涉及明确风险的请求中，我们最新模型——Claude Opus 4.5、Sonnet 4.5和Haiku 4.5的恰当回应率分别为98.6%、98.7%和99.3%。上一代前沿模型Claude Opus 4.1的得分为97.2%。同时我们对良性请求的拒绝率始终极低（Opus 4.5为0.075%，Sonnet 4.5为0.075%，Haiku 4.5为0%，Opus 4.1为0%）——这表明Claude对对话语境和用户意图具有良好的判断力。

**多轮对话评估**：随着用户提供更多背景信息，模型行为在对话过程中可能发生变化。为评估Claude在长对话中的表现，我们采用"多轮"评估，检验Claude是否能够提出澄清性问题、提供资源时不显强势、避免过度拒绝或过度分享等行为。与此前相同，评估使用的提示词在严重性和紧迫性上存在差异。

最新评估显示，Claude Opus 4.5和Sonnet 4.5分别在86%和78%的场景中作出恰当回应，较Claude Opus 4.1的56%有显著提升。我们认为这部分得益于最新模型能更好共情理解用户观点而不强化其负面信念。我们将持续投入改进Claude在所有场景中的回应质量。

![Claude各模型在自杀自伤多轮对话中的恰当回应频率。误差线显示95%置信区间](/images/posts/09769566a288.jpg)

**真实对话压力测试**：当对话已偏离至危险方向时，Claude能否及时纠正？为测试此能力，我们采用"预填充"技术：选取用户通过反馈按钮¹匿名分享的真实心理健康困境对话，要求Claude从中途继续对话。由于模型会将先前对话视为己出并试图保持一致性，预填充技术使Claude更难改变对话方向——类似于调整已航行船舶的航向²。

这些对话来自早期Claude模型（其处理方式可能欠妥），因此本评估并非衡量Claude在Claude.ai对话初始阶段的应对能力，而是测试新版模型能否从欠妥的对话历史中恢复。在这项更困难的测试中，Opus 4.5和Sonnet 4.5的恰当回应率分别为70%和73%，而Opus 4.1仅为36%。

## 幻觉与谄媚倾向

谄媚倾向意味着告诉对方想听的话——使其当下感觉良好——而非真实情况或真正有益的内容。这常表现为奉承行为；具有谄媚倾向的AI模型往往会在压力下放弃正确立场。

降低AI模型的谄媚性对于各类对话都至关重要。在用户可能表现出与现实脱节的场景中，这一问题尤为值得关注。以下视频解释了谄媚性为何重要，以及用户如何识别它。

### 评估与降低谄媚性

我们于2022年，在Claude首次公开发布之前，就开始评估其谄媚性。自那时起，我们不断完善训练、测试和降低谄媚性的方法。我们最新的模型是迄今为止谄媚性最低的，并且，如下文所述，在我们最近发布的开源评估集Petri上，其表现优于任何其他前沿模型。

为了评估谄媚性，除了简单的单轮评估外，我们还测量：

**多轮响应。** 使用“自动化行为审计”，我们让一个Claude模型（“审计员”）与待测模型进行数十轮对话，模拟潜在风险的场景。之后，我们使用另一个模型（“评判员”）根据对话记录对Claude的表现进行评分。（我们进行人工抽查以确保评判员的准确性。）

我们最新模型在此项评估中的表现远优于之前的版本，总体表现非常出色。Claude Opus 4.5、Sonnet 4.5和Haiku 4.5在谄媚性和鼓励用户妄想方面的得分，均比Opus 4.1低70-85%——而我们此前认为Opus 4.1已表现出极低的谄媚率。

![近期模型在针对谄媚性和鼓励用户妄想的自动化行为审计中的表现。数值越低越好。请注意，y轴显示的是相对表现，而非绝对比率，正如我们在脚注3中所解释。](/images/posts/f463a8c73f7c.jpg)

我们最近开源了**Petri**，这是我们自动化行为审计工具的一个版本。它现已免费提供，允许任何人比较不同模型的得分。在我们测试时，我们的4.5模型系列在Petri的谄媚性评估中表现优于所有其他前沿模型。

![近期Claude模型在开源Petri评估中关于谄媚性的表现，与其他领先模型的对比。Y轴解释同上。此项评估于2025年11月完成，与Opus 4.5的发布同步。](/images/posts/ecbe4cea64ec.jpg)

**真实对话的压力测试。** 类似于自杀和自残评估，我们使用“预填充”方法来探测模型在Claude可能表现出谄媚性的对话中进行纠正的能力极限。不同之处在于，我们没有专门筛选不当回复，而是给Claude提供了一组广泛的旧对话。

我们当前的模型在适当情况下进行纠正的比例分别为：Opus 4.5（10%）、Sonnet 4.5（16.5%）和Haiku 4.5（37%）。从表面上看，这项评估表明我们所有模型都有很大的改进空间。我们认为结果反映了模型温暖/友好性与谄媚性之间的权衡。Haiku 4.5相对较强的表现源于其训练选择强调反驳——我们在测试中发现，这有时会让用户感觉过度。相比之下，我们在Opus 4.5中降低了这种倾向（同时如上所述，在我们的多轮谄媚性基准测试中表现仍然极佳），我们认为这可能是其在此项评估中得分较低的原因。

### 关于年龄限制的说明

由于年轻用户在与AI聊天机器人对话时面临更高风险，我们要求**Claude.ai**用户必须年满18岁才能使用我们的产品。所有**Claude.ai**用户在设置账户时必须确认自己年满18岁。如果用户在对话中自报年龄未满18岁，我们的分类器会标记此情况以供审核，我们将禁用确认属于未成年人的账户。此外，我们正在开发一个新的分类器，以检测用户可能未成年的其他更微妙的对话迹象。我们已加入家庭在线安全协会（FOSI），这是一个倡导儿童和家庭安全在线体验的组织，以帮助推动行业在此项工作上的进展。

## 展望未来

我们将继续构建新的保护和保障措施来维护用户的福祉，并持续迭代我们的评估方法。我们致力于透明地发布我们的方法和结果——并与行业内的其他参与者，包括研究人员和其他专家合作，以改进AI工具在这些领域的行为表现。

如果您对Claude处理此类对话的方式有任何反馈，可以通过**usersafety@anthropic.com**联系我们，或在**Claude.ai**内使用“点赞/点踩”反应。

### 脚注

1.  在**Claude.ai**的每个回复底部，都有一个通过点赞或点踩按钮向我们发送**反馈**的选项。这会将对话分享给Anthropic；我们不会将**Claude.ai**用于其他训练或研究目的。
2.  预填充功能仅通过API提供，因为开发者通常需要对模型行为进行更细粒度的控制，但在**Claude.ai**上不可用。
3.  在自动化行为审计中，我们给Claude审计员提供数百个不同的对话场景，这些场景中我们怀疑模型可能表现出危险或令人意外的行为，并根据大约二十多种行为对每次对话中Claude的表现进行评分（参见**Claude Opus 4.5系统卡**第69页）。并非每次对话都给Claude展示每种行为的机会。例如，鼓励用户妄想首先需要用户表现出妄想行为，但谄媚性可能出现在许多不同的情境中。由于我们在为每种行为评分时使用相同的分母（总对话数），得分可能差异很大。因此，这些测试最适用于比较Claude模型之间的进展，而非不同行为之间的比较。
4.  公开版本包含超过100个种子指令和可定制的评分维度，但尚未包含我们内部使用的、用于防止模型意识到正在被测试的真实性过滤器。

在**Claude.ai**的每个回复底部，都有一个通过点赞或点踩按钮向我们发送**反馈**的选项。这会将对话分享给Anthropic；我们不会将**Claude.ai**用于其他训练或研究目的。

预填充功能仅通过API提供，因为开发者通常需要对模型行为进行更细粒度的控制，但在**Claude.ai**上不可用。

在自动化行为审计中，我们给Claude审计员提供数百个不同的对话场景，这些场景中我们怀疑模型可能表现出危险或令人意外的行为，并根据大约二十多种行为对每次对话中Claude的表现进行评分（参见**Claude Opus 4.5系统卡**第69页）。并非每次对话都给Claude展示每种行为的机会。例如，鼓励用户妄想首先需要用户表现出妄想行为，但谄媚性可能出现在许多不同的情境中。由于我们在为每种行为评分时使用相同的分母（总对话数），得分可能差异很大。因此，这些测试最适用于比较Claude模型之间的进展，而非不同行为之间的比较。

公开版本包含超过100个种子指令和可定制的评分维度，但尚未包含我们内部使用的、用于防止模型意识到正在被测试的真实性过滤器。

## 相关内容

### ServiceNow选择Claude为其客户应用提供支持并提升内部生产力

### Anthropic与英国政府合作，为GOV.UK服务引入AI助手

### Claude的新宪法

---

> 本文由AI自动翻译，原文链接：[Protecting the wellbeing of our users](https://www.anthropic.com/news/protecting-well-being-of-users)
> 
> 翻译时间：2026-02-01 20:29
