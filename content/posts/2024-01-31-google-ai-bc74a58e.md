---
title: MobileDiffusion：移动端亚秒级文生图技术
title_original: 'MobileDiffusion: Rapid text-to-image generation on-device'
date: '2024-01-31'
source: Google AI Blog
source_url: http://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html
author: null
summary: 本文介绍了谷歌Core ML团队提出的MobileDiffusion，一种专为移动设备设计的高效潜在扩散模型。该模型通过优化UNet架构（如在瓶颈处集中Transformer块、使用分离卷积）并结合DiffusionGAN实现一步采样，将参数压缩至5.2亿，可在高端移动设备上半秒内生成512x512图像。研究重点解决了扩散模型在移动端部署时面临的迭代采样和架构复杂两大效率挑战，为设备端快速文生图提供了可行方案。
categories:
- AI研究
tags:
- 文生图
- 移动端AI
- 扩散模型
- 模型优化
- 设备端推理
draft: false
translated_at: '2026-01-06T14:49:02.031Z'
---

MobileDiffusion：在设备端实现快速文生图
2024年1月31日
作者：Core ML 团队高级软件工程师赵阳，高级主任软件工程师侯廷波
快速链接
文生图扩散模型在根据文本提示词生成高质量图像方面展现出卓越能力。然而，领先的模型通常拥有数十亿参数，因此运行成本高昂，需要强大的台式机或服务器（例如 Stable Diffusion、DALL·E 和 Imagen）。尽管过去一年通过 MediaPipe 在 Android 和通过 Core ML 在 iOS 上的推理解决方案取得了进展，但在移动设备上实现快速（亚秒级）文生图生成仍然遥不可及。

为此，在论文《MobileDiffusion：移动设备上的亚秒级文生图生成》中，我们介绍了一种有望在设备端实现快速文生图生成的新方法。MobileDiffusion 是一种专为移动设备设计的高效潜在扩散模型。我们还采用了 DiffusionGAN 来实现推理过程中的一步采样，该方法在微调预训练扩散模型的同时，利用 GAN 对去噪步骤进行建模。我们已在 iOS 和 Android 高端设备上测试了 MobileDiffusion，它可以在半秒内生成一张 512x512 的高质量图像。其相对较小的模型尺寸（仅 5.2 亿参数）使其特别适合移动端部署。

| 在设备端实现快速文生图生成。 |
背景
文生图扩散模型的相对低效源于两个主要挑战。首先，扩散模型的固有设计需要迭代去噪来生成图像，这需要对模型进行多次评估。其次，文生图扩散模型中网络架构的复杂性涉及大量参数，通常达到数十亿，导致计算评估成本高昂。因此，尽管在移动设备上部署生成模型具有潜在优势，例如增强用户体验和解决新出现的隐私问题，但在当前文献中，这方面的探索仍然相对较少。

优化文生图扩散模型的推理效率一直是一个活跃的研究领域。先前的研究主要集中在解决第一个挑战，即寻求减少函数评估次数。通过利用先进的数值求解器（例如 DPM）或蒸馏技术（例如渐进式蒸馏、一致性蒸馏），必要的采样步骤已从数百步显著减少到个位数。最近的一些技术，如 DiffusionGAN 和对抗性扩散蒸馏，甚至减少到仅需一步。

然而，在移动设备上，由于模型架构的复杂性，即使是少量的评估步骤也可能很慢。迄今为止，文生图扩散模型的架构效率受到的关注相对较少。少数早期工作简要提及了这个问题，涉及移除冗余的神经网络块（例如 SnapFusion）。然而，这些努力缺乏对模型架构内每个组件的全面分析，因此未能为设计高效架构提供全面的指导。

MobileDiffusion
要有效克服移动设备计算能力有限带来的挑战，需要对模型的架构效率进行深入而全面的探索。为了实现这一目标，我们的研究对 Stable Diffusion 的 UNet 架构中的每个组成部分和计算操作进行了详细检查。我们提出了一份全面的指南，用于构建高效的文生图扩散模型，最终成果便是 MobileDiffusion。

MobileDiffusion 的设计遵循潜在扩散模型。它包含三个组件：文本编码器、扩散 UNet 和图像解码器。对于文本编码器，我们使用 CLIP-ViT/L14，这是一个适合移动端的小型模型（1.25 亿参数）。然后我们将重点转向扩散 UNet 和图像解码器。

扩散 UNet
如下图所示，扩散 UNet 通常交替使用 Transformer 块和卷积块。我们对这两种基本构建块进行了全面研究。在整个研究中，我们控制了训练流程（例如数据、优化器），以研究不同架构的效果。

在经典的文生图扩散模型中，一个 Transformer 块包含一个用于建模视觉特征间长程依赖关系的自注意力层、一个用于捕捉文本条件与视觉特征间交互的交叉注意力层，以及一个用于对注意力层输出进行后处理的前馈层。这些 Transformer 块在文生图扩散模型中起着关键作用，是负责文本理解的主要组件。然而，它们也带来了显著的效率挑战，因为注意力操作的计算成本很高，与序列长度呈二次方关系。我们遵循 UViT 架构的思想，在 UNet 的瓶颈处放置更多的 Transformer 块。这一设计选择的动机是，由于瓶颈处的维度较低，注意力计算对资源的消耗也较少。

| 我们的 UNet 架构在中间部分加入了更多 Transformer，并在高分辨率层级跳过了自注意力层。 |
卷积块，特别是 ResNet 块，部署在 UNet 的每个层级。虽然这些块有助于特征提取和信息流，但相关的计算成本，尤其是在高分辨率层级，可能相当可观。在这方面，一个经过验证的方法是分离卷积。我们观察到，在 UNet 的较深部分用轻量级分离卷积层替换常规卷积层，可以获得相似的性能。

在下图中，我们比较了几种扩散模型的 UNet。我们的 MobileDiffusion 在 FLOPs（浮点运算次数）和参数数量方面表现出卓越的效率。

| 几种扩散 UNet 的比较。 |
图像解码器
除了 UNet，我们还优化了图像解码器。我们训练了一个变分自编码器，将 RGB 图像编码为一个 8 通道的潜在变量，其空间尺寸比原始图像小 8 倍。潜在变量可以被解码为图像，尺寸放大 8 倍。为了进一步提高效率，我们通过剪裁原始解码器的宽度和深度，设计了一个轻量级解码器架构。由此产生的轻量级解码器带来了显著的性能提升，延迟降低了近 50%，且质量更好。更多细节，请参阅我们的论文。

| VAE 重建。我们的 VAE 解码器比 SD（Stable Diffusion）具有更好的视觉质量。 |
| 解码器 | 参数量（百万） | PSNR↑ | SSIM↑ | LPIPS↓ |
| SD | 49.5 | 26.7 | 0.76 | 0.037 |
| 我们的 | 39.3 | 30.0 | 0.83 | 0.032 |
| 我们的轻量版 | 9.8 | 30.2 | 0.84 | 0.032 |
| VAE 解码器的质量评估。我们的轻量解码器比 SD 小得多，且具有更好的质量指标，包括峰值信噪比、结构相似性指数和感知图像块相似度。 |
一步采样
除了优化模型架构，我们还采用了 DiffusionGAN 混合方法来实现一步采样。为文生图训练 DiffusionGAN 混合模型会遇到一些复杂性。值得注意的是，判别器（一个区分真实数据和生成数据的分类器）必须同时基于纹理和语义进行判断。此外，训练文生图模型的成本可能极高，尤其是在基于 GAN 的模型中，判别器会引入额外的参数。

纯基于GAN的文本到图像模型（例如StyleGAN-T、GigaGAN）面临着类似的复杂性，导致训练过程极其复杂且成本高昂。
为了克服这些挑战，我们使用预训练的扩散UNet来初始化生成器和判别器。这种设计能够实现与预训练扩散模型的无缝初始化。我们假设扩散模型内部的特征包含了文本与视觉数据之间复杂相互作用的丰富信息。这种初始化策略显著简化了训练过程。
下图展示了训练流程。初始化后，一张带噪声的图像被送入生成器进行一步扩散。其结果通过重建损失与真实值进行评估，类似于扩散模型的训练。随后，我们对输出添加噪声并将其送入判别器，其结果通过GAN损失进行评估，这有效地采用了GAN来建模一个去噪步骤。通过使用预训练权重初始化生成器和判别器，训练过程变成了一个微调过程，在不到1万次迭代内即可收敛。
| DiffusionGAN微调示意图。|

**结果**
下方展示了我们使用MobileDiffusion配合DiffusionGAN一步采样生成的示例图像。凭借如此紧凑的模型（总计5.2亿参数），MobileDiffusion能够为各种领域生成高质量且多样化的图像。
| 由我们的MobileDiffusion生成的图像。|

我们使用不同的运行时优化器，在iOS和Android设备上测量了MobileDiffusion的性能。延迟数据如下所示。我们看到MobileDiffusion非常高效，能够在半秒内生成一张512x512的图像。这种闪电般的速度有望在移动设备上实现许多有趣的应用场景。
| 移动设备上的延迟测量（秒）。|

**结论**
凭借在延迟和模型大小方面的卓越效率，MobileDiffusion有潜力成为移动部署的一个非常友好的选择，因为它能够在用户输入文本提示词时提供快速的图像生成体验。我们将确保该技术的任何应用都符合谷歌负责任的人工智能实践。

**致谢**
我们要感谢帮助将MobileDiffusion带到设备端的合作者和贡献者：Zhisheng Xiao, Yanwu Xu, Jiuqiang Tang, Haolin Jia, Lutz Justen, Daniel Fenner, Ronald Wotzlaw, Jianing Wei, Raman Sarokin, Juhyun Lee, Andrei Kulik, Chuo-Ling Chang, 以及Matthias Grundmann。

---

> 本文由AI自动翻译，原文链接：[MobileDiffusion: Rapid text-to-image generation on-device](http://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html)
> 
> 翻译时间：2026-01-06 04:29
