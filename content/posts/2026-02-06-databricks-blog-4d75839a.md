---
title: 如何构建生产就绪的Genie空间并建立用户信任
title_original: How to Build Production-Ready Genie Spaces, and Build Trust Along
  the Way
date: '2026-02-06'
source: Databricks Blog
source_url: https://www.databricks.com/blog/how-build-production-ready-genie-spaces-and-build-trust-along-way
author: ''
summary: 本文探讨了在Databricks Genie等自然语言分析工具中建立用户信任的挑战。文章指出，信任的建立不能仅依赖主观评估，而需要通过系统化的基准测试进行可衡量的验证。作者通过一个营销活动分析的端到端示例，详细介绍了从定义基准测试套件、建立基线准确度、系统化优化到最终衡量与沟通的完整流程。该方法旨在将Genie空间从基线开发转变为可供生产使用的可靠系统，确保用户能够依赖其输出做出关键业务决策。
categories:
- AI产品
tags:
- Databricks Genie
- 自然语言分析
- 基准测试
- 数据信任
- 自助分析
draft: false
translated_at: '2026-02-07T04:11:08.276102'
---

## 自助分析中的信任挑战

Genie 是 Databricks 的一项功能，允许业务团队使用自然语言与其数据进行交互。它使用根据您组织的术语和数据定制的生成式 AI，并能够通过用户反馈监控和完善其性能。

任何自然语言分析工具面临的一个常见挑战是与最终用户建立信任。以营销领域专家 Sarah 为例，她第一次尝试使用 Genie 来代替她的仪表板。

Sarah："我们上个季度的点击率是多少？"

Genie：8.5%

Sarah 的想法：等等，我记得上个季度我们达到 6% 时还庆祝过……

这是一个 Sarah 知道答案但看不到正确结果的问题。也许生成的查询包含了不同的营销活动，或者在应该使用公司财年日历时，使用了标准的日历定义来指代"上个季度"。但 Sarah 不知道问题出在哪里。这个不确定的时刻引发了怀疑。如果没有对答案进行适当的评估，这种对可用性的怀疑可能会增长。用户会重新请求分析师支持，这会干扰其他项目，并增加生成单一洞察的成本和价值实现时间。自助服务的投资未能得到充分利用。

问题不仅仅在于您的 Genie 空间能否生成 SQL。更在于您的用户是否足够信任这些结果，并据此做出决策。

建立这种信任需要超越主观评估（"似乎可行"），转向可衡量的验证（"我们已经系统地测试过"）。我们将演示 Genie 内置的基准测试功能如何将基线实现转变为可供生产使用的系统，让用户能够依赖它做出关键决策。基准测试提供了一种数据驱动的方式来评估 Genie 空间的质量，并帮助您在策划 Genie 空间时如何解决差距。

在本博客中，我们将通过一个端到端的示例，引导您完成构建一个带有基准测试的 Genie 空间，以开发一个值得信赖的系统。

## 数据：营销活动分析

我们的营销团队需要分析四个相互关联的数据集上的营销活动表现。

- Prospects - 公司信息，包括行业和地点
- Contacts - 收件人信息，包括部门和设备类型
- Campaigns - 营销活动详情，包括预算、模板和日期
- Events - 电子邮件事件跟踪（发送、打开、点击、垃圾邮件报告）

工作流程：识别目标公司（潜在客户）→ 找到这些公司的联系人 → 发送营销活动 → 跟踪收件人如何响应这些活动（事件）。

用户需要回答的一些示例问题包括：

- "按行业划分，哪些营销活动带来了最佳投资回报率？"
- "我们不同营销活动类型的合规风险如何？"
- "参与度模式（点击率）如何因设备和部门而异？"
- "哪些模板针对特定的潜在客户细分表现最佳？"

这些问题需要连接表、计算特定领域的指标，并应用关于什么使营销活动"成功"或"高风险"的领域知识。正确获取这些答案很重要，因为它们直接影响预算分配、营销活动策略和合规决策。让我们开始吧！

## 历程：从基线开发到生产

不应期望随意添加一些表和少量文本提示词就能为最终用户生成足够准确的 Genie 空间。对最终用户需求的透彻理解，结合对数据集和 Databricks 平台功能的了解，才能带来期望的结果。

在这个端到端的示例中，我们通过基准测试评估 Genie 空间的准确性，诊断导致错误答案的上下文差距，并实施修复。请考虑将此框架用于指导您的 Genie 开发和评估。

- **定义您的基准测试套件**（目标是 10-20 个具有代表性的问题）。这些问题应由主题专家和预期使用 Genie 进行分析的实际最终用户确定。理想情况下，这些问题应在 Genie 空间实际开发之前创建。
- **建立您的基线准确度**。在您的空间中运行所有基准问题，此时 Genie 空间仅添加了基线数据对象。记录准确度、哪些问题通过、哪些失败以及原因。
- **系统化优化**。实施一组更改（例如，添加列描述）。重新运行所有基准问题。衡量影响和改进，并遵循已发布的**最佳实践**继续进行迭代开发。
- **衡量与沟通**。运行基准测试提供了客观的评估标准，证明 Genie 空间充分满足期望，从而与用户和利益相关者建立信任。

我们创建了一套包含 13 个基准问题的套件，代表了最终用户希望从我们的营销数据中寻求答案的问题。每个基准问题都是一个用简单英语表述的现实问题，并附带一个经过验证的、能回答该问题的 SQL 查询。

根据设计，Genie **不**将这些基准 SQL 查询作为现有上下文包含在内。它们纯粹用于评估。我们的工作是提供正确的上下文，以便这些问题能够得到正确回答。让我们开始吧！

### 迭代 0：建立基线

我们故意从糟糕的表名（如 `cmp` 和 `proc_delta`）以及列名（如 `uid_seq` 对应 campaign_id，`label_txt` 对应 campaign_name，`num_val` 对应 cost，`proc_ts` 对应 event_date）开始。这个起点反映了许多组织实际面临的情况——数据建模遵循技术惯例而非业务含义。

仅凭表本身也无法为如何计算特定领域的关键绩效指标和度量标准提供上下文。Genie 知道如何利用数百个内置的 SQL 函数，但它仍然需要正确的列和逻辑作为输入。那么，当 Genie 没有足够的上下文时会发生什么？

**基准分析**：Genie 无法正确回答我们 13 个基准问题中的任何一个。不是因为 AI 不够强大，而是因为它缺乏任何相关的上下文，如下所示。

**洞察**：最终用户提出的每个问题都依赖于 Genie 根据您提供的数据对象生成 SQL 查询。因此，糟糕的数据命名约定将影响生成的每一个查询。您不能跳过基础数据质量而期望与最终用户建立信任！Genie 不会为每个问题都生成 SQL 查询。它只在有足够上下文时才这样做。这是防止产生**幻觉**和误导性答案的预期行为。

**下一步行动**：较低的初始基准分数表明您应首先专注于清理 Unity Catalog 对象，因此我们从这里开始。

### 迭代 1：模糊的列含义

我们将表名改进为 `campaigns`、`events`、`contacts` 和 `prospects`，并在 Unity Catalog 中添加了清晰的表描述。

然而，我们遇到了另一个相关的挑战：具有误导性的列名或注释，暗示了不存在的关联关系。

例如，像 `workflow_id`、`resource_id` 和 `owner_id` 这样的列存在于多个表中。这些听起来应该将表连接在一起，但实际上并非如此。`events` 表使用 `workflow_id` 作为指向 campaigns 表的外键（而不是指向一个单独的 workflow 表），并使用 `resource_id` 作为指向 contacts 表的外键（而不是指向一个单独的 resource 表）。同时，`campaigns` 表有自己的 `workflow_id` 列，但两者完全无关。如果这些列名和描述没有适当标注，可能会导致对这些属性的不准确使用。我们在 Unity Catalog 中更新了列描述，以阐明每个模糊列的目的。注意：如果您无法在 UC 中编辑元数据，可以在 Genie 空间的知识库中添加表和列描述。

基准分析：得益于清晰的命名和描述，简单的单表查询开始正常工作。"统计2023年按类型分类的事件数量"和"过去三个月启动了哪些营销活动？"这类问题现在能得到正确答案。然而，任何需要跨表关联的查询都失败了——Genie仍然无法正确判断哪些列代表了表间关系。

洞察：清晰的命名规范有帮助，但如果没有明确定义关系，Genie必须猜测哪些列将表连接在一起。当多个列拥有像`workflow_id`或`resource_id`这类名称时，这些猜测可能导致不准确的结果。恰当的元数据是基础，但关系应当被明确定义。

后续行动：定义数据对象之间的关联关系。像`id`或`resource_id`这样的列名随处可见。让我们明确指定这些列中哪些是引用其他表对象的。

### 迭代2：模糊的数据模型

阐明Genie在关联表时应使用哪些列的最佳方法是通过主键和外键。我们在Unity Catalog中添加了主键和外键约束，明确告知Genie表如何连接：`campaigns.campaign_id`关联到`events.campaign_id`，后者链接到`contacts.contact_id`，再连接到`prospects.prospect_id`。这消除了猜测，并规定了默认情况下如何创建多表关联。注意：如果您无法在UC中编辑关系，或者表关系复杂（例如，多个JOIN条件），您可以在Genie空间知识库中定义这些关系。

或者，我们可以考虑创建一个度量视图，该视图可以在对象定义中明确包含关联细节。稍后将详述。

基准分析：稳步进展。需要跨多表关联的问题开始正常工作："显示2024年第一季度按行业划分的营销活动成本"和"哪些营销活动在一月份拥有超过1,000个事件？"现在能成功回答。

洞察：关系使得能够实现带来真正商业价值的复杂多表查询。Genie正在生成结构正确的SQL，并能正确执行成本汇总和事件计数等简单操作。

行动：在剩余不正确的基准测试中，许多包含了用户打算用作数据筛选条件的值引用。最终用户提问的方式与数据集中出现的值并不直接匹配。

### 迭代3：理解数据值

应精心设计Genie空间以回答特定领域的问题。然而，人们使用的术语并不总是与数据呈现的方式完全一致。用户可能说"生物工程公司"，但数据值是"生物技术"。

启用值字典和数据采样可以更快、更准确地查找数据中实际存在的值，而不是让Genie仅使用最终用户提示中的确切值。

示例值和值字典现在默认开启，但值得再次检查，确保用于筛选的常用列已启用，并在需要时拥有自定义值字典。

基准分析：超过50%的基准问题现在能得到成功回答。涉及特定类别值（如"生物技术"）的问题开始正确识别这些筛选条件。当前的挑战在于实现自定义度量和聚合。例如，Genie基于找到"点击"这一数据值及其对基于比率的度量的理解，提供了关于如何计算CTR的**最佳猜测**。但它还不够自信直接生成查询：

这是一个我们希望100%正确计算的度量，因此我们需要向Genie澄清这个细节。

洞察：值采样通过提供对真实数据值的访问，改善了Genie的SQL生成能力。当用户提出带有拼写错误或不同术语的对话式问题时，值采样帮助Genie将提示词与表中实际的数据值进行匹配。

后续行动：现在最常见的问题是Genie仍然没有为我们的自定义度量生成正确的SQL。让我们明确处理度量定义，以获得更准确的结果。

### 迭代4：定义自定义度量

此时，Genie已经了解了数据中存在的分类数据属性的上下文，可以筛选到我们的数据值，并能使用标准SQL函数执行直接的聚合（例如，"按类型统计事件数量"使用`COUNT()`）。为了更清晰地说明Genie应如何计算我们的度量，我们向genie空间添加了示例SQL查询。此示例展示了CTR的正确度量定义：

注意，建议在SQL查询中添加注释，因为这与代码一起是相关的上下文。

基准分析：这带来了迄今为止最大的单次准确性提升。考虑到我们的目标是使Genie能够为特定受众回答非常详细的问题。预计大多数最终用户问题将依赖于自定义度量，如CTR、垃圾邮件率、参与度指标等。更重要的是，这些问题的变体也能正常工作。Genie学习了我们度量的定义，并将应用于未来的任何查询。

洞察：示例查询传授了仅靠元数据无法传达的业务逻辑。一个精心设计的示例查询通常能同时解决一整类基准测试差距。这比迄今为止任何其他单个迭代步骤都带来了更多价值。

后续行动：只剩下少数基准问题仍然不正确。经过进一步检查，我们注意到剩余的基准测试失败有两个原因：

1.  用户询问的数据属性在数据中并不**直接**存在。例如，"上个季度有多少营销活动产生了**高**CTR？"Genie不知道用户所说的"高"CTR是什么意思，因为没有相应的数据属性存在。
2.  这些数据表包含我们应该排除的记录。例如，我们有许多不面向客户的测试营销活动。我们需要将这些从我们的KPI中排除。

### 迭代5：记录领域特定规则

这些剩余的差距是适用于所有查询创建方式的全局性上下文片段，并且与数据中不直接存在的值相关。

让我们以第一个关于高CTR的例子，或类似的高成本营销活动为例。向表中添加领域特定数据并不总是容易的，甚至不建议这样做，原因如下：

1.  对数据表进行更改，例如添加`campaign_cost_segmentation`字段（高、中、低），需要时间并会影响其他流程，因为表结构和数据管道都需要修改。
2.  对于像CTR这样的聚合计算，随着新数据流入，CTR值会发生变化。无论如何我们不应该预计算这个结果，我们希望这个计算能在我们明确时间段和营销活动等筛选条件时动态完成。

因此，我们可以在Genie中使用基于文本的指令来为我们执行这种领域特定的细分。

同样，我们可以指定Genie应如何编写查询以符合业务预期。这可以包括自定义日历、强制性的全局筛选条件等。例如，这些营销活动数据包含应从我们的KPI计算中排除的测试活动。

基准分析：基准测试准确率达到100%！边缘情况和基于阈值的问题开始持续正常工作。关于"高绩效营销活动"或"合规风险营销活动"的问题现在正确应用了我们的业务定义。

洞察：基于文本的指令是一种简单有效的方法，可以填补先前步骤留下的任何剩余差距，确保为最终用户生成正确的查询。不过，它不应该是您依赖上下文注入的第一个地方或唯一地方。

请注意，在某些情况下可能无法达到100%的准确性。例如，有时基准问题需要非常复杂的查询或多个提示词才能生成正确答案。如果无法轻松创建单个示例SQL查询，只需在与其他人员分享基准评估结果时注明此差距即可。通常的期望是，在进入用户验收测试（UAT）之前，Genie的基准准确率应达到80%以上。

**下一步行动：** 既然Genie在我们的基准问题上已达到预期的准确率水平，我们将进入UAT阶段，并收集更多最终用户的反馈！

### （可选）迭代6：预计算复杂指标

在最后一次迭代中，我们创建了一个自定义视图，用于预定义关键营销指标并应用业务分类。在数据集全部适用于单一数据模型且拥有数十个自定义指标的情况下，创建视图或指标视图可能更为简便。将所有指标整合到数据对象定义中，比为Genie空间中的每个指标编写示例SQL查询要容易得多。

**基准测试结果：** 通过使用视图而非仅基表，我们仍实现了100%的基准测试准确率，因为元数据内容保持不变。

**洞察：** 与其通过示例或指令解释复杂计算，不如将它们封装在视图或指标视图中，从而定义单一事实来源。

## 我们的收获：基准驱动开发的影响

配置Genie空间没有能解决所有问题的"银弹"。要获得生产就绪的准确率，通常需要具备高质量数据、适当丰富的元数据、已定义的指标逻辑，以及注入到空间中的特定领域上下文。在我们的端到端示例中，我们遇到了跨越所有这些领域的常见问题。

基准测试对于评估您的空间是否达到预期并准备好接收用户反馈至关重要。它还指导我们的开发工作，以弥补Genie在问题理解上的差距。回顾如下：

- **迭代1-3 - 基准准确率54%。** 这些迭代侧重于让Genie更清晰地了解我们的数据和元数据。实施适当的表名、表描述、列描述、连接键以及启用示例值，都是任何Genie空间的**基础性**步骤。具备这些能力后，Genie应能正确识别正确的表、列和连接条件，这些都会影响其生成的任何查询。它还能进行简单的聚合和过滤。仅凭这些基础知识，Genie就能正确回答超过一半的我们特定领域的基准问题。
- **迭代4 - 基准准确率77%。** 本次迭代侧重于澄清我们的自定义指标定义。例如，CTR并非每个基准问题都涉及，但它是一个非标准指标（即非 sum()、avg() 等）的示例，需要每次都正确回答。
- **迭代5 - 基准准确率100%。** 本次迭代展示了如何使用基于文本的指令来填补剩余的不准确之处。这些指令涵盖了常见场景，例如为分析用途包含数据全局过滤器、特定领域定义（例如，什么构成**高参与度活动**），以及指定的财年日历信息。

通过遵循评估Genie空间的系统化方法，我们**主动**发现了非预期的查询行为，而不是**被动地**从Sarah那里听说。我们将主观评估（"似乎可行"）转变为客观衡量（"我们已经验证它适用于13个代表性场景，涵盖了最终用户最初定义的关键用例"）。

## 前进之路

建立对自助分析服务的信任并非一蹴而就。它关乎通过可衡量的验证进行系统性改进，关乎在用户发现问题之前发现问题。

基准测试功能提供了实现这一目标的衡量层。它将Databricks文档推荐的迭代方法转变为可量化、建立信心的过程。让我们回顾一下这个基准驱动的系统化开发流程：

1.  创建代表用户真实问题的**基准问题**（目标10-15个）。
2.  测试您的空间以建立基准准确率。
3.  按照Databricks在我们的**最佳实践**中推荐的迭代方法进行配置改进。
4.  每次更改后重新测试所有基准问题，以衡量影响并从不正确的问题中识别上下文差距。记录您的准确率进展，以建立利益相关者的信心。

从坚实的Unity Catalog基础开始。添加业务上下文。通过基准测试进行全面测试。衡量每一次变更。通过经过验证的准确率建立信任。

您和您的最终用户都将受益！

## 下一步是什么？

---

> 本文由AI自动翻译，原文链接：[How to Build Production-Ready Genie Spaces, and Build Trust Along the Way](https://www.databricks.com/blog/how-build-production-ready-genie-spaces-and-build-trust-along-way)
> 
> 翻译时间：2026-02-07 04:11
