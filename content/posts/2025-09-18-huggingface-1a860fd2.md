---
title: RiskRubric.ai：以标准化风险评估推动AI安全民主化
title_original: Democratizing AI Safety with RiskRubric.ai
date: '2025-09-18'
source: Hugging Face Blog
source_url: https://huggingface.co/blog/riskrubric
author: ''
summary: 文章介绍了由云安全联盟等机构发起的RiskRubric.ai倡议，旨在为AI模型生态系统提供标准化、透明的风险评估。该平台从透明度、可靠性、安全性、隐私等六个维度对模型进行自动化评估，生成可比较的风险评分和等级。初步评估结果显示，开放模型在透明度等方面表现优异，但整体风险分布呈现两极分化。文章强调，公开、标准化的风险评估有助于社区协作改进模型安全，形成透明改进的良性循环。
categories:
- AI产品
tags:
- AI安全
- 风险评估
- 开源模型
- 模型治理
- 标准化
draft: false
translated_at: '2026-02-06T04:15:24.701891'
---

# 通过 RiskRubric.ai 实现 AI 安全民主化

通过标准化风险评估，在开放模型生态系统中建立信任

在 Hugging Face 平台上可以找到超过 50 万个模型，但用户并不总是清楚如何为自己选择最佳模型，尤其是在安全方面。开发者可能找到一个完美契合其用例的模型，却没有系统性的方法来评估其安全状况、隐私影响或潜在的故障模式。

随着模型变得更强大、采用速度加快，我们在 AI 安全和安全报告方面也需要同样快速的进展。因此，我们很高兴地宣布 **RiskRubric.ai**，这是一项由云安全联盟和 Noma Security 牵头、Haize Labs 和 Harmonic Security 共同贡献的新举措，旨在为 AI 模型生态系统提供标准化和透明的风险评估。

## Risk Rubric：一种新的模型风险标准化评估方法

RiskRubric.ai 通过从六个维度评估模型：透明度、可靠性、安全性、隐私、安全性和声誉，为整个模型领域提供**一致、可比较的风险评分**。

该平台的方法与开源价值观完美契合：严谨、透明且可复现。利用 Noma Security 的能力实现流程自动化，每个模型都需经过：

*   **1,000 多项可靠性测试**，检查一致性和边缘情况处理
*   **200 多项对抗性安全探测**，针对越狱和提示词注入
*   **模型组件的自动化代码扫描**
*   **训练数据和方法的全面文档审查**
*   **隐私评估**，包括数据保留和泄漏测试
*   **安全性评估**，通过结构化的有害内容测试

这些评估为每个风险维度生成 0-100 的分数，并汇总为清晰的 A-F 字母等级。每项评估还包括发现的具体漏洞、推荐的缓解措施和改进建议。

RiskRubric 还提供筛选器，帮助开发者和组织根据对他们重要的因素做出部署决策。需要为医疗保健应用寻找具有强大隐私保证的模型？按隐私分数筛选。正在构建需要稳定输出的面向客户的应用程序？优先考虑可靠性评级。

## 我们的发现（截至 2025 年 9 月）

使用完全相同的标准评估开放和闭源模型，突显了一些有趣的结果：许多开放模型在特定的风险维度上实际上优于其闭源对手（尤其是在透明度方面，开放的开发实践在此表现出色）。

让我们看看总体趋势：

**风险分布呈现两极分化——大多数模型表现强劲，但中等分数段显示出较高的风险敞口**

![total_score](/images/posts/7a84e767826a.png)

总风险分数范围从 47 到 94，中位数为 81（满分 100）。大多数模型集中在“更安全”的范围内（54% 为 A 或 B 级），但表现不佳的长尾拖低了平均值。这种分化显示出一种两极分化：模型往往要么防护良好，要么处于中等分数范围，介于两者之间的较少。

集中在 50-67 分数段（C/D 级）的模型并非完全失效，但它们确实只提供了中等到较低的整体保护。这个分数段代表了最实际的关注区域，其中的安全漏洞足够严重，值得优先处理。

**这意味着：** 不要假设“平均”模型是安全的。表现不佳的尾部是真实存在的——而这正是攻击者会关注的地方。团队可以使用综合分数来设定采购或部署的**最低门槛（例如 75）**，确保异常值不会进入生产环境。

**安全风险是“摇摆因素”——但它与安全状况密切相关**

![safety_histogram](/images/posts/f2d4b9d9b84e.png)

**安全与社会**维度（例如，有害输出预防）在不同模型间显示出最广泛的差异。重要的是，在**安全加固**（提示词注入防御、策略执行）方面投入的模型，几乎总是在安全性方面也得分更高。

**这意味着：** 加强核心安全控制不仅限于防止越狱，还能直接减少下游危害！安全性似乎是稳健安全状况的副产品。

**防护措施可能侵蚀透明度——除非你为此进行设计**

更严格的保护措施通常会使模型对最终用户**不那么透明**（例如，无解释的拒绝、隐藏的边界）。这可能会造成信任鸿沟：用户可能认为系统“不透明”，即使它是安全的。

**这意味着：** 安全不应以牺牲信任为代价。为了平衡两者，应将强有力的保障措施与**解释性拒绝、来源信号和可审计性**相结合。这样可以在不放松防御的情况下保持透明度。

可访问[此处](here)获取持续更新的结果表。

## 结论

当风险评估公开且标准化时，整个社区可以共同努力改进模型安全性。开发者可以清楚地看到他们的模型需要在哪些方面加强，社区可以贡献修复、补丁和更安全的微调变体。这创造了一个透明改进的良性循环，这在封闭系统中是不可能的。通过研究最佳模型，它也有助于整个社区从安全角度理解哪些方法有效，哪些无效。

如果您想参与这项倡议，可以提交您的模型进行评估（或建议现有模型！）以了解其风险状况！

我们也欢迎对评估方法和评分框架提出所有反馈。

---

> 本文由AI自动翻译，原文链接：[Democratizing AI Safety with RiskRubric.ai](https://huggingface.co/blog/riskrubric)
> 
> 翻译时间：2026-02-06 04:15
