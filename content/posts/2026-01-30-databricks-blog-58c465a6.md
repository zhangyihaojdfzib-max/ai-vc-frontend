---
title: 生成式AI系统的隐性技术债务
title_original: Hidden Technical Debt of GenAI Systems
date: '2026-01-30'
source: Databricks Blog
source_url: https://www.databricks.com/blog/hidden-technical-debt-genai-systems
author: ''
summary: 本文比较了传统机器学习与生成式AI在工作流程上的差异，指出生成式AI引入了独特的技术债务来源，如工具泛滥、提示词堆砌、流程不透明等。文章强调，团队需要调整开发实践，将更多时间投入评估、利益相关者管理和主观质量监控，而非传统的数据清洗和特征工程，以有效管理这些债务，确保系统长期可维护性。
categories:
- AI基础设施
tags:
- 生成式AI
- 技术债务
- 机器学习工作流
- 模型部署
- AI评估
draft: false
translated_at: '2026-01-30T04:13:14.008842'
---

## 引言

如果我们广泛比较传统机器学习与生成式人工智能的工作流程，会发现两者在总体工作步骤上保持相似。两者都需要数据收集、特征工程、模型优化、部署、评估等步骤，但执行细节和时间分配却存在根本性差异。最重要的是，生成式人工智能引入了独特的技术债务来源，如果管理不当，这些债务会迅速累积，包括：

- **工具泛滥** - 难以管理和选择日益增多的 Agent（智能体）工具
- **提示词堆砌** - 提示词过于复杂，变得难以维护
- **流程不透明** - 缺乏适当的追踪机制，导致调试困难
- **反馈系统不足** - 未能有效捕获和利用人类反馈
- **利益相关者参与不足** - 未能与最终用户保持定期沟通

在本博客中，我们将逐一探讨每种形式的技术债务。最终，从传统机器学习转向生成式人工智能的团队需要意识到这些新的债务来源，并相应调整其开发实践——将更多时间投入到评估、利益相关者管理、主观质量监控和工具化上，而不是像传统机器学习项目那样，将主要精力放在数据清洗和特征工程上。

## 传统机器学习与生成式人工智能工作流程有何不同？

要理解该领域的现状，比较生成式人工智能工作流程与用于传统机器学习问题的工作流程会很有帮助。以下是一个高层次的概述。正如这个比较所揭示的，广义的工作流程步骤保持不变，但在执行细节上存在差异，导致不同步骤的侧重点不同。正如我们将看到的，生成式人工智能也引入了新形式的技术债务，这对我们如何在生产环境中维护系统产生了影响。

通常由包含三个步骤的基础流程覆盖：

- **预处理**（统计列转换，如标准化、归一化或独热编码）
- **模型预测**（将预处理后的数据传递给模型以产生输出）
- **后处理**（用附加信息丰富模型输出，通常是业务逻辑过滤器）

## 机器学习开发者在生成式人工智能项目中如何以不同方式分配时间？

根据平衡价格预测项目与构建工具调用 Agent（智能体）项目的第一手经验，我们发现模型开发和部署步骤存在一些重大差异。

### 模型开发循环

内部开发循环通常指机器学习开发者在构建和完善其模型流程时所经历的迭代过程。它通常发生在生产测试和模型部署之前。

以下是传统机器学习和生成式人工智能专业人员在此步骤中如何以不同方式分配时间：

**传统机器学习模型开发的时间消耗**

- **数据收集和特征优化**：在传统机器学习项目中，大部分时间都花在迭代优化特征和输入数据上。当涉及多个团队或特征过多难以手动管理时，会使用管理和共享特征的工具，例如 Databricks Feature Store。相比之下，评估是直接的——运行模型，查看定量指标是否有改善，然后再回头考虑如何通过更好的数据收集和特征来增强模型。例如，在我们的价格预测模型中，团队观察到大多数错误预测源于未能考虑数据异常值。然后，我们必须考虑如何纳入能代表这些异常值的特征，使模型能够识别这些模式。

**生成式人工智能模型和流程开发的时间消耗**

- **评估**：在生成式人工智能项目中，数据收集与转换和评估之间的相对时间分配是颠倒的。数据收集通常涉及为模型收集足够的上下文，这些上下文可以是非结构化的知识库文档或手册形式。这些数据不需要大量清洗。但评估则主观得多且复杂，因此也更耗时。你不仅要在模型流程上进行迭代，还需要在评估集上进行迭代。并且，相比传统机器学习，需要花费更多时间来考虑边缘情况。例如，初始的10个评估问题可能无法覆盖用户可能向支持机器人提出的全部问题范围，在这种情况下，你需要收集更多评估；或者，你设置的 LLM（大语言模型）评判标准可能过于严格，以至于你需要重新措辞其提示词，以防止相关答案无法通过测试。MLflow 的评估数据集对于版本控制、开发和审计必须始终正确工作的“黄金标准”示例集非常有用。
- **利益相关者管理**：此外，由于响应质量取决于最终用户的输入，工程师需要花费更多时间与业务最终用户和产品经理会面，以收集和确定需求优先级，并根据用户反馈进行迭代。历史上，传统机器学习通常不广泛面向最终用户（例如时间序列预测），或者较少暴露给非技术用户，因此生成式人工智能对产品管理的要求要高得多。可以通过托管在 Databricks Apps 上并调用 MLflow 反馈 API 的简单 UI 来收集响应质量反馈。然后，反馈可以被添加到 MLflow Trace 和 MLflow 评估数据集中，从而在反馈和模型改进之间形成良性循环。

下图比较了传统机器学习和生成式人工智能在模型开发循环中的时间分配。

### 模型部署循环

与模型开发循环不同，模型部署循环的重点不在于优化模型性能。相反，工程师专注于生产环境中的系统测试、部署和监控。

在此阶段，开发者可能会将配置移入 YAML 文件，以便更轻松地更新项目。他们也可能重构静态数据处理流程，使其以流式方式运行，使用更健壮的框架（如 PySpark）替代 Pandas。最后，他们需要考虑如何建立测试、监控和反馈流程以维持模型质量。

此时，自动化至关重要，持续集成和持续交付是不可或缺的要求。对于在 Databricks 上管理数据和人工智能项目的 CI/CD，Databricks Asset Bundles 通常是首选工具。它们使得将 Databricks 资源（如作业和管道）描述为源文件成为可能，并提供了一种在项目源文件旁边包含元数据的方法。

与模型开发阶段类似，在此阶段，生成式人工智能项目与传统机器学习项目中最耗时的活动并不相同。

**传统机器学习模型部署的时间消耗**

- **代码重构**：在传统的机器学习项目中，笔记本代码可能相当混乱。不同的数据集、特征和模型组合被不断测试、废弃和重组。因此，可能需要投入大量精力来重构笔记本代码，使其更加健壮。拥有一套固定的代码仓库文件夹结构（例如 Databricks Asset Bundles MLOps Stacks 模板）可以为这种重构过程提供所需的脚手架。重构活动的一些例子包括：
    - 将辅助代码抽象为函数
    - 创建辅助库，以便实用函数可以被导入和重复使用多次
    - 将配置从笔记本中提取到 YAML 文件中
    - 创建运行更快、更高效（例如，移除嵌套的 `for` 循环）的代码实现

- **质量监控**：质量监控是另一个耗时的工作，因为数据错误可能以多种形式出现且难以检测。特别是，正如 Shreya Shankar 等人在其论文《机器学习操作化：一项访谈研究》中指出的：“软错误，例如数据点中几个空值的特征，危害性较小，仍然可以产生合理的预测，这使得它们难以捕捉和量化。”此外，不同类型的错误需要不同的应对措施，而确定合适的应对措施并不总是那么容易。另一个挑战是，不同类型的模型漂移（如特征漂移、数据漂移和标签漂移）需要在不同的时间粒度（每日、每周、每月）上进行测量，这增加了复杂性。为了使这个过程更容易，开发人员可以使用 **Databricks Data Quality Monitoring** 在一个整体框架内跟踪模型质量指标、输入数据质量以及模型输入和预测的潜在漂移。

生成式 AI 模型部署的耗时点

- **质量监控**：对于生成式 AI，监控也占用了大量时间，但原因不同：
    - **实时性要求**：用于客户流失预测、价格预测或患者再入院等任务的传统机器学习项目可以以批处理模式提供预测，可能每天、每周或每月运行一次。然而，许多生成式 AI 项目是实时应用，例如虚拟支持 Agent、实时转录 Agent 或编码 Agent。因此，需要配置实时监控工具，这意味着需要实时端点监控、实时推理分析管道和实时警报。设置 API 网关（例如 **Databricks AI Gateway**）来对 LLM API 执行护栏检查，可以支持安全和数据隐私要求。这与传统的模型监控方法不同，后者是作为离线过程进行的。
    - **主观评估**：如前所述，生成式 AI 应用的评估是主观的。模型部署工程师必须考虑如何在他们的推理管道中操作化收集主观反馈。这可能采取对模型响应运行 LLM 评判评估的形式，或者选择一部分模型响应呈现给领域专家进行评估。专有模型提供商会随着时间的推移优化他们的模型，因此他们的“模型”实际上是容易退化的服务，评估标准必须考虑到模型权重不像自训练模型中那样是冻结的这一事实。提供自由形式反馈和主观评分的能力变得至关重要。像 **Databricks Apps** 和 **MLflow Feedback API** 这样的框架提供了更简单的用户界面，可以捕获此类反馈并将该反馈与特定的 LLM 调用关联起来。

- **测试**：在生成式 AI 应用中，测试通常更耗时，原因如下：
    - **未解决的挑战**：生成式 AI 应用本身越来越复杂，但评估和测试框架尚未跟上。一些使测试具有挑战性的场景包括：
        - 长的多轮对话
        - 可能捕获也可能未捕获企业组织上下文重要细节的 SQL 输出
        - 确保在链中使用了正确的工具
        - 评估应用中的多个 Agent
        处理这种复杂性的第一步通常是尽可能准确地捕获 Agent 输出的轨迹（工具调用、推理和最终响应的执行历史）。自动轨迹捕获和手动插桩的结合可以提供覆盖全部 Agent 交互所需的灵活性。例如，**MLflow Traces** 的 `trace` 装饰器可以用于任何函数以捕获其输入和输出。同时，可以在特定的代码块内创建自定义的 MLflow Traces spans 来记录更细粒度的操作。只有在使用插桩从 Agent 输出中聚合出可靠的真实来源后，开发人员才能开始识别故障模式并相应地设计测试。
    - **整合人类反馈**：在评估质量时整合这种输入至关重要。但有些活动是耗时的。例如：
        - 设计评分标准，以便标注人员有指导方针可循
        - 为不同场景设计不同的指标和评判者（例如，输出是否安全与输出是否有帮助）
        通常需要面对面的讨论和研讨会来创建一个关于 Agent 应如何响应的共享评分标准。只有在人类标注者达成一致后，他们的评估才能可靠地整合到基于 LLM 的评判者中，使用像 MLflow 的 `make_judge` API 或 **SIMBA Alignment Optimizer** 这样的功能。

- **待解决的挑战**：生成式AI应用本身正变得越来越复杂，但评估与测试框架尚未跟上步伐。一些使测试具有挑战性的场景包括：
    - 冗长的多轮对话
    - 可能捕捉也可能遗漏企业组织上下文重要细节的SQL输出
    - 确保链式调用中使用了正确的工具
    - 评估应用中的多个Agent（智能体）

处理这种复杂性的第一步，通常是尽可能准确地捕获Agent（智能体）输出的执行轨迹（即工具调用、推理和最终响应的执行历史记录）。结合自动轨迹捕获和手动埋点，可以提供覆盖全部Agent（智能体）交互所需的灵活性。例如，`MLflow Traces`的`trace`装饰器可用于任何函数以捕获其输入和输出。同时，可以在特定代码块内创建自定义的MLflow Traces跨度，以记录更细粒度的操作。只有通过埋点从Agent（智能体）输出中聚合出可靠的事实来源后，开发者才能开始识别故障模式并据此设计测试。

- **整合人类反馈**：在评估质量时，整合人类反馈至关重要。但有些活动非常耗时，例如：
    - 设计评分标准，以便标注人员有可遵循的指南
    - 为不同场景设计不同的指标和评判标准（例如，输出是否安全 vs. 输出是否有帮助）

通常需要通过现场讨论和研讨会来建立关于Agent（智能体）预期响应的共同评分标准。只有在人类标注者达成一致后，才能将其评估结果可靠地整合到基于LLM（大语言模型）的评判器中，例如使用MLflow的`make_judge` API或`SIMBA Alignment Optimizer`等功能。

## AI技术债

当开发者以实现快速但粗糙的解决方案为代价，牺牲了长期可维护性时，技术债便会累积。

**经典机器学习技术债**

Dan Sculley等人对这些系统可能累积的技术债类型做了很好的总结。在他们的论文《机器学习：技术债的高息信用卡》中，将其分为三大类：

- **数据债**：记录不全、未被考虑或悄然变化的数据依赖
- **系统级债**：大量的胶水代码、管道“丛林”以及“死”的硬编码路径
- **外部变化**：修改的阈值（如精确率-召回率阈值）或先前重要相关性的消失

**生成式AI引入了新形式的技术债，其中许多可能并不明显。本节将探讨这些隐性技术债的来源。**

### 工具蔓延

工具是扩展LLM（大语言模型）能力的强大方式。然而，随着使用工具数量的增加，它们可能变得难以管理。

工具蔓延不仅带来了可发现性和重用性问题，还可能对生成式AI系统的质量产生负面影响。当工具激增时，会出现两个关键的故障点：

- **工具选择**：LLM（大语言模型）需要能够从众多工具中正确选择要调用的工具。如果工具功能大致相似（例如调用数据API获取周度与月度销售统计数据），确保调用正确的工具就变得困难。LLM（大语言模型）将开始出错。
- **工具参数**：即使在成功选择了正确的工具后，LLM（大语言模型）仍需能够将用户问题解析成正确的参数集传递给工具。这是另一个需要考虑的故障点，当多个工具具有相似的参数结构时，这一点变得尤其困难。

应对工具蔓延最清晰的解决方案是，团队在使用工具时要有策略性并保持最少化。

然而，随着越来越多的团队将生成式AI集成到他们的项目和系统中，正确的治理策略有助于实现多工具和访问权限的可扩展管理。Databricks的产品Unity Catalog和AI Gateway正是为此类规模而构建的。

### 提示词堆砌

尽管最先进的模型能够处理多页指令，但过于复杂的提示词可能会引入诸如指令矛盾或信息过时等问题。当提示词未被编辑，而只是由不同领域专家或开发者随时间不断追加时，这种情况尤其突出。

随着不同故障模式的出现，或新的查询被纳入范围，人们很容易倾向于不断向LLM（大语言模型）提示词中添加越来越多的指令。例如，一个提示词可能最初提供处理财务相关问题的指令，然后扩展到产品、工程和人力资源相关的问题。

正如软件工程中的“上帝类”不是一个好主意并应被拆分一样，巨型提示词也应被分解为更小的提示词。事实上，Anthropic在其提示词工程指南中也提到了这一点。作为一般规则，使用多个较小的提示词而非一个冗长复杂的提示词，有助于提高清晰度、准确性和故障排除效率。

框架可以通过跟踪提示词版本以及强制执行预期的输入和输出来帮助保持提示词的可管理性。提示词版本控制工具的一个例子是`MLflow Prompt Registry`，而像`DSPy`这样的提示词优化器可以在Databricks上运行，将提示词分解为可独立或整体优化的自包含模块。

### 不透明的流水线

最近追踪功能受到关注是有原因的，大多数LLM（大语言模型）库和跟踪工具都提供了追踪LLM（大语言模型）链输入和输出的能力。当响应返回错误——例如可怕的“抱歉，我无法回答您的问题”——检查中间LLM（大语言模型）调用的输入和输出对于定位根本原因至关重要。

我曾参与一个应用开发，最初我假设SQL生成会是工作流中最成问题的步骤。然而，检查我的追踪记录却揭示了不同的情况：最大的错误来源实际上是一个查询重写步骤，我们在该步骤中将用户问题中的实体更新为与数据库值匹配的实体。LLM（大语言模型）会重写不需要重写的查询，或者开始向原始查询中塞入各种额外信息。这通常会扰乱后续的SQL生成过程。追踪功能在此帮助我快速识别了问题。

追踪正确的LLM调用可能耗时。仅实现开箱即用的追踪是不够的。使用如MLflow Traces这样的框架，通过可观测性对应用进行适当的埋点，是使Agent（智能体）交互更加透明的第一步。

### 用于捕获和利用人类反馈的系统不足

LLM（大语言模型）之所以卓越，是因为你可以向它们传递一些简单的提示词，将结果串联起来，最终得到似乎能很好理解细微差别和指令的东西。但若在这条路上走得太远，而不通过用户反馈来锚定响应，质量债可能会迅速累积。这就是尽早创建“数据飞轮”可以发挥作用的地方，它包含三个步骤：

- 确定成功指标
- 自动化测量这些指标的方式，或许通过一个用户界面，让用户可以对有效的内容提供反馈
- 迭代调整提示词或流水线以改进指标

在开发一个用于查询体育统计数据的文本转SQL应用时，我意识到了人类反馈的重要性。领域专家能够解释体育迷希望如何与数据交互，阐明他们关心什么，并提供其他见解，而作为一个很少观看体育比赛的人，我永远无法想到这些。没有他们的输入，我创建的应用很可能无法满足用户的需求。

尽管捕获人类反馈非常宝贵，但这通常极其耗时。首先需要与领域专家安排时间，然后创建评估标准以协调专家之间的差异，接着评估反馈以进行改进。如果反馈用户界面托管在业务用户无法访问的环境中，与IT管理员沟通以提供适当访问权限的过程可能会感觉永无止境。

### 缺乏与利益相关者的定期沟通

定期咨询最终用户、业务发起方和相邻团队，以确认是否在构建正确的东西，是各类项目的基本要求。然而，对于生成式AI项目，利益相关者沟通比以往任何时候都更为关键。

为什么频繁、深入的沟通很重要：

- 所有权与控制：定期会议有助于利益相关者感到他们有能力影响应用的最终质量。他们可以成为协作者，而不仅仅是批评者。当然，并非所有反馈都同等重要。一些利益相关者不可避免地会开始提出对于MVP来说为时过早、或者超出LLM当前处理能力的要求。就什么可以实现、什么不能实现进行协商和教育非常重要。否则，可能会出现另一个风险：过多的功能请求，却没有刹车机制。
- 我们不知道我们不知道什么：生成式AI是如此之新，以至于大多数人，无论是技术人员还是非技术人员，都不知道LLM能妥善处理什么、不能处理什么。开发LLM应用对所有参与者来说都是一次学习之旅，定期沟通是让每个人保持信息同步的一种方式。

生成式AI项目中可能还需要解决许多其他形式的技术债，包括强制执行适当的数据访问控制、设置防护栏以管理安全性和防止提示词注入、防止成本失控等等。这里我只列出了似乎最重要且可能容易被忽视的那些。

## 结论

经典机器学习和生成式AI是同一技术领域的不同风味。虽然意识到它们之间的差异并考虑这些差异对我们构建和维护解决方案方式的影响很重要，但某些真理始终不变：沟通仍然能弥合差距，监控仍然能防止灾难，而从长远来看，清晰、可维护的系统仍然优于混乱的系统。

想要评估您组织自身的AI成熟度吗？请阅读我们的指南：解锁AI价值：企业AI就绪指南。

## 下一步是什么？

---

> 本文由AI自动翻译，原文链接：[Hidden Technical Debt of GenAI Systems](https://www.databricks.com/blog/hidden-technical-debt-genai-systems)
> 
> 翻译时间：2026-01-30 04:13
