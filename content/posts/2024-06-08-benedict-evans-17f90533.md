---
title: 构建AI产品：如何应对大模型的“幻觉”与不确定性
title_original: Building AI products — Benedict Evans
date: '2024-06-08'
source: Benedict Evans
source_url: https://www.ben-evans.com/benedictevans/2024/6/8/building-ai-products
author: Benedict Evans
summary: 本文以作者使用ChatGPT获取错误签证信息为例，指出当前大语言模型（LLM）本质是概率性系统，无法保证事实准确性。文章核心观点在于：不应将LLM的“幻觉”视为无用，而应将其视为产品设计问题。作者提出两种解决路径：一是通过科学进步提升模型能力；二是从产品层面入手，通过限定应用领域、设计明确界面或将其作为隐藏功能集成，来构建真正有用的大众市场AI产品。文章强调，成功的AI产品需要向用户靠拢，明确传达其能力边界，而非要求用户适应技术。
categories:
- AI产品
tags:
- AI产品设计
- 大语言模型
- 幻觉问题
- 人机交互
- 生成式AI
draft: false
---

构建AI产品
我将于周一飞往印度进行短暂旅行，因此刚刚花了一个小时，在一个漏洞百出的在线签证申请流程中艰难操作。完成后，鉴于我现在了解了流程，我向ChatGPT 4o询问了相关问题。它给出的大部分要点都部分或完全错误。

这是一个“不公平”的测试。它是使用LLM（大语言模型）的一种“糟糕”方式的绝佳例证。这些模型不是数据库。它们无法针对问题给出精确的事实性答案，它们是概率性系统，而非确定性系统。目前的LLM无法就这个问题给我一个完全且精确的正确答案。答案可能碰巧正确，但你无法保证。

有一种趋势（常与加密货币和NFT类比）认为这意味着这些东西毫无用处。这是一种误解。相反，思考生成式AI模型的一个有用方式是：它们极其擅长告诉你，一个针对此类问题的“好答案”大概会是什么样子。有些用例中，“看起来像好答案”正是你想要的；而另一些用例中，“大致正确”就是“精确错误”。

实际上，更进一步说，完全相同的提示词和完全相同的输出，其好坏可能取决于你为何需要它。

尽管如此，在这个案例中，我确实需要一个精确答案，而原则上无法依赖ChatGPT给我一个，结果它给了我一个错误答案。我要求它做了它做不到的事，所以这是个不公平的测试，但它是个相关的测试。答案仍然是错的。

有两种方法试图解决这个问题。一是将其视为科学问题——目前尚处早期，模型会变得更好。你可以大谈“RAG（检索增强生成）”和“多Agent（智能体）”。模型当然会变得更好，但能好多少？你可能会花上几周时间观看机器学习科学家们就此争论的YouTube视频，结果只发现他们其实也不知道。实际上，这是“LLM会产生AGI吗？”争论的一个变体，因为一个能完全正确回答“任何”问题的模型，在我看来至少像是某种AGI的一个不错定义（不过，再次强调，没人知道）。

但另一条路径是将其视为产品问题。我们如何围绕这些我们应假定会“出错”的模型，构建有用的大众市场产品？

AI从业者对像我这样的例子的典型反应是说“你用错了”——我犯了两个错误：1. 问了错误类型的问题；2. 以错误的方式提问。我应该做一堆提示词工程！但过去50年消费级计算传达的信息是：你不能通过让用户学习命令行来推动普及——你必须向用户靠拢。

我认为我们可以进一步分解这个问题，将其分为两类产品问题。

一方面，截图中的产品设计在模型本身具有内在不确定性的情况下，却传达了确定性。谷歌（大多时候）给你十个蓝色链接，这传达的是“答案很可能在这些里面”，但这里我们得到的是一个“正确”答案。这误导了很多人，尤其是因为文本生成（与实际答案不同）几乎完美无缺。事实上，德勤这项引人入胜的调查表明，人们在使用这些系统后，更可能被这种表面的确定性所误导。

但问题的另一半是，甚至在我得到“答案”之前，产品也没有告诉我可以问什么。我给了它一个“糟糕”的查询（一个它无法很好回答的问题），但产品中没有任何东西告诉我这一点。相反，它被呈现给我作为一个通用工具。如果产品必须尝试回答任何问题，那会让模型更难正确，也会让界面更难传达什么是好问题。

我为将在印度做的演讲制作了下面的幻灯片，试图概括由此提出的替代方案。

最激进的方法是完全通用的聊天机器人即产品，其挑战我刚刚已经讨论过。但至少还有另外两种方法。

第一种是将产品限定在一个狭窄的领域内，这样你就可以围绕输入和输出创建自定义的用户界面，传达它能做什么、不能做什么以及你可以问什么，或许还能聚焦模型本身（因此用到RAG）。这让我们得到了过去12个月爆炸式增长的编码助手和营销工具，以及知识管理工具的初步尝试。WPP构建了一个内部仪表板，让员工引导模型适应特定的品牌语调或目标人群。因此，“让这个工具为品牌Y的产品X针对演示Z提出50个创意点子——但不要问它你是否得了阑尾炎。”你将提示词包装在按钮和用户界面中——融入产品。

但另一种方法是，用户永远看不到提示词或输出，甚至不知道这是生成式AI，输入和输出都被抽象为其他事物内部的功能。模型实现了某种能力，或者让构建这种能力变得更快更容易，即使你以前也能做到。上一波机器学习大多就是这样融入软件的：出现了新功能，或者功能更好用、能更快更便宜地构建，但用户永远不知道它们是“AI”——它们不是紫色的，也没有小星星簇。因此有个老笑话：AI就是任何还没起作用的东西，因为一旦它起作用了，它就只是软件。

从另一个轴来看：对于任何新技术，我们开始时都试图让它适应我们已有的问题，而现有巨头则试图将其变成一项功能（因此过去一年谷歌和微软将LLM撒遍其产品）。然后初创公司用它来解构现有巨头（解构搜索、Oracle或电子邮件），但与此同时，其他初创公司试图找出我们能构建什么，才是真正原生于此新技术的。这是分阶段进行的。最初，Flickr有iPhone应用，但后来Instagram利用了智能手机摄像头，并利用本地计算添加滤镜，再后来，Snap和TikTok利用触摸屏、视频和位置，创造了真正原生于此平台的东西。那么，我们用这个技术构建什么原生体验，它们不是聊天机器人本身，或者“错误率”无关紧要，而是以某种方式抽象了这种新能力？

这当然提出了一个悖论，我之前也谈过：我们这里有一种通用技术，但部署的方式却是将其解构为单一用途的工具和体验。然而，将此视为悖论可能只是抽象层次没找对。电动机是一种通用技术，但你不是从家得宝买一盒电动机——你买的是电钻、洗衣机和搅拌机。通用技术是通过用例来实例化的。个人电脑和智能手机是取代单一用途工具的通用工具——它们取代了打字机、计算器、录音机和音乐播放器——但每一项功能都是通过一个单一用途的软件实现的：大多数人不用Excel当文字处理器。一些人如此兴奋于LLM的一个原因是，它们可能不会遵循那种模式：它们可能向上突破所有这些抽象层次，直达顶端。那将不会给“单薄的GPT包装器”留下空间。然而，我认为它们目前还做不到这一点，所以我刚才写的一切，其实只是在思考：即使那种情况永远不会发生，你能构建什么来改变世界。

---

> 本文由AI自动翻译，原文链接：[Building AI products — Benedict Evans](https://www.ben-evans.com/benedictevans/2024/6/8/building-ai-products)
> 
> 翻译时间：2026-01-05 17:05
