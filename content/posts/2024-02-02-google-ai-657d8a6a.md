---
title: 早期读出干预：缓解虚假特征与简单性偏差
title_original: Intervening on early readouts for mitigating spurious features and
  simplicity bi
date: '2024-02-02'
source: Google AI Blog
source_url: http://blog.research.google/2024/02/intervening-on-early-readouts-for.html
author: null
summary: 本文探讨了深度学习中模型因数据偏差而依赖虚假特征的问题，并提出了基于早期读出的解决方案。研究通过两种方法应对挑战：一是利用早期读出的错误置信度作为指标，改进模型蒸馏过程，提升学生模型对虚假特征的鲁棒性；二是引入特征筛技术，通过交替识别和擦除网络早期层中的简单偏差特征，迫使模型学习更复杂、更具预测性的特征，从而显著提升模型在未知领域的泛化能力。实验在多个标准基准数据集上验证了方法的有效性。
categories:
- AI研究
tags:
- 深度学习
- 模型偏差
- 特征学习
- 模型泛化
- 负责任AI
draft: false
translated_at: '2026-01-06T14:49:02.033Z'
---

2024年2月2日
作者：Google Research 预博士研究员 Rishabh Tiwari 与研究科学家 Pradeep Shenoy
快速链接

现实世界中的机器学习模型通常在有限数据上训练，这些数据可能包含非预期的统计偏差。例如，在CELEBA名人图像数据集中，女性名人中金发比例过高，导致分类器在预测大多数女性面孔的发色时错误地输出“金发”——在此例中，性别成为预测发色的虚假特征。此类不公平偏差可能在医疗诊断等关键应用中产生重大影响。

令人惊讶的是，近期研究还发现深度网络存在放大此类统计偏差的内在倾向，即所谓的深度学习简单性偏差。这种偏差表现为深度网络倾向于在训练早期识别弱预测性特征，并持续依赖这些特征，而未能识别更复杂且可能更准确的特征。

基于上述认知，我们通过应用早期读出和特征遗忘技术，针对虚假特征与简单性偏差的双重挑战提出了简单有效的解决方案。首先，在《使用早期读出来调节蒸馏中的特征偏差》中，我们证明从深度网络早期层进行预测（称为“早期读出”）能够自动提示所学表征的质量问题。具体而言，当网络依赖虚假特征时，这些预测更容易出错，且错误置信度更高。我们利用这种错误置信度来改进模型蒸馏的效果——在这种场景中，较大的“教师”模型指导较小“学生”模型的训练。随后在《使用特征筛克服深度网络中的简单性偏差》中，我们通过让网络“遗忘”问题特征并促使它寻找更优、更具预测性的特征，直接对这些指示信号进行干预。与先前方法相比，这显著提升了模型在未知领域中的泛化能力。我们的AI原则与负责任AI实践指导着这些高级应用的研究开发，并帮助我们应对统计偏差带来的挑战。

| 对比使用特征筛与未使用特征筛训练的两种模型假设性响应的动画 |
用于去偏差蒸馏的早期读出

我们首先阐述早期读出的诊断价值及其在去偏差蒸馏中的应用，即确保学生模型通过蒸馏继承教师模型对特征偏差的鲁棒性。我们从标准蒸馏框架出发，该框架中学生模型的训练结合了标签匹配（最小化学生输出与真实标签的交叉熵损失）和教师匹配（最小化学生与教师输出在给定输入下的KL散度损失）。

假设在学生模型的中间表征之上训练一个线性解码器（即名为Aux的小型辅助神经网络）。我们将该线性解码器的输出称为网络表征的早期读出。我们的发现是：早期读出在包含虚假特征的实例上会产生更多错误，且这些错误的置信度高于其他错误的置信度。这表明早期读出错误的置信度是模型依赖潜在虚假特征的一个相当可靠的自动化指标。

我们利用该信号在逐实例基础上调节教师模型对蒸馏损失的贡献，从而显著提升了训练后学生模型的性能。

> 本文由AI自动翻译，原文链接：[Intervening on early readouts for mitigating spurious features and simplicity bi](http://blog.research.google/2024/02/intervening-on-early-readouts-for.html)
> 
> 翻译时间：2026-01-06 04:29
