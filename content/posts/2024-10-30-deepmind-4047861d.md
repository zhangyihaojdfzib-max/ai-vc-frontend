---
title: 谷歌音频生成前沿：多说话人对话模型突破
title_original: Pushing the frontiers of audio generation
date: '2024-10-30'
source: Google DeepMind
source_url: https://deepmind.google/blog/pushing-the-frontiers-of-audio-generation/
author: ''
summary: 本文介绍了谷歌在音频生成领域的最新研究进展，重点展示了其能够生成长达2分钟、高质量多说话人对话的语音生成技术。该技术基于SoundStream和AudioLM等前期工作，通过开发更高效的语音编解码器和专用Transformer架构，实现了在单个TPU芯片上以比实时快40倍的速度生成自然、连贯的对话音频。目前，该技术已应用于NotebookLM
  Audio Overviews和Illuminate等产品功能，旨在提升数字助手和AI工具的交互自然度与内容可及性。
categories:
- AI研究
tags:
- 音频生成
- 语音合成
- 多说话人对话
- 谷歌AI
- Transformer
draft: false
translated_at: '2026-02-10T04:32:24.808224'
---

# 推动音频生成的前沿发展

Zalán Borsos, Matt Sharifi 和 Marco Tagliasacchi

![一幅描绘语音模式、对话生成的迭代进展以及两个声音之间轻松对话的插图。](/images/posts/5baf0133354c.jpg)

我们开创性的语音生成技术正在帮助世界各地的人们与更自然、更具对话性和更直观的数字助手及AI工具进行交互。

语音是人类联系的核心。它帮助世界各地的人们交流信息和想法、表达情感并建立相互理解。随着我们为生成自然、动态语音而构建的技术不断改进，我们正在解锁更丰富、更具吸引力的数字体验。

在过去的几年里，我们一直在推动音频生成的前沿，开发能够从一系列输入（如文本、节奏控制和特定声音）中生成高质量、自然语音的模型。这项技术为许多谷歌产品和实验中的单说话人音频提供支持——包括Gemini Live、Project Astra、Journey Voices和YouTube的自动配音——并正在帮助世界各地的人们与更自然、更具对话性和更直观的数字助手及AI工具进行交互。

与谷歌内部的合作伙伴合作，我们最近帮助开发了两项新功能，可以生成长篇、多说话人对话，使复杂内容更易于获取：

- **NotebookLM Audio Overviews** 将上传的文档转化为引人入胜、生动活泼的对话。只需点击一下，两位AI主持人就会总结用户材料，在主题之间建立联系，并进行来回的轻松交谈。
- **Illuminate** 创建关于研究论文的正式AI生成讨论，以帮助知识更易于获取和消化。

在此，我们概述了支撑所有这些产品和实验工具的最新语音生成研究。

## 开创性的音频生成技术

多年来，我们一直投资于音频生成研究，并在我们的产品和实验工具中探索生成更自然对话的新方法。在我们之前关于SoundStorm的研究中，我们首次展示了生成多个说话人之间30秒自然对话片段的能力。

这扩展了我们早期的工作SoundStream和AudioLM，使我们能够将许多基于文本的语言建模技术应用于音频生成问题。

SoundStream是一种神经音频编解码器，可以在不损失质量的情况下高效压缩和解压缩音频输入。作为训练过程的一部分，SoundStream学习如何将音频映射到一系列声学Token。这些Token捕获了以高保真度重建音频所需的所有信息，包括韵律和音色等属性。

AudioLM将音频生成视为一项语言建模任务，以生成像SoundStream这样的编解码器的声学Token。因此，AudioLM框架对生成音频的类型或构成不做任何假设，并且可以灵活处理各种声音，而无需调整架构——这使其成为建模多说话人对话的良好候选方案。

两个说话人演示惊讶和怀疑的音频样本。

两个说话人语音重叠的音频样本。

两个说话人讲述一个有趣故事、在笑点处有笑声的音频片段。

两个说话人对惊喜生日派对表达兴奋的音频片段。

NotebookLM Audio Overview基于几份与土豆相关的文档生成的多说话人对话示例。

基于这项研究，我们最新的语音生成技术可以在给定对话脚本和说话人轮换标记的情况下，生成2分钟的对话，并具有改进的自然度、说话人一致性和声学质量。该模型在单个Tensor Processing Unit (TPU) v5e芯片上，通过一次推理过程，在3秒内即可完成此任务。这意味着它生成音频的速度比实时快40倍以上。

## 扩展我们的音频生成模型

将我们的单说话人生成模型扩展到多说话人模型，就变成了数据和模型容量的问题。为了帮助我们最新的语音生成模型产生更长的语音片段，我们创建了一种更高效的语音编解码器，用于将音频压缩成一系列Token，速率低至每秒600比特，同时不损害其输出质量。

我们的编解码器产生的Token具有分层结构，并按时间帧分组。组内的第一个Token捕获语音和韵律信息，而最后一个Token编码精细的声学细节。

即使使用我们新的语音编解码器，生成2分钟的对话也需要生成超过5000个Token。为了对这些长序列进行建模，我们开发了一种专门的Transformer架构，可以高效处理信息层次结构，与我们的声学Token结构相匹配。

利用这种技术，我们可以在一次自回归推理过程中，高效生成与对话相对应的声学Token。一旦生成，这些Token就可以使用我们的语音编解码器解码回音频波形。

![动画展示了我们的语音生成模型如何自回归地产生音频Token流，这些Token被解码回由双说话人对话组成的波形。](/images/posts/f0186397c8b8.jpg)

![](/images/posts/4bf21547fada.jpg)

动画展示了我们的语音生成模型如何自回归地产生音频Token流，这些Token被解码回由双说话人对话组成的波形。

为了教导我们的模型如何生成多个说话人之间的真实交流，我们在数十万小时的语音数据上对其进行了预训练。然后，我们在一个规模小得多、但具有高声学质量和精确说话人标注的对话数据集上对其进行了微调，该数据集包含来自多位配音演员的非脚本对话和真实的不流畅现象——真实对话中的“嗯”和“啊”。这一步教会了模型在生成的对话中如何可靠地在说话人之间切换，并仅输出具有真实停顿、语调和时机的录音室质量音频。

根据我们的AI原则以及负责任地开发和部署AI技术的承诺，我们正在整合我们的SynthID技术，为来自这些模型的非瞬时AI生成音频内容添加水印，以帮助防范该技术的潜在滥用。

## 未来的新语音体验

我们现在专注于提高模型的流畅度、声学质量，并为韵律等功能添加更精细的控制，同时探索如何最好地将这些进展与其他模态（如视频）相结合。

先进语音生成的潜在应用非常广泛，尤其是与我们的Gemini系列模型结合时。从增强学习体验到使内容更普遍可访问，我们很高兴能继续突破基于语音技术的可能性边界。

致谢

本工作的作者：Zalán Borsos, Matt Sharifi, Brian McWilliams, Yunpeng Li, Damien Vincent, Félix de Chaumont Quitry, Martin Sundermeyer, Eugene Kharitonov, Alex Tudor, Victor Ungureanu, Karolis Misiunas, Sertan Girgin, Jonas Rothfuss, Jake Walker 和 Marco Tagliasacchi。

我们感谢Leland Rechis, Ralph Leith, Paul Middleton, Poly Pata, Minh Truong 和 RJ Skerry-Ryan在对话数据方面的关键努力。

我们非常感谢Labs、Illuminate、Cloud、Speech和YouTube的协作者们，感谢他们将这些模型引入产品的出色工作。

我们也感谢Françoise Beaufays, Krishna Bharat, Tom Hume, Simon Tokumine, James Zhao对项目的指导。

### 新的生成式AI工具开启音乐创作之门

![](/images/posts/b6e56a6c713d.jpg)

### 为视频生成音频

![](/images/posts/82441e510765.jpg)

### 重塑音乐创作的未来

![](/images/posts/b0c697b7dd5e.jpg)

### 运用WaveNet技术，让言语障碍用户重获原声

![](/images/posts/f2400d9ee414.jpg)

### WaveNet：一种原始音频生成模型

---

> 本文由AI自动翻译，原文链接：[Pushing the frontiers of audio generation](https://deepmind.google/blog/pushing-the-frontiers-of-audio-generation/)
> 
> 翻译时间：2026-02-10 04:32
