---
title: 关于AGI的思考：从科幻到现实的技术迷思
title_original: Ways to think about AGI — Benedict Evans
date: '2024-05-04'
source: Benedict Evans
source_url: https://www.ben-evans.com/benedictevans/2024/5/4/ways-to-think-about-agi
author: ''
summary: 本文以作者祖父的科幻小说为引，探讨了人工智能（AI）向人工通用智能（AGI）发展的历史与现状。文章指出，尽管大语言模型（LLM）的突破引发了AGI可能临近的乐观预测，但专家们对其实质、实现路径及时间表仍无共识。AGI本身更像一个思想实验或占位符，我们缺乏关于“通用智能”的连贯理论模型，当前讨论多基于直觉而非科学定论。文章提醒应避免循环定义，并区分AGI的生存风险与日常AI滥用问题。
categories:
- AI研究
tags:
- AGI
- 大语言模型
- 人工智能
- 技术哲学
- AI风险
draft: false
translated_at: '2026-01-08T04:30:12.329343'
---

# 关于AGI的思考方式

《逻辑乔伊》手稿

1946年，我的祖父以“默里·莱因斯特”为笔名，发表了一篇名为《逻辑乔伊》的科幻小说。故事中，每个人都拥有一台连接全球网络的计算机（称为“逻辑”），它能处理从银行业务、新闻阅读到视频通话的一切事务。有一天，其中一台名为“乔伊”的逻辑开始为网络上的任何请求提供“有用”的答案：比如发明一种无法检测的毒药，或者建议抢劫银行的最佳方式。恐慌随之而来——“检查你们的审查电路！”——直到人们找出该拔掉哪根插头。（与此同时，我的另一位祖父正在使用计算机监视德国人，然后是俄罗斯人。）

自我们开始思考计算机以来，我们一直在疑惑它们能否从仅仅处理穿孔卡片和数据库的机器，跃升为某种“人工智能”，并思考那将意味着什么，以及我们使用“智能”这个词究竟想表达什么。有个老笑话是：“AI”指的是任何尚未实现的技术，因为一旦它实现了，人们就会说“那不是AI——那只是软件”。计算器能进行超人般的数学运算，数据库拥有超人般的记忆力，但它们做不了其他事，也不理解自己在做什么，就像洗碗机不理解碗碟，电钻不理解孔洞一样。电钻只是机器，数据库是“超人般的”，但它们终究只是软件。而人类在某种程度上拥有不同的东西，同样，狗、黑猩猩、章鱼和许多其他生物也在某种尺度上拥有这种特质。AI研究人员开始将这种特质称为“通用智能”，因此创造它就成了“人工通用智能”——AGI。

如果我们真的能在软件中创造出与人类智能具有实质等同性的东西，那么这显然将是一件大事。我们能制造出能够推理、规划和理解的软件吗？至少，这将极大地改变我们能够自动化的工作范畴，正如我祖父和无数其他科幻作家所指出的，它可能意味着更多。

自1946年以来，每隔几十年，就会出现一波认为类似技术即将实现的兴奋浪潮，每次又都伴随着失望和“AI寒冬”，因为当时的技术进展放缓，我们意识到还需要数量未知的进一步突破。1970年，AI先驱马文·明斯基声称“在三到八年内，我们将拥有一台具备普通人通用智能的机器”，但每次我们认为找到了实现它的方法时，结果发现那只是更复杂的软件（或者根本行不通）。

众所周知，18个月前兴起的大语言模型（LLM）再次掀起了这样一波浪潮。此前认为AGI可能还需数十年才能实现的严肃AI科学家们，现在认为它可能近在眼前。极端情况下，所谓的“末日论者”认为，当前研究确实存在AGI自发涌现的风险，这可能对人类构成威胁，并呼吁政府采取紧急行动。其中部分言论来自寻求竞争壁垒的自利公司（“这非常危险，我们正在以最快速度构建它，但别让其他人做”），但也有许多是真诚的担忧。

（顺便提一下，我应该指出，末日论者关于AGI可能想要并且能够毁灭或控制人类，或将我们视为宠物的“生存风险”担忧，与那些更日常的担忧——例如政府将如何利用AI进行人脸识别，或讨论AI偏见、AI深度伪造，以及人们滥用AI或搞砸的其他所有方式——是完全独立的，就像他们对其他所有技术所做的那样。）

然而，对于每一位认为AGI可能即将到来的专家，就有另一位专家持不同意见。有些人认为LLM可能通过规模扩展一路达到AGI，而另一些人则认为，我们仍然需要数量未知的进一步突破。

更重要的是，他们都会同意自己实际上并不知道。这就是为什么我使用“可能”或“或许”这类术语——我们的第一站是诉诸权威（这通常被视为逻辑谬误，姑且不论其价值），但权威们告诉我们他们不知道，而且意见不一。

他们之所以不知道，是因为我们没有一个关于通用智能究竟是什么、为什么人类似乎比狗更擅长于此，以及人类或狗与乌鸦或章鱼究竟有何不同的连贯理论模型。同样，我们不知道LLM为何表现得如此出色，也不知道它们能改进到什么程度。我们在基础和机械层面上了解神经元和Token，但我们不知道它们为何有效。我们对此有许多局部理论，但我们不了解整个系统。除非诉诸宗教，我们不知道有任何理由认为AGI无法被创造（它似乎不违反任何物理定律），但我们不知道如何创造它，也不知道它是什么，除了作为一个概念。

因此，一些专家看到LLM的惊人进展后说“或许有可能！”，另一些则说“或许有可能，但很可能不行！”，这从根本上说是一种直觉和本能的评估，而非科学的判断。

事实上，“AGI”本身就是一个思想实验，或者可以说，是一个占位符。因此，我们必须小心循环定义，以及通过定义来断言其存在、确定性或必然性。

如果我们一开始就将AGI定义为一种实际上等同于人类“所有”方面（除了某种物理形态）的新生命形式，甚至包括“意识”、情感和权利等概念，然后假设在获得更多算力的情况下它会变得智能得多（并且假设地球上确实有大量闲置算力可用），并假设它能立即突破任何控制，那么这听起来很危险，但实际上，你只是回避了问题的实质。

正如安瑟伦所论证的，如果你将上帝定义为存在之物，那么你就证明了上帝存在，但你无法说服任何人。确实，许多关于AGI的讨论听起来就像过去一些神学家和哲学家试图从第一性原理出发，通过推理来推断上帝本质的尝试。你的论证内部逻辑可能非常严密（哲学家们花了几个世纪才弄清楚为什么安瑟伦的证明是无效的），但你无法通过这种方式创造知识。

同样，你可以调查许多AI科学家，了解他们感到有多不确定，并得出统计上准确的平均结果，但这本身并不能产生确定性，就像对神学家进行统计上准确的抽样调查无法确定上帝的本质一样，或者，也许就像将足够多的次级抵押贷款打包在一起就能产生AAA级债券——这是另一种试图通过平均不确定性来产生确定性的尝试。预测技术时最根本的谬误之一是说“人们过去对X的判断是错的，所以他们对Y的判断现在也一定是错的”，而顶尖AI科学家过去确实判断错误这一事实，绝对不意味着他们现在也是错的，但这确实告诉我们要谨慎。他们完全有可能同时都错了。

那么，你又如何知道通用智能会是什么样子呢？以赛亚·伯林曾提出，即使原则上宇宙存在一个目的，并且这个目的原则上是可以被发现的，也没有先验的理由认为它必须是有趣的。"上帝"可能是真实的，也可能是无聊的，并且不关心我们，而我们也不知道会得到什么样的AGI。它可能扩展到比人类智能100倍，也可能只是快得多但并不更智能（智能"仅仅"关乎速度吗？）。我们可能创造出极其有用但聪明程度不超过狗的通用智能——毕竟，狗确实拥有通用智能，并且像数据库或计算器一样，具备一种超人的能力（嗅觉）。我们不知道。

更进一步说，当我听马克·扎克伯格谈论Llama 3时，我注意到他将"通用智能"描述为一种分阶段到来的事物，不同的模态会一点点逐步实现。也许人们会指着Llama 6或ChatGPT 7的"通用智能"说："那不是AGI，那只是软件！"我们创造AGI这个术语，是因为AI已经逐渐仅指代软件，也许"AGI"也会遭遇同样的命运，届时我们将需要发明另一个新词。

这种根本上的不确定性，甚至在我们讨论的层面都存在，或许就是为什么所有关于AGI的讨论似乎都会转向类比。如果你能将其比作核裂变，你就知道会发生什么，也知道该怎么做。但这不是裂变，不是生物武器，也不是陨石。这是软件，它可能变成也可能不会变成AGI，它可能具有也可能不具有某些特性，其中一些特性可能是有害的，而我们不知道。虽然一颗巨大的陨石撞击地球只会带来灾难，但软件和自动化是工具，在过去200年里，自动化有时对人类不利，但大多数时候它都是一件非常好的事情，我们应该希望得到更多。

因此，我已经用神学做过类比，但我更倾向于用阿波罗计划来比喻。我们当时有引力理论，也有火箭工程理论。我们知道火箭为什么不会爆炸，如何模拟燃烧室的压力，以及如果将其尺寸增大25%会发生什么。我们知道它们为什么能升空，以及需要飞多远。你可以把土星火箭的规格交给艾萨克·牛顿，他至少原则上可以进行计算：这样的重量，这样的推力，这样的燃料……它能到达目的地吗？在这方面，我们没有任何对等物。我们不知道LLM（大语言模型）为何有效，它们能变得多大，或者它们需要走多远。然而，我们不断把它们做大，而它们似乎确实越来越接近目标。它们能到达吗？也许，是的！

在这个主题上，一些人认为我们正处于AI或AGI的经验主义阶段：我们在构建事物并进行观察，却不知道它们为何有效，理论可以稍后出现，有点像伽利略出现在牛顿之前（英国有个老笑话，讲一个法国人说"这在实践中都很好，但它在理论上成立吗？"）。然而，尽管我们可以凭经验看到火箭升空，但我们不知道月球有多远。我们无法将人类和ChatGPT绘制在一张图表上，然后画一条线来预测一个何时会达到另一个的水平，即使只是按当前增长速度进行外推。

所有类比都有缺陷，当然，我这个类比的缺陷在于，如果阿波罗计划出错，其负面影响即使在理论上也不会是人类灭绝。在我祖父那个时代稍早一些，另一位杂志作家曾这样描述未知风险：

前几天我在报纸上读到关于那些试图分裂原子的家伙，关键在于他们完全不清楚如果成功了会发生什么。可能一切安好。另一方面，也可能并非如此。毫无疑问，如果一个人在分裂原子后，突然发现房子在浓烟中炸毁，自己也被撕成碎片，他会觉得自己相当愚蠢。

好吧，吉夫斯，P.G. 沃德豪斯，1934年

那么，对于真实但未知的风险，你更倾向于持何种态度？你更喜欢哪个思想实验？我们可以回到那些半被遗忘的本科哲学（帕斯卡的赌注！安瑟伦的证明！），但如果你无法知道，你是担忧，还是耸耸肩？我们如何看待其他风险？陨石作为AGI的类比很糟糕，因为我们知道它们是真实的，知道它们可能毁灭人类，而且它们毫无益处（除非它们非常非常小）。然而，我们并没有真正在寻找它们。

不过，假设你认为末日论者是正确的：你能做什么？这项技术原则上是公开的。开源模型正在激增。目前，LLM（大语言模型）需要大量昂贵的芯片（英伟达在过去12个月里卖出了475亿美元，仍无法满足需求），但从十年的视角看，模型会变得更高效，芯片将无处不在。最终，你无法禁止数学。在几十年的时间尺度上，它无论如何都会发生。如果你必须用核裂变来类比，想象一下如果我们发现了一种方法，任何人都可以用家用材料在车库里造出炸弹——想要阻止这种情况，祝你好运。（末日论者可能会回应说，这解答了费米悖论：在某个阶段，每个文明都会创造出AGI，然后AGI把他们变成了回形针。）

不过，默认情况下，这将遵循AI的所有其他浪潮，变成"仅仅"是更多的软件和更多的自动化。自动化总是带来摩擦性的痛苦，可以追溯到卢德分子，而英国邮政局丑闻提醒我们，即使没有AGI，软件也能毁掉人们的生活。LLM（大语言模型）将带来更多的痛苦和更多的丑闻，但生活仍将继续。至少，这是我自己更倾向的答案。

> 本文由AI自动翻译，原文链接：[Ways to think about AGI — Benedict Evans](https://www.ben-evans.com/benedictevans/2024/5/4/ways-to-think-about-agi)
> 
> 翻译时间：2026-01-08 04:30
