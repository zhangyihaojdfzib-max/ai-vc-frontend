---
title: Gemma Scope 2发布：开源大模型可解释性工具套件，助力AI安全研究
title_original: 'Gemma Scope 2: Helping the AI Safety Community Deepen Understanding
  of Complex Language Model Behavior'
date: '2025-12-16'
source: Google DeepMind
source_url: https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/
author: Language Model Interpretability Team
summary: 本文介绍了Gemma Scope 2的发布，这是一套全面、开源的语言模型可解释性工具套件，适用于所有Gemma 3模型规模。该工具套件结合稀疏自编码器和转码器，旨在帮助研究人员深入理解模型内部决策过程，追踪潜在风险，并加速开发针对越狱、幻觉等安全问题的干预措施。作为迄今AI实验室发布的最大规模开源可解释性工具，其构建涉及海量数据处理，旨在推动AI安全社区对复杂模型行为的研究，提升AI系统的可靠性与安全性。
categories:
- AI研究
tags:
- 可解释性AI
- 大语言模型
- AI安全
- 开源工具
- 模型调试
draft: false
translated_at: '2026-01-05T16:35:07.395Z'
---

发布一套全新的开源语言模型可解释性工具套件
大语言模型（LLM）能够完成令人惊叹的推理任务，但其内部的决策过程在很大程度上仍然不透明。如果系统行为不符合预期，缺乏对其内部运作的可见性将使我们难以准确定位其行为的确切原因。去年，我们通过 Gemma Scope 推进了可解释性科学，这是一个旨在帮助研究人员理解 Gemma 2（我们轻量级开源模型系列）内部运作的工具包。

今天，我们发布 Gemma Scope 2：一套全面、开源的可解释性工具套件，适用于所有 Gemma 3 模型规模，从 2.7 亿到 270 亿参数。这些工具使我们能够追踪模型整个"大脑"中的潜在风险。

据我们所知，这是迄今为止人工智能实验室发布的最大规模的开源可解释性工具。构建 Gemma Scope 2 涉及存储约 110 PB 的数据，以及训练总计超过 1 万亿的参数。

随着人工智能的持续发展，我们期待人工智能研究社区使用 Gemma Scope 2 来调试模型涌现的行为，利用这些工具更好地审计和调试 AI Agent（智能体），并最终加速开发实用且稳健的安全干预措施，以应对越狱、幻觉和谄媚等问题。

我们的交互式 Gemma Scope 2 演示可供试用，由 Neuronpedia 提供支持。

可解释性研究旨在理解人工智能模型的内部运作及其学习到的算法。随着人工智能能力日益强大且日益复杂，可解释性对于构建安全可靠的人工智能至关重要。

与其前身一样，Gemma Scope 2 充当了 Gemma 系列语言模型的显微镜。通过结合稀疏自编码器（SAEs）和转码器，它使研究人员能够深入模型内部，观察模型正在"思考"什么，以及这些"思想"是如何形成并与模型行为联系起来的。这反过来使得对越狱或其他与安全相关的 AI 行为（例如模型对外表达的推理与其内部状态之间的差异）进行更丰富的研究成为可能。

虽然最初的 Gemma Scope 支持在关键安全领域进行研究，例如模型幻觉、识别模型已知的秘密以及训练更安全的模型，但 Gemma Scope 2 通过重大升级支持更具雄心的研究：

通过发布 Gemma Scope 2，我们的目标是让人工智能安全研究社区能够利用一套尖端可解释性工具推动该领域向前发展。这种新级别的访问权限对于解决仅在更大规模、更现代的 LLM 中才会出现的现实世界安全问题至关重要。

了解更多关于 Gemma Scope 的信息

---

> 本文由AI自动翻译，原文链接：[Gemma Scope 2: Helping the AI Safety Community Deepen Understanding of Complex Language Model Behavior](https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/)
> 
> 翻译时间：2026-01-05 13:05
