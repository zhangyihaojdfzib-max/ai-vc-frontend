---
title: 预测性逆动力学模型：重新思考模仿学习，以更少数据实现高效学习
title_original: Rethinking imitation learning with Predictive Inverse Dynamics Models
date: '2026-02-05'
source: Microsoft Research
source_url: https://www.microsoft.com/en-us/research/blog/rethinking-imitation-learning-with-predictive-inverse-dynamics-models/
author: ''
summary: 本文探讨了预测性逆动力学模型（PIDM）在模仿学习中的优势。与传统的行为克隆（BC）方法不同，PIDM将问题分解为两个步骤：首先预测合理的未来状态，然后推断实现该状态所需的行动。这种“先定目标，再找行动”的方式，即使预测不完美，也能显著减少当前状态下行动选择的模糊性，从而更清晰地理解专家意图。研究表明，PIDM的数据效率远高于BC，仅需后者五分之一的数据量即可达到相当性能，尤其在行为多变、目标驱动的复杂任务（如3D视频游戏）中表现突出。
categories:
- AI研究
tags:
- 模仿学习
- 预测性逆动力学模型
- 行为克隆
- 强化学习
- AI智能体
draft: false
translated_at: '2026-02-06T04:14:22.286453'
---

![Smart Replay - 显示编码器、状态预测器和策略之间流程的流程图](/images/posts/b6d5db07b798.jpg)

*   当 AI Agent（智能体）理解为何采取某个行动时，模仿学习会变得更容易。
*   预测性逆动力学模型（PIDM）预测合理的未来状态，从而在模仿学习中明确行为方向。
*   即使是不完美的预测也能减少模糊性，使当下采取何种行动更为清晰。
*   这使得 PIDM 的数据效率远高于传统方法。

模仿学习通过示例教导 AI Agent（智能体）：向智能体展示人类执行任务的记录，让它推断该做什么。最常见的方法——行为克隆（BC）——将其视为一个简单的问题：“给定环境的当前状态，专家会采取什么行动？”

在实践中，这是通过监督学习完成的，其中状态作为输入，专家行动作为输出。虽然原理简单，但 BC 通常需要大量的演示数据集来应对人类行为的自然变化，而在现实环境中收集此类数据集可能成本高昂且困难。

预测性逆动力学模型（PIDM）通过改变智能体解释人类行为的方式，为模仿学习提供了不同的视角。PIDM 不是直接将状态映射到行动，而是将问题分解为两个子问题：预测接下来应该发生什么，以及推断从当前状态过渡到预测的未来状态的适当行动。虽然 PIDM 通常优于 BC，但其表现出色的原因一直不明确，这促使我们深入研究其性能背后的机制。

在论文《预测性逆动力学何时优于行为克隆？》中，我们展示了这种两阶段方法如何使 PIDM 能够比 BC 使用少得多的演示来学习有效的策略。通过将选择过程建立在合理的未来基础上，PIDM 为推理期间选择行动提供了更清晰的基础。在实践中，这意味着即使预测不完美，PIDM 也能仅使用 BC 所需演示数量的五分之一，就达到可比的性能。

## PIDM 如何重新思考模仿

PIDM 的模仿学习方法包含两个核心要素：一个预测合理未来状态的模型，以及一个逆动力学模型（IDM），用于预测从当前状态向该未来状态过渡所需的行动。PIDM 不是问“专家会采取什么行动？”，而是有效地问“专家试图实现什么目标，以及什么行动会导致该目标？”这种转变将当前观察（例如视频帧）中的信息转化为连贯的方向感，减少了意图的模糊性，并使行动预测更容易。

播客系列

## 医学中的 AI 革命，再探

与微软的 Peter Lee 一起踏上旅程，探索 AI 如何影响医疗保健及其对医学未来的意义。

## 在 3D 游戏环境中的真实世界验证

为了在现实条件下评估 PIDM，我们在一个视觉丰富的视频游戏中，使用人类游戏演示来训练智能体。这些条件包括直接从原始视频输入操作、以每秒 30 帧的速度与复杂的 3D 环境实时交互，以及处理视觉伪影和不可预测的系统延迟。

智能体从头到尾运行，将视频帧作为输入，并持续决定按下哪个按钮以及如何移动操纵杆。该模型不依赖于手动编码的游戏变量和规则集，而是直接从视觉输入工作，使用过去的示例来预测接下来会发生什么，并选择使游戏朝该方向发展的行动。

我们在云游戏平台上运行了所有实验，这引入了额外的延迟和视觉失真。尽管存在这些挑战，PIDM 智能体始终匹配人类的游戏模式，并在各项任务中取得了高成功率，如下方视频 1 及附录中的视频 2 和 3 所示。

## PIDM 为何及何时优于 BC

当然，AI Agent（智能体）无法获知未来结果。它们只能根据可用数据生成预测，而这些预测有时是错误的。这为 PIDM 带来了一个核心权衡。

一方面，预测智能体应该前往何处可以澄清当下采取何种行动是合理的。了解预期方向有助于缩小原本模糊的选择范围。另一方面，不准确的预测偶尔会导致模型采取错误的行动。

关键的洞见在于，这些影响并不对称。虽然预测错误会带来一些风险，但减少当下的模糊性往往更为重要。我们的理论分析表明，即使预测不完美，只要预测误差保持在适度水平，PIDM 的表现就会优于 BC。如果未来状态被完美知晓，PIDM 将直接优于 BC。

在实践中，这意味着澄清意图通常比准确预测未来更重要。这种优势在 BC 难以应对的情况下最为明显：即人类行为多变，且行动由潜在目标驱动，而非屏幕上即时可见的内容。

BC 需要大量演示，因为每个示例都存在噪声，并且可以有多种解释。相比之下，PIDM 通过将行动与其旨在达到的未来状态联系起来，从而锐化了每个演示。因此，PIDM 可以从少得多的示例中学习有效的行动策略。

## 评估

为了在现实条件下测试这些想法，我们设计了一系列实验，从一个简单、可解释的 2D 环境（附录中的视频 4）开始，最终在一个复杂的 3D 视频游戏中达到高潮。我们在非常小的数据集上同时训练了 BC 和 PIDM，2D 环境中的演示数量从 1 到 50 不等，3D 视频游戏中的演示数量从 5 到 30 不等。在所有任务中，PIDM 达到高成功率所需的演示数量远少于 BC。

在 2D 环境中，BC 需要比 PIDM 多 2 到 5 倍的数据才能达到其性能水平（图 2）。在 3D 游戏中，BC 需要多 66% 的数据才能达到可比的结果（附录中的视频 5）。

![图 2. 2D 环境中的性能提升。随着训练演示数量的增加，PIDM 在所有四项任务中始终比 BC 获得更高的成功率。曲线显示平均性能，阴影部分表示 20 次实验的可变性以确保可复现性。](/images/posts/c54d3f885dcd.png)

## 要点：意图在模仿学习中至关重要

我们研究的主要信息很简单：当意图变得明确时，模仿会变得更容易。预测一个合理的未来，即使是不完美的未来，也有助于解决当下采取何种行动是合理的模糊性问题，就像在雾中驾驶时，如果司机已经知道道路的走向，就会更有信心一样。PIDM 将模仿学习从纯粹的复制转向了以目标为导向的行动。

这种方法有其局限性。如果对未来状态的预测变得过于不可靠，它们可能会误导模型对预期下一步行动的判断。在这些情况下，增加的不确定性可能会超过减少模糊性带来的好处，导致 PIDM 表现不如 BC。

但是，当预测相当准确时，将行动预测重新定义为“我如何从这里到达那里？”有助于解释为什么从小而杂乱的人类数据集中学习可以出奇地有效。在数据昂贵且演示有限的环境中，这种视角的转变可以产生有意义的差异。

## 附录：可视化与结果（视频）

### 一名玩家、一个简单的行动回放基线和一名 PIDM 智能体在《Bleeding Edge》中游戏

### 一名玩家和一名 PIDM 智能体在《Bleeding Edge》中执行复杂任务

### 2D 导航环境可视化

### 3D 环境中的 PIDM 与 BC 对比

## 认识作者

### Pallavi Choudhury

首席 RSDE 经理

### Lukas Schäfer

研究员

### Chris Lovett

首席研究软件工程经理

### Katja Hofmann

合作研究经理

### Sergio Valcarcel Macua

首席研究科学家

## 继续阅读

### 多模态强化学习与具验证能力的智能体用于AI智能体

### Magma：一个横跨数字与物理世界的多模态AI智能体基础模型

### ExACT：通过测试时计算扩展提升AI智能体的决策能力

![在IEEE CoG 2024上展示的“LLM驱动游戏叙事中的玩家驱动涌现”](/images/posts/eb434920dd41.jpg)

### 玩家、创作者与AI协作构建并扩展丰富的游戏叙事

---

> 本文由AI自动翻译，原文链接：[Rethinking imitation learning with Predictive Inverse Dynamics Models](https://www.microsoft.com/en-us/research/blog/rethinking-imitation-learning-with-predictive-inverse-dynamics-models/)
> 
> 翻译时间：2026-02-06 04:14
