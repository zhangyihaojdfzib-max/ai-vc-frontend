---
title: 加州大学圣地亚哥分校引入NVIDIA DGX B200，加速生成式AI研究
title_original: UC San Diego Lab Advances Generative AI Research With NVIDIA DGX B200
  System
date: '2025-12-17'
source: NVIDIA AI Blog
source_url: https://blogs.nvidia.com/blog/ucsd-generative-ai-research-dgx-b200/
author: Zoe Kessler
summary: 加州大学圣地亚哥分校郝人工智能实验室获得NVIDIA DGX B200系统，用于推进大语言模型推理研究。该系统将支持FastVideo视频生成模型和Lmgame基准测试等项目，并助力探索低延迟LLM服务。文章重点介绍了实验室提出的DistServe解耦推理方法，该方法通过分离预填充与解码任务来优化“有效吞吐量”，在保证用户体验的同时提升系统整体性能与效率。
categories:
- AI研究
tags:
- NVIDIA DGX B200
- 大语言模型推理
- 解耦推理
- 有效吞吐量
- 加州大学圣地亚哥分校
draft: false
---

加州大学圣地亚哥分校郝人工智能实验室研究团队——处于人工智能模型创新前沿的先锋力量——近日获得了一台NVIDIA DGX B200系统，以提升其在大语言模型推理方面的关键研究工作。

目前生产环境中的许多LLM推理平台（例如NVIDIA Dynamo）都采用了源自郝人工智能实验室的研究概念，包括DistServe。

**郝人工智能实验室如何运用DGX B200？**

随着DGX B200在计算、信息与数据科学学院的圣地亚哥超级计算机中心全面向郝人工智能实验室及更广泛的加州大学圣地亚哥分校社区开放，研究机遇变得无限广阔。

“DGX B200是NVIDIA迄今为止最强大的人工智能系统之一，这意味着其性能位居世界前列，”加州大学圣地亚哥分校哈利西奥卢数据科学研究所及计算机科学与工程系助理教授郝张表示，“它使我们能够比使用上一代硬件更快地进行原型设计和实验。”

DGX B200正在加速推进的两个郝人工智能实验室项目是FastVideo和Lmgame基准测试。

FastVideo专注于训练一系列视频生成模型，旨在仅用五秒时间，根据给定的文本提示词生成一段五秒长的视频。

FastVideo的研究阶段除了使用DGX B200系统外，还利用了NVIDIA H200 GPU。

Lmgame-bench是一个基准测试套件，通过《俄罗斯方块》和《超级马里奥兄弟》等热门在线游戏来测试LLM。用户可以一次测试一个模型，或者让两个模型相互对抗以衡量其性能。

郝人工智能实验室的其他进行中项目探索实现低延迟LLM服务的新方法，推动大语言模型向实时响应能力迈进。

“我们当前的研究利用DGX B200，基于该系统提供的出色硬件规格，探索低延迟LLM服务的新前沿，”加州大学圣地亚哥分校计算机科学博士生陈俊达表示。

**DistServe如何影响解耦式服务？**

解耦推理是一种确保大规模LLM服务引擎能够实现最优系统总吞吐量，同时将用户请求的延迟维持在可接受的低水平的方法。

解耦推理的优势在于优化LLM服务引擎中DistServe所称的“有效吞吐量”，而非单纯的“吞吐量”。

区别如下：

吞吐量通过整个系统每秒能生成的Token数量来衡量。更高的吞吐量意味着服务用户时生成每个Token的成本更低。长期以来，吞吐量是LLM服务引擎用来相互比较性能的唯一指标。

虽然吞吐量衡量的是系统的整体性能，但它与用户感知的延迟并不直接相关。如果用户要求生成Token的延迟更低，系统就必须牺牲吞吐量。

这种吞吐量与延迟之间的天然权衡，促使DistServe团队提出了一个新指标——“有效吞吐量”：即在满足用户指定的延迟目标（通常称为服务级别目标）前提下的吞吐量度量。换言之，有效吞吐量代表了在满足用户体验的同时系统的整体健康度。

DistServe表明，有效吞吐量是衡量LLM服务系统更优的指标，因为它同时考虑了成本和服务质量。有效吞吐量能带来模型的最佳效率和理想输出。

**开发者如何实现最优有效吞吐量？**

当用户在LLM系统中发出请求时，系统会接收用户输入并生成第一个Token，这称为预填充。然后，系统逐个生成大量输出Token，根据过往请求的结果预测每个Token的未来行为。这个过程称为解码。

历史上，预填充和解码运行在同一GPU上，但DistServe背后的研究人员发现，将它们拆分到不同的GPU上可以最大化有效吞吐量。

“以前，如果你把这两个任务放在一个GPU上，它们会相互竞争资源，从用户角度看可能会导致速度变慢，”陈俊达说，“现在，如果我将任务拆分到两组不同的GPU上——一组执行计算密集型的预填充，另一组执行内存密集型的解码——我们就能从根本上消除两个任务之间的干扰，使两者都运行得更快。”

这个过程被称为预填充/解码解耦，即将预填充与解码分离以获得更高的有效吞吐量。

提高有效吞吐量并采用解耦推理方法，使得工作负载能够持续扩展，同时不牺牲低延迟或高质量的模型响应。

NVIDIA Dynamo——一个旨在以最高效率和最低成本加速和扩展生成式人工智能模型的开源框架——支持扩展解耦推理。

除了这些项目，加州大学圣地亚哥分校正在开展跨部门合作（例如在医疗健康和生物学领域），以利用NVIDIA DGX B200进一步优化一系列研究项目，研究人员们持续探索人工智能平台如何加速创新。

了解更多关于NVIDIA DGX B200系统的信息。

---

> 本文由AI自动翻译，原文链接：[UC San Diego Lab Advances Generative AI Research With NVIDIA DGX B200 System](https://blogs.nvidia.com/blog/ucsd-generative-ai-research-dgx-b200/)
> 
> 翻译时间：2026-01-06 00:56
