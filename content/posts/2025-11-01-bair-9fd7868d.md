---
title: 强化学习新范式：无需时序差分，分而治之攻克长视野任务
title_original: RL without TD learning
date: '2025-11-01'
source: Berkeley AI Research (BAIR)
source_url: http://bair.berkeley.edu/blog/2025/11/01/rl-without-td-learning/
author: Ritwik Gupta
summary: 本文提出了一种基于“分而治之”范式的强化学习新方法，旨在解决传统离策略强化学习（如Q学习）中因时序差分（TD）学习导致的误差累积问题，从而更好地扩展到长视野复杂任务。文章指出，TD学习在长视野任务中面临可扩展性挑战，而分而治之方法通过将轨迹递归分割并组合价值，能将贝尔曼递归次数降至对数级，且无需调整超参数。作者介绍了一种在目标条件强化学习中的实用实现，展示了该方法的潜力。
categories:
- AI研究
tags:
- 强化学习
- 离策略学习
- 分而治之
- 时序差分学习
- 目标条件RL
draft: false
translated_at: '2026-01-04T23:53:41.566Z'
---

在这篇文章中，我将介绍一种基于"替代"范式的强化学习（RL）算法：分而治之。与传统方法不同，这种算法不基于时序差分（TD）学习（后者存在可扩展性挑战），并且能很好地扩展到长视野任务中。

我们可以基于分而治之，而非时序差分（TD）学习来进行强化学习（RL）。

**问题设定：离策略RL**

我们的问题设定是离策略RL。让我们简要回顾一下这意味着什么。

RL中有两类算法：同策略RL和离策略RL。同策略RL意味着我们只能使用当前策略收集的新数据。换句话说，每次更新策略时，我们都必须丢弃旧数据。像PPO、GRPO（以及一般的策略梯度方法）这类算法属于此类别。

离策略RL意味着我们没有这个限制：我们可以使用任何类型的数据，包括旧经验、人类演示、互联网数据等等。因此，离策略RL比同策略RL更通用、更灵活（当然也更难！）。Q学习是最著名的离策略RL算法。在数据收集成本高昂的领域（例如机器人、对话系统、医疗保健等），我们通常别无选择，只能使用离策略RL。这就是为什么它是一个如此重要的问题。

截至2025年，我认为我们已经有了一些相当不错的方案来扩展同策略RL（例如PPO、GRPO及其变体）。然而，我们仍然没有找到一种能够很好地扩展到复杂、长视野任务的"可扩展"离策略RL算法。让我简要解释一下原因。

**价值学习中的两种范式：时序差分（TD）和蒙特卡洛（MC）**

在离策略RL中，我们通常使用时序差分（TD）学习（即Q学习）来训练价值函数，使用以下贝尔曼更新规则：
\[\begin{aligned} Q(s, a) \gets r + \gamma \max_{a'} Q(s', a'), \end{aligned}\]
问题在于：下一个价值 $Q(s', a')$ 中的误差通过自举传播到当前价值 $Q(s, a)$，并且这些误差在整个视野范围内累积。这基本上是导致TD学习难以扩展到长视野任务的原因（如果你对更多细节感兴趣，可以参阅这篇文章）。

为了缓解这个问题，人们将TD学习与蒙特卡洛（MC）回报混合使用。例如，我们可以进行 $n$ 步TD学习（TD-$n$）：
\[\begin{aligned} Q(s_t, a_t) \gets \sum_{i=0}^{n-1} \gamma^i r_{t+i} + \gamma^n \max_{a'} Q(s_{t+n}, a'). \end{aligned}\]
这里，我们对前 $n$ 步使用实际的蒙特卡洛回报（来自数据集），然后对视野的其余部分使用自举价值。这样，我们可以将贝尔曼递归的次数减少 $n$ 倍，从而减少误差累积。在 $n = \infty$ 的极端情况下，我们得到纯粹的蒙特卡洛价值学习。

虽然这是一个合理的解决方案（并且通常效果不错），但它非常不令人满意。首先，它没有从根本上解决误差累积问题；它只是将贝尔曼递归的次数减少了一个常数因子（$n$）。其次，随着 $n$ 增大，我们会遇到高方差和次优性问题。因此，我们不能简单地将 $n$ 设为一个很大的值，而需要为每个任务仔细调整它。

有没有一种根本不同的方法来解决这个问题？

**"第三"范式：分而治之**

我的主张是，价值学习中的第三种范式——分而治之——可能为离策略RL提供一个理想的解决方案，能够扩展到任意长视野的任务。

分而治之能以对数方式减少贝尔曼递归的次数。

分而治之的关键思想是将一条轨迹分成两个等长的片段，并组合它们的价值来更新整条轨迹的价值。这样，我们（理论上）可以将贝尔曼递归的次数减少到对数级别（而不是线性！）。此外，它不需要像 $n$ 步TD学习那样选择一个超参数 $n$，并且不一定像 $n$ 步TD学习那样遭受高方差或次优性问题。

从概念上讲，分而治之确实具备了我们在价值学习中想要的所有优良特性。因此，我对这个高层次的想法一直感到兴奋。问题在于，直到最近，我们还不清楚如何在实践中真正实现这一点……

**一种实用算法**

在与Aditya共同领导的一项近期工作中，我们在实现和扩展这一想法方面取得了有意义的进展。具体来说，我们能够将分而治之价值学习扩展到高度复杂的任务（据我所知，这是第一个这样的工作！），至少在一类重要的RL问题——目标条件RL中。目标条件RL旨在学习一个能够从任何状态到达任何其他状态的策略。这提供了一个自然的分而治之结构。让我解释一下。

结构如下。首先假设动态是确定性的，并将两个状态 $s$ 和 $g$ 之间的最短路径距离（"时间距离"）表示为 $d^*(s, g)$。那么，它满足三角不等式：
\[\begin{aligned} d^*(s, g) \leq d^*(s, w) + d^*(w, g) \end{aligned}\]
对所有 $s, g, w \in \mathcal{S}$ 成立。

在价值方面，我们可以等价地将这个三角不等式转化为以下"传递性"贝尔曼更新规则：
\[\begin{aligned} V(s, g) \gets \begin{cases} \gamma^0 & \text{if } s = g, \\\\ \gamma^1 & \text{if } (s, g) \in \mathcal{E}, \\\\ \max_{w \in \mathcal{S}} V(s, w)V(w, g) & \text{otherwise} \end{cases} \end{aligned}\]
其中 $\mathcal{E}$ 是环境转移图中的边集，$V$ 是与稀疏奖励 $r(s, g) = 1(s = g)$ 相关联的价值函数。直观地说，这意味着我们可以使用两个"更小"的价值：$V(s, w)$ 和 $V(w, g)$ 来更新 $V(s, g)$ 的价值，前提是 $w$ 是最短路径上的最优"中点"（子目标）。这正是我们一直在寻找的分而治之价值更新规则！

**问题**

然而，这里有一个问题。问题在于，在实践中如何选择最优子目标 $w$ 并不明确。在表格设置中，我们可以简单地枚举所有状态来找到最优的 $w$（这本质上是Floyd-Warshall最短路径算法）。但在具有大状态空间的连续环境中，我们无法做到这一点。基本上，这就是为什么以前的工作难以扩展分而治之价值学习，尽管这个想法已经存在了几十年（事实上，它可以追溯到Kaelbling（1993）在目标条件RL中的最早工作——关于相关工作的进一步讨论，请参阅我们的论文）。我们工作的主要贡献就是为这个问题提供了一个实用的解决方案。

**解决方案**

这是我们的关键思想：我们将 $w$ 的搜索空间限制在数据集中出现的状态，特别是那些在数据集轨迹中位于 $s$ 和 $g$ 之间的状态。此外，我们不搜索最优的 $\text{argmax}_w$，而是使用期望回归计算一个"软" $\text{argmax}$。也就是说，我们最小化以下损失：
\[\begin{aligned} \mathbb{E}\left[\ell^2_\kappa (V(s_i, s_j) - \bar{V}(s_i, s_k) \bar{V}(s_k, s_j))\right], \end{aligned}\]
其中 $\bar{V}$ 是目标价值网络，$\ell^2_\kappa$ 是具有期望值 $\kappa$ 的期望损失，期望是对随机采样的数据集轨迹中所有满足 $i \leq k \leq j$ 的 $(s_i, s_k, s_j)$ 三元组计算的。

这有两个好处。首先，我们不需要在整个状态空间上进行搜索。其次，我们通过使用"更软"的期望回归来防止 $\max$ 算子导致的价值高估。我们将此算法称为传递性RL（TRL）。更多细节和进一步讨论，请查看我们的论文！

**效果好吗？**

humanoidmaze
puzzle
为了验证我们的方法是否能很好地扩展到复杂任务，我们直接在OGBench（一个离线目标条件RL基准测试）中一些最具挑战性的任务上评估了TRL。我们主要使用了humanoidmaze和puzzle任务的最难版本，这些任务具有大型的10亿规模数据集。

这些任务极具挑战性：它们需要在多达3000个环境步骤中执行组合复杂的技能。
TRL在极具挑战性的长视野任务上取得了最佳性能。
结果非常令人兴奋！与不同类别（TD、MC、准度量学习等）的众多强基线相比，TRL在大多数任务上都取得了最佳性能。
TRL与经过单独调优的最佳TD-$n$方法表现相当，且无需设置$\boldsymbol{n}$参数。
这是我最喜欢的图表。我们将TRL与不同$n$值（从$1$（纯TD）到$\infty$（纯MC））的$n$步TD学习进行了比较。结果非常出色。TRL在所有任务上都与最佳TD-$n$方法表现相当，且无需设置$\boldsymbol{n}$参数！这正是我们希望从分治范式中获得的效果。通过递归地将轨迹分割成更小的片段，它能够自然地处理长视野问题，而无需任意选择轨迹块的长度。
论文中包含了大量额外的实验、分析和消融研究。如果您感兴趣，请查阅我们的论文！

接下来是什么？
在这篇文章中，我分享了我们新的分治价值学习算法——Transitive RL（TRL）——的一些有前景的结果。这仅仅是旅程的开始。还有许多开放问题和令人兴奋的方向值得探索：
-
或许最重要的问题是如何将TRL扩展到目标条件RL之外的常规、基于奖励的RL任务中。常规RL是否也存在类似的可供我们利用的分治结构？我对此相当乐观，因为至少在理论上，任何基于奖励的RL任务都可以转换为目标条件任务（参见本书第40页）。
-
另一个重要挑战是处理随机环境。当前版本的TRL假设环境动态是确定性的，但许多现实世界环境是随机的，这主要源于部分可观测性。对此，"随机"三角不等式可能提供一些线索。
-
从实践角度看，我认为TRL仍有很大的改进空间。例如，我们可以找到更好的方法来选择子目标候选（不局限于同一轨迹中的点），进一步减少超参数，进一步稳定训练，并使算法更加简化。

总的来说，我对分治范式的潜力感到非常兴奋。我仍然认为，RL（甚至机器学习）中最重要的问题之一是找到一个可扩展的离策略RL算法。我不知道最终的解决方案会是什么样子，但我确实认为分治，或者广义上的递归决策，是通往这一圣杯的最有力候选方案之一（顺便提一下，我认为其他有力的竞争者是（1）基于模型的RL和（2）带有一些"神奇"技巧的TD学习）。事实上，其他领域最近的几项工作已经展示了递归和分治策略的前景，例如快捷模型、对数线性注意力、递归语言模型（当然，还有经典算法如快速排序、线段树、FFT等）。我希望在不久的将来能看到可扩展离策略RL领域出现更多令人兴奋的进展！

致谢
我要感谢Kevin和Sergey对本文提出的宝贵反馈。
本文最初发表于Seohong Park的博客。

---

> 本文由AI自动翻译，原文链接：[RL without TD learning](http://bair.berkeley.edu/blog/2025/11/01/rl-without-td-learning/)
> 
> 翻译时间：2026-01-04 23:53
