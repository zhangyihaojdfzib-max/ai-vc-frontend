---
title: Vercel如何打造高效代码生成智能体v0
title_original: How we made v0 an effective coding agent  - Vercel
date: '2026-01-07'
source: Vercel Blog
source_url: https://vercel.com/blog/how-we-made-v0-an-effective-coding-agent
author: ''
summary: 本文介绍了Vercel如何通过复合模型管道提升v0代码生成智能体的成功率。核心在于三个关键技术：动态系统提示词，通过向量匹配和文件系统示例注入最新知识，避免模型依赖过时信息；LLM
  Suspense流式处理层，在输出过程中实时修正错误引用和格式问题；以及自动修复器，处理跨文件或需要AST分析的复杂错误。这些措施将大语言模型的代码生成错误率显著降低，实现了两位数的成功率提升。
categories:
- AI产品
tags:
- 代码生成
- 智能体
- Vercel
- 大语言模型
- AI工程化
draft: false
translated_at: '2026-01-08T04:45:46.829034'
---

去年我们推出了v0复合模型家族，并描述了v0模型如何在多步骤智能体管道中运行。该管道中有三个部分对可靠性产生了最大影响。它们分别是动态系统提示词、我们称为“LLM Suspense”的流式处理层，以及一组在模型完成流式输出后（或同时！）运行的确定性和模型驱动的自动修复器。

我们优化的主要指标是生成成功率。成功的生成是指在v0预览中产生一个可工作的网站，而不是错误或空白屏幕。但问题是，孤立运行的大语言模型在大规模生成代码时会遇到各种问题。

根据我们的经验，大语言模型生成的代码出错率可能高达10%。我们的复合管道能够在大语言模型流式输出时实时检测并修复其中许多错误。这可以将成功率提高两位数。

### 动态系统提示词

你产品的护城河不能仅仅是你的系统提示词。然而，这并不能改变一个事实：系统提示词是你引导模型最强大的工具。

例如，以AI SDK的使用为例。AI SDK会定期发布主要和次要版本。模型通常依赖于过时的内部知识（它们的“训练截止日期”），但我们希望v0使用最新版本。这可能导致错误，例如使用旧版本SDK的API。这些错误直接降低了我们的成功率。

许多智能体依赖网络搜索工具来获取新信息。网络搜索很好（v0也使用它），但它有其缺陷。你可能会得到旧的搜索结果，比如过时的博客文章和文档。此外，许多智能体会让一个较小的模型来总结网络搜索的结果，这反过来又会在小模型和主模型之间形成一场糟糕的“传话游戏”。小模型可能会产生幻觉、错误引用或遗漏重要信息。

我们不依赖网络搜索，而是使用嵌入/向量和关键词匹配来检测与AI相关的意图。当一条消息被标记为与AI相关且与AI SDK相关时，我们会向提示词中注入描述目标SDK版本的知识。我们保持这种注入的一致性，以最大化提示词缓存的命中率并保持较低的Token使用量。

除了文本注入，我们还与AI SDK团队合作，在v0智能体的只读文件系统中提供了示例。这些是精心策划的目录，包含专为大语言模型使用而设计的代码示例。当v0决定使用SDK时，它可以搜索这些目录以查找相关模式，例如图像生成、路由或集成网络搜索工具。

这些动态系统提示词用于各种主题，包括前端框架和集成。

### LLM Suspense

LLM Suspense是一个在文本流向用户时对其进行操作的框架。这包括诸如查找和替换以清理不正确导入等操作，但也可以变得更加复杂。

两个例子展示了它提供的灵活性：

一个简单的例子是替换大语言模型经常引用的长字符串。例如，当用户上传附件时，我们给v0一个blob存储URL。该URL可能非常长（数百个字符），这会消耗数十个Token并影响性能。

在我们调用大语言模型之前，我们用较短的版本替换长URL，这些短版本在大语言模型完成响应后会被转换为正确的URL。这意味着大语言模型读取和写入的Token更少，为用户节省了金钱和时间。

在生产环境中，这些简单的规则处理了引用、格式化和混合导入块中的变化。因为这是在流式传输过程中发生的，用户永远不会看到中间的错误状态。

Suspense也可以处理更复杂的情况。默认情况下，v0使用lucide-react图标库。它每周更新，添加和删除图标。这意味着大语言模型经常会引用不再存在或从未存在过的图标。

为了确定性地纠正这一点，我们：

1.  将每个图标名称嵌入到向量数据库中。
2.  在运行时分析lucide-react的实际导出。
3.  当图标可用时，直接传递正确的图标。
4.  当图标不存在时，运行嵌入/向量搜索以找到最接近的匹配项。
5.  在流式传输期间重写导入。

例如，对“Vercel logo icon”的请求可能会产生：

```
1import { VercelLogo } from ‘lucide-react’
```

LLM Suspense会将其替换为：

```
1import { Triangle as VercelLogo } from ‘lucide-react’
```

这个过程在100毫秒内完成，并且不需要进一步的模型调用。

### 自动修复器

有时，存在我们的系统提示词和LLM Suspense无法修复的问题。这些问题通常涉及跨多个文件的更改或需要分析抽象语法树（AST）。

对于这些情况，我们在流式传输后收集错误，并将其传递给我们的自动修复器。这些修复器包括确定性修复和一个在大量真实生成数据上训练的、小型、快速、微调的模型。

一些自动修复的例子包括：

-   来自@tanstack/react-query的useQuery和useMutation需要被包裹在QueryClientProvider中。我们解析AST以检查它们是否被包裹，但自动修复模型决定在哪里添加它。
-   通过扫描生成的代码并确定性地更新文件，来补全package.json中缺失的依赖项。
-   修复在Suspense转换过程中遗漏的常见JSX或TypeScript错误。

这些修复在250毫秒内运行，并且仅在需要时运行，使我们能够在保持低延迟的同时提高可靠性。

## 整合起来

将动态系统提示词、LLM Suspense和自动修复器结合起来，我们得到了一个管道，它能够以比独立模型更高的比率产生稳定、可运行的生成结果。管道的每个部分都针对特定的故障模式，它们共同显著提高了用户在v0中首次尝试就看到渲染网站的可能性。

> 本文由AI自动翻译，原文链接：[How we made v0 an effective coding agent  - Vercel](https://vercel.com/blog/how-we-made-v0-an-effective-coding-agent)
> 
> 翻译时间：2026-01-08 04:45
