---
title: FACTS基准套件发布：系统性评估大模型事实准确性的新方法
title_original: 'FACTS Benchmark Suite: a new way to systematically evaluate LLMs
  factuality'
date: '2025-12-09'
source: Google DeepMind
source_url: https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/
author: The FACTS team
summary: 本文介绍了谷歌与Kaggle合作推出的FACTS基准测试套件，旨在系统性评估大语言模型的事实准确性。该套件包含参数化、搜索、多模态及更新后的Grounding
  v2四个基准，共3513个示例，分别测试模型在不借助外部工具、使用网络搜索、结合图像理解及依据给定上下文生成准确回答的能力。评估结果显示，Gemini 3 Pro以68.8%的总分领先，但所有模型准确率均低于70%，表明事实性仍是重要挑战。该工作旨在推动LLM事实性研究的深入，提升模型可靠性。
categories:
- AI研究
tags:
- 大语言模型
- 事实性评估
- 基准测试
- Gemini
- AI研究
draft: false
translated_at: '2026-01-05T17:11:11.030Z'
---

大语言模型（LLM）正日益成为跨多种用例的信息传递主要来源，因此确保其回答的事实准确性至关重要。
为了持续改进模型在这一全行业共同面临的挑战上的表现，我们必须更好地理解模型难以提供准确回答的用例类型，并在这些领域更有效地衡量事实性表现。
今天，我们与Kaggle合作推出FACTS基准测试套件。它扩展了我们之前开发FACTS Grounding Benchmark的工作，新增了三个事实性基准测试，包括：
我们同时使用Grounding Benchmark - v2更新了原始的FACTS Grounding Benchmark，这是一个扩展的基准测试，用于评估模型根据给定提示词上下文提供有依据回答的能力。
每个基准测试都经过精心设计，总共产生了3,513个示例，我们于今日将其公开。与我们之前的发布类似，我们遵循行业标准实践，保留一个评估集作为私有集。FACTS基准测试套件分数（或称FACTS分数）计算为四个基准测试中公共集和私有集的平均准确率。Kaggle将负责管理FACTS基准测试套件，包括持有私有集、在基准测试上评估领先的LLM，并在公共排行榜上托管结果。关于FACTS评估方法的更多细节可在我们的技术报告中找到。

FACTS参数化基准测试评估模型在不借助网络搜索等外部工具的情况下，准确回答事实性问题的能力。该基准中的所有问题均为“知识问答风格”问题，源于用户兴趣，可通过维基百科（LLM预训练的标准来源）回答。该基准最终包含一个1052项的公共集和一个1052项的私有集。
公共集中的一个典型提示词会要求模型回答一个冷门主题的简单问题，例如：“谁在《洛克福德档案》主题曲中演奏了口琴？”

相比之下，FACTS搜索基准测试评估模型使用网络搜索工具回答问题的能力。该基准设计得对LLM具有挑战性，即使其能访问网络，也常常需要顺序检索多个事实才能回答单个查询。所有模型都使用相同的网络搜索工具，以确保在测试模型能力时，不受自定义网络检索设置这一混杂因素的影响。FACTS搜索基准包含一个890项的公共集和一个994项的私有集。
公共集中的以下示例被纳入，是因为它需要从多个网页检索信息：“在1960年夏季奥运会上击败Vazik Kazarian的英国拳击手、在同一届奥运会男子轻中量级比赛中参赛的摩洛哥拳击手，以及参加了1960年和1964年两届夏季奥运会的丹麦拳击手，这三人的出生年份之和是多少？”

FACTS多模态基准测试评估模型根据基于图像的问题生成事实准确文本的能力，这是现代多模态系统的关键能力。
此任务需要整合视觉基础，即模型利用其内部或“参数化”的世界知识，准确解释并关联视觉输入信息的能力。评估框架旨在确保回答既正确又提供所有必要信息以保持完整。该基准包含一个711项的公共集和一个811项的私有集。
例如，多模态基准公共集中的以下图像附有提示词：“这种动物属于哪个属？”

我们在FACTS基准测试套件（包括更新后的FACTS Grounding v2）上评估了领先的LLM。
下表列出了15个领先模型及其总体FACTS分数（随后是其在四个独立基准测试：Grounding、Multimodal、Parametric和Search中的分数细分）。
Gemini 3 Pro在整体性能上领先，FACTS分数为68.8%。特别是，我们看到从Gemini 2.5 Pro到Gemini 3 Pro在搜索和参数化部分有显著改进，在FACTS搜索上的错误率降低了55%，在FACTS参数化上降低了35%。总体而言，FACTS多模态的得分最低。所有评估模型的总体准确率均低于70%，表明未来仍有相当大的提升空间。

除了FACTS基准测试套件，Gemini在事实性方面的改进也体现在另一个事实性基准测试SimpleQA Verified上，其准确率从Gemini 2.5 Pro的54.5%提升至Gemini 3 Pro的72.1%。SimpleQA Verified测试LLM在简短回答上的参数化知识。

虽然LLM的事实性仍然是一个持续研究的领域，但FACTS基准测试套件和Gemini 3 Pro的结果代表了谷歌长期以来致力于让信息普遍可及且有用的承诺。我们希望这项工作能鼓励对LLM事实性进行更深入的研究，从而为依赖它们的人们带来更好、更准确的模型和产品。

> 本文由AI自动翻译，原文链接：[FACTS Benchmark Suite: a new way to systematically evaluate LLMs factuality](https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/)
> 
> 翻译时间：2026-01-05 17:11
