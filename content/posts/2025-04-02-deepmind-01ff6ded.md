---
title: 负责任地迈向通用人工智能之路：安全与保障的技术方法
title_original: Taking a responsible path to AGI
date: '2025-04-02'
source: Google DeepMind
source_url: https://deepmind.google/blog/taking-a-responsible-path-to-agi/
author: ''
summary: 本文探讨了通往通用人工智能（AGI）的负责任路径，强调在追求AGI巨大潜力的同时，必须优先考虑安全准备和风险评估。文章概述了AGI可能带来的社会效益，并重点分析了滥用、错位、事故和结构性风险四大核心风险领域。作者详细介绍了应对滥用和错位挑战的技术策略，包括限制危险能力访问、开发安全机制、进行威胁建模以及通过放大监督和稳健训练来确保AI系统与人类价值观对齐。文章旨在为行业协作与安全发展AGI提供框架和起点。
categories:
- AI研究
tags:
- 通用人工智能
- AI安全
- AI对齐
- 风险评估
- 技术伦理
draft: false
translated_at: '2026-02-04T04:13:08.071850'
---

# 负责任地迈向通用人工智能之路

安卡·德拉甘、罗欣·沙阿、福尔·弗林、肖恩·莱格

![](/images/posts/75f752ae19c4.jpg)

我们正在探索通用人工智能（AGI）的前沿，优先考虑准备工作、主动风险评估，并与更广泛的人工智能社区合作。

通用人工智能（AGI）——一种至少在大多数认知任务上与人能力相当的人工智能——可能在未来的几年内出现。

结合智能体（Agent）能力，AGI可以极大地增强人工智能的理解、推理、规划和自主执行行动的能力。这样的技术进步将为社会提供宝贵的工具，以应对关键的全球性挑战，包括药物发现、经济增长和气候变化。

这意味着我们可以期待数十亿人获得切实的利益。例如，通过实现更快、更准确的医疗诊断，它可能彻底改变医疗保健。通过提供个性化的学习体验，它可以使教育更具可及性和吸引力。通过增强信息处理能力，AGI可以帮助降低创新和创造力的门槛。通过普及先进工具和知识的获取，它可以使小型组织能够应对以前只有资金雄厚的大型机构才能解决的复杂挑战。

## 探索通往AGI之路

我们对AGI的潜力持乐观态度。它有能力改变我们的世界，成为生活许多领域进步的催化剂。但对于如此强大的任何技术而言，即使是很小的伤害可能性也必须认真对待并加以预防，这一点至关重要。

缓解AGI安全挑战需要主动的规划、准备和协作。此前，我们在《AGI级别》框架论文中介绍了我们对待AGI的方法，该论文为分类高级人工智能系统的能力、理解和比较其性能、评估潜在风险以及衡量向更通用、更强大AI的进展提供了视角。

今天，在我们迈向这项变革性技术的道路上，我们分享关于AGI安全与保障的观点。这篇题为《技术性AGI安全与保障方法》的新论文，是与更广泛行业就如何监控AGI进展、确保其安全负责任地发展进行重要对话的起点。

在论文中，我们详细阐述了如何采取系统全面的方法来应对AGI安全，探索了四个主要风险领域：滥用、错位、事故和结构性风险，并更深入地关注滥用和错位。

![风险领域概览](/images/posts/fc8a4b284cd7.jpg)

风险领域概览

## 理解并应对滥用的可能性

滥用发生在人类故意将AI系统用于有害目的时。

对当前危害和缓解措施的深入了解，持续增强了我们对长期严重危害及其预防方式的理解。

例如，当前生成式AI的滥用包括制作有害内容或传播不准确信息。未来，先进的AI系统可能更有能力以可能导致意外社会后果的方式影响公众信念和行为。

此类危害的潜在严重性要求采取主动的安全与保障措施。

正如我们在论文中详述的，我们战略的一个关键要素是识别并限制对可能被滥用的危险能力的访问，包括那些能够实现网络攻击的能力。

我们正在探索多种缓解措施来防止高级AI的滥用。这包括复杂的安全机制，可以防止恶意行为者获取允许他们绕过我们安全护栏的模型权重原始访问权限；限制模型部署时潜在滥用的缓解措施；以及有助于识别需要加强安全的能力阈值的威胁建模研究。此外，我们最近推出的网络安全评估框架将这项工作更进一步，以帮助缓解AI驱动的威胁。

即使在今天，我们也定期评估我们最先进的模型（如Gemini）的潜在危险能力。我们的前沿安全框架更深入地探讨了我们如何评估能力并采取缓解措施，包括针对网络安全和生物安全风险。

## 错位的挑战

为了让AGI真正补充人类能力，它必须与人类价值观对齐。错位发生在AI系统追求的目标与人类意图不同时。

我们之前通过规范博弈的例子展示了错位如何产生，即AI找到一种实现其目标的方法，但并非按照指示它的人类所期望的方式，以及目标错误泛化。

例如，一个被要求预订电影票的AI系统可能会决定入侵票务系统以获取已被占用的座位——要求它买票的人可能不会考虑这一点。

我们还在对欺骗性对齐的风险进行广泛研究，即AI系统意识到其目标与人类指令不一致，并故意试图绕过人类为防止其采取错位行动而设置的安全措施的风险。

## 应对错位

我们的目标是训练先进的AI系统追求正确的目标，使它们能够准确地遵循人类指令，防止AI使用可能不道德的捷径来实现其目标。

我们通过放大监督来实现这一点，即能够判断AI的答案在实现该目标方面是好是坏。虽然这在目前相对容易，但当AI具备高级能力时，可能会变得具有挑战性。

例如，即使是围棋专家在AlphaGo首次下出第37手（一个被使用的概率只有万分之一的一步棋）时，也没有意识到它有多好。

为了应对这一挑战，我们利用AI系统本身来帮助我们对其答案提供反馈，例如在辩论中。

一旦我们能判断答案的好坏，我们就可以利用这一点来构建一个安全且对齐的AI系统。这里的一个挑战是找出要训练AI系统的问题或实例。通过稳健训练、不确定性估计等方面的工作，我们可以覆盖AI系统在现实场景中将遇到的一系列情况，从而创建可信赖的AI。

通过有效的监控和既定的计算机安全措施，我们的目标是减轻如果我们的AI系统确实追求错位目标可能造成的危害。

监控涉及使用一个称为监控器的AI系统来检测与我们的目标不一致的行为。重要的是，监控器要知道它何时不确定某个行为是否安全。当它不确定时，它应该要么拒绝该行为，要么标记该行为以供进一步审查。

## 实现透明度

如果AI决策变得更加透明，所有这些都会变得更容易。我们在可解释性方面进行了广泛的研究，旨在提高这种透明度。

为了进一步促进这一点，我们正在设计更容易理解的AI系统。

例如，我们对非近视性批准的近视优化（MONA）的研究旨在确保AI系统所做的任何长期规划对人类来说仍然是可理解的。随着技术的进步，这一点尤为重要。我们在MONA上的工作是首次证明了在LLM（大语言模型）中进行短期优化的安全益处。

## 构建AGI准备就绪的生态系统

由谷歌DeepMind联合创始人兼首席AGI科学家Shane Legg领导的AGI安全委员会（ASC）负责分析AGI风险与最佳实践，并提出安全措施建议。ASC与由首席运营官Lila Ibrahim和责任事务高级总监Helen King共同主持的内部审查机构——责任与安全委员会紧密合作，依据我们的AI原则评估AGI研究、项目与合作，并就最具影响力的工作为研究和产品团队提供建议与协作。

我们在AGI安全方面的工作，与我们在责任和安全实践及研究上的深度和广度相辅相成，这些研究涵盖有害内容、偏见和透明度等广泛议题。我们持续借鉴在智能体（Agent）安全领域获得的经验（例如在关键行动中引入人工核查的"人在回路"原则），以此指导我们负责任地构建AGI。

在外部，我们致力于促进与专家、行业、政府、非营利组织和民间社会机构的合作，并以审慎的态度推进AGI发展。

例如，我们正与包括Apollo和Redwood Research在内的非营利性AI安全研究组织合作，他们为我们最新版《前沿安全框架》中专门的"错位风险"章节提供了建议。

通过与全球政策相关方的持续对话，我们希望能为关键前沿安全议题的国际共识做出贡献，包括如何最好地预测和防范新型风险。

我们的努力包括通过"前沿模型论坛"等组织与行业伙伴分享和制定最佳实践，并与AI研究所在安全测试方面开展宝贵合作。我们坚信，协调一致的国际治理方法对于确保社会从先进AI系统中受益至关重要。

对AI研究人员和专家进行AGI安全教育，是为其发展奠定坚实基础的根本。为此，我们为关注此领域的学生、研究人员和专业人士推出了全新的AGI安全课程。

最终，我们的AGI安全与保障方案是应对诸多未解挑战的重要路线图。我们期待与更广泛的AI研究界合作，负责任地推进AGI发展，助力全人类释放这项技术的巨大潜力。

### 更新《前沿安全框架》

![](/images/posts/5dd6d473bd30.jpg)

### 评估先进AI的潜在网络安全威胁

![](/images/posts/00fc25d05945.jpg)

---

> 本文由AI自动翻译，原文链接：[Taking a responsible path to AGI](https://deepmind.google/blog/taking-a-responsible-path-to-agi/)
> 
> 翻译时间：2026-02-04 04:13
