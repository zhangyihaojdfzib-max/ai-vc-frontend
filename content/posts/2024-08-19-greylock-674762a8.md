---
title: AI的人类因素：安全、伦理与未来展望
title_original: AI's Human Factor | Greylock
date: '2024-08-19'
source: Greylock Partners
source_url: https://greylock.com/reid-hoffman/ais-human-factor/
author: ''
summary: 本文记录了Greylock合伙人里德·霍夫曼对斯坦福HAI联合主任李飞飞和OpenAI CTO米拉·穆拉蒂的访谈。双方探讨了AI领域的最新进展，特别是如何将伦理与安全考量融入GPT-3、DALL-E等系统的开发过程。李飞飞分享了其团队在构建基于真实人类活动的机器人任务基准测试集上的工作，穆拉蒂则阐述了OpenAI通过可控部署（如API）来理解和降低AI风险的实际策略。文章强调，在追求通用人工智能系统的同时，研究人员、行业领袖与政策制定者需协作建立全面的安全防护措施。
categories:
- AI研究
tags:
- 人工智能安全
- AI伦理
- 李飞飞
- OpenAI
- 人机交互
draft: false
translated_at: '2026-01-12T04:42:53.386040'
---

## 人工智能越接近人类，我们就越了解大脑的实际运作方式。通过这一探索过程，研究人员正在寻找将安全性和道德影响纳入考量的人工智能设计方法。

Greylock普通合伙人里德·霍夫曼采访了斯坦福大学以人为本人工智能研究所（HAI）联合主任、计算机科学教授李飞飞博士，以及OpenAI首席技术官米拉·穆拉蒂。在本次访谈中，他们探讨了该领域的最新进展；技术专家如何带着伦理考量训练GPT-3和DALL-E等复杂AI工具的过程；以及研究人员、行业领袖和政策制定者之间合作建立全面防护措施的必要性。

"我们正试图构建能够以类似人类方式理解世界的通用系统，使其拥有稳健的世界概念，"穆拉蒂表示，其组织的使命是确保人工智能的开发和部署方式能造福全人类。

随着人工智能的发展，这项任务变得更具挑战性。AI能力的增强伴随着复杂性的提升，研究人员和企业家们不断发现并定义需要解决的新安全问题。

"安全就像健康这个词：每个人都想要，但很难准确定义，"李博士表示，她去年在HAI成立伦理与社会审查委员会后不久也曾与霍夫曼交流。"人工智能并非单一事物。设计AI系统实际上是分阶段的工作决策，我们相信在AI开发的每个阶段都需要注入伦理和以人为本的价值观。"

本次访谈发生在Greylock"智能未来"活动期间，这场为期一天的峰会汇聚了人工智能领域的专家和企业家。您可以通过[此处链接](YouTube channel here)观看本次访谈视频，或通过下方链接及任意播客平台收听讨论。

虽然我们将重点讨论安全问题——这在当前基础模型时代尤为重要，因为李飞飞和米拉都是成就卓著的技术专家，其工作远超出安全范畴；她们都在创造非凡成果并作出了历史性贡献——但我想先从一个更宏观的问题开始：当前AI领域最令您兴奋的是什么？飞飞，从您开始，接着是米拉，然后我们再深入探讨安全议题。

**李飞飞：** 令人兴奋的事物很多，但我想谈谈当前正在思考的课题。我们即将完成一篇论文的评审流程，这篇论文将重新定义机器人技术的北极星目标。

当前从事机器人学习研究的感觉，有点像当年构建图像数据库的时期——我们真正在畅想如何实现机器人技术的梦想。这篇论文（目前正处于复审阶段）将建立一个包含1000个机器人任务的基准测试集，这些任务灵感来源于真实人类活动，其规模在机器人研究领域前所未见。

**霍夫曼：** 能否提前透露一两个最有趣的任务？

**李飞飞：** 当然可以。实际上，所有1000个任务都源自美国劳工局的时间使用调查及欧洲相关政府机构数据，我们研究了人类的日常活动，并对人们希望机器人提供哪些帮助进行了大量调研。例如，多数人显然不希望机器人拆圣诞礼物（我们仍想亲手完成），但几乎所有人都希望机器人清洁马桶——这项任务排名很高。为孩子准备午餐也排名靠前。

我们关注的是真实人类活动，而非机器人研究中常见的玩具示例。这将涵盖1000项真实任务。

**米拉·穆拉蒂：** 最令我兴奋的是探索当前驱动AI领域发展的范式能走多远——即大规模神经网络、海量数据与巨大算力的结合。过去几年，这个公式推动了AI研究的巨大进展，我们在GPT-3、Codex和初代DALL-E中都见证了这一点。

从OpenAI的视角，我们正试图构建能以类似人类方式理解世界的通用系统；即拥有稳健世界概念的系统。当AI系统看到牛油果或读到"牛油果"这个词时，触发的概念应当完全一致。令人兴奋的是，我们已经实现了具备语言能力和视觉概念理解的系统，并将继续推进这一范式。

**霍夫曼：** 现在转向安全议题，因为两位所在的机构以不同方式成为AI安全领域最重要的行业引领者。

米拉，这次从您开始如何？请谈谈OpenAI作为基于501(c)(3)条款构建的新型组织，如何定义安全性，如何在自身行动中建立规范并推动行业共同发展。

**穆拉蒂：** 如您所知，我们的使命是构建通用系统并探索如何有益地部署到世界。"有益"这个词看似简单，但如何稳健实现却是巨大挑战。预测未来本就困难，预测这些系统可能产生有害偏见或其他难以想象的风险更是难上加难。但至少我们可以尽可能理解、收集知识并保持选择开放——这就是OpenAI的策略。我们尝试以可控方式持续部署系统，这意味着通过API。GPT-3最初通过API向小规模用户部署，随着对风险管控能力的理解加深，才逐步扩大访问范围。

我认为这确实困难，而这正是我们决定部署GPT-3的部分原因。因为在实验室环境中，基于现有认知可能会判断某种风险最突出；但当模型接触真实世界时，这种判断会经受检验。我们多次在部署后发现判断失误，最突出的风险其实是其他方面。

例如对于GPT-3，我们曾确信虚假信息是最重要的风险（这确实很重要）；但实践中发现垃圾信息才是需要重点关注的风险，DALL-E也出现类似情况。因此我认为让这些模型接触真实用户和现实世界至关重要，这样才能理解摩擦点和局限性，并迭代构建缓解措施。我们构建的缓解方案可能无法一劳永逸，但这是起点，能为我们指明前进方向。

但与此同时，我们需要思考，随着模型能力越来越强，复杂性将如何增加。例如，对于语言模型，目前我们通过人工监督敏感用例的模型输出，但这无法随着更强大、更先进的模型而扩展。因此，我们必须开发新技术来帮助人类评估这些模型的输出。OpenAI 一直在与其他语言模型开发者合作，协调制定一些标准实践，以探索如何安全部署语言模型。

RH：李飞飞，同样的问题，但与 OpenAI 不同，从以人为本人工智能研究所、大学与行业及政府的角色，以及你们正在进行的工作角度，你怎么看？

FFL：很好的问题，Reid。安全就像健康这个词一样：每个人都想要，但很难定义。作为斯坦福以人为本人工智能研究所（我们称之为斯坦福 HAI）的联合主任，我们深入思考我们究竟希望 AI 和未来的 AI 成为什么样子，最终归结到“以人为本”这个词。我们专注于将以人为本的理念注入 AI 研究、开发、教育和政策工作的每一个阶段。显然，我们是一所大学，不部署产品，但我们希望我们激发的应用能对下游的产品化产生影响。

因此，AI 并非单一事物。设计 AI 系统实际上是分阶段的工作决策，我们相信在 AI 开发的每个阶段，都需要注入伦理和以人为本的价值观。

最简单的说法是，我们如何定义一个问题？例如，你的目标是在不考虑所有社会影响的情况下取代人类，还是增强人类能力？在你写第一行代码之前，你已经在思考人类价值观了。数据从何而来？如何收集？如何确保数据完整性？如何标注？这里有一大堆……从公平性、隐私性到一大堆问题和考量。然后是算法本身，它安全吗？可靠吗？是否存在偏见？接着是使用算法进行决策、推理、涉及人类。它是辅助人类还是为人类提供信息？

AI 开发的每个阶段都需要考虑人类因素，斯坦福 HAI 正努力将这一过程具体化，这甚至不是一个副产品。这个过程的核心产品是我们教育的人——学生、本科生、研究生。当他们经过在斯坦福 HAI 课程和实验室工作一年的学习后，会成为理解 AI 以人为本理念的技术专家、商业领袖或政策思考者。

“安全就像健康这个词一样：每个人都想要，但很难定义。”

RH：现在让我们更深入地探讨每个具体组织。对于 DALL-E 和 GPT-3，API 的前端部分是为了了解潜在的风险流是什么样的，以便你们可以开始训练模型。你们如何利用这些信息迭代出超越人类安全标准的模型？又如何以此帮助引领行业内的安全思考？

MM：以 GPT-3 为例，最初我们开放了我们认为已有适当缓解措施的用例访问权限。这意味着在最初几天，允许的用例主要围绕搜索和分类，但对于开放式生成我们并不太放心。因此，我们与来自不同领域的行业专家以及其他研究人员合作，对模型进行了更深入的红队测试。我们还与其他可信用户合作，以了解模型可能无法满足用户期望的各种方式，这样的方式有很多。在此基础上，我们尝试从模型角度以及部署后的工具层面构建缓解措施。

从模型角度看，GPT-3 以及其他生成式语言模型的一个核心问题是，模型会编造内容，并且在它不擅长特定主题或不知道答案时不会承认，这显然是个问题。在其他方面，它也可能用答案误导你。

因此，我们想弄清楚“如何让模型更稳健、更可靠？”，我们利用从 API 用户那里获得的反馈，并采用人类反馈强化学习（RLHF）来创建 InstructGPT 模型系列。这些模型可靠得多，并且能真正执行操作者希望它们做的事情。因此，它们不仅更有帮助、更安全，而且实际上更有用。

如今 API 中已有默认模型，我们的用户更喜欢它们而不是基础模型。这是我们利用部署使模型更安全、更可靠、更有效的一种方式。这非常有趣，因为这是安全首次从理论领域进入实践领域，并与能力相结合；通过这样做，它迫使行业形成标准，因为它不仅更安全，而且实际上更有效、更有用。

RH：类似地，斯坦福 HAI 在某些方面是先锋，特别是我想提到伦理审查委员会，但你可以就这个问题畅所欲言。

FFL：接着刚才的说，看到企业真正在思考伦理以及开发和发布产品的安全方式，这实际上是一个令人鼓舞的趋势。与此同时，学术界也在思考。例如，在我们的研究活动中设置一些防护栏的传统方式是，如果研究涉及人类受试者，我们会由 IRB（机构审查委员会）进行审查。这在美国大学乃至国际大学的学术界都至关重要，许多医学研究和涉及人类受试者的研究都需要由专家教授组成的委员会审查等等。但当 AI 开始成为大学研究的很大一部分时，我们没有类似的 IRB。起初这没问题，因为研究是理论性的，但我们很快意识到：“等等，AI 研究的例子太多了。”

例如，人脸识别。我们在全球范围内看到了偏见以及偏见带来的危害。那么，我们如何在 AI 研究设计中设置防护栏，以确保即使是最技术性的研究人员和学生也能考虑到他们工作的伦理和社会影响？

在 HAI，我们的职能之一是促进跨学科研究。Reid，你一直是我们研究所的董事会成员，所以我们经常讨论这一点。这非常重要。我们鼓励研究人员进行跨学科研究，但我们需要一种新方式，一个新的 IRB 来引导研究人员思考伦理和社会影响。据我们所知，HAI 是国内第一个成立所谓“伦理与社会审查委员会”（ESR）的大学组织，该委员会审查所有资助申请；在我们向研究人员分配资助时，我们确保资助项目与 ESR 委员会合作，阐明对研究的社会和伦理问题的理解以及潜在的缓解方案。这是我们高度重视的一个例子。

另一个例子是我们与政策制定者的合作。我们直接与监管机构、政策制定者，坦率地说，还有民间社会、不同人群、多方利益相关者合作，听取并传达 AI 的影响，倾听他们的想法，促进对话。我们特别注重将行业和民权组织、联邦、州和地方政府及政策制定者聚集到同一张桌子上，创建一个论坛，一个安全中立的论坛来进行这些对话，因为我们都关心创新和防护栏，我们需要这种对话。

RH：同时也请简要谈谈国家研究云，因为它是赋能大学及其他类似机构能够充分参与其中的重要组成部分。

FFL：我们认识到其中存在一种魔力。美国在过去一个世纪的神奇配方之一，就是我们作为一个国家、一个民族所具备的创新能力。我们拥有一个不可思议的生态系统，这要归功于我们的创业环境、高等教育体系、实验室、研究人员，以及联邦政府在激励研究方面发挥的作用——无论是NASA、DARPA还是NSF等机构。

但随着人工智能在过去十年的迅猛发展，我们观察到资源正迅速集中在少数拥有算力、数据和人才的公司手中——这三者正是实现卓越人工智能创新的关键资源，这固然很好。我们看到OpenAI微软、Google DeepMind、Facebook等公司不断推出令人惊叹的成果，但这不足以提升整个国家的实力，也不足以培养更多人工智能人才来应对全球竞争。我们需要确保这个生态系统保持健康，而不是仅仅集中在产业界内部。

当前在拜登政府的领导下，白宫和国会已授权成立一个特别工作组研究小组，由12名成员组成的特别工作组正在研究国家研究云的具体形态。我本人正是这个工作组的成员之一，与其他来自产业界、学术界和政府的人士共同参与。希望很快我们能完成报告并推动这项法案，以建立这一国家资源。

“我们需要确保这个生态系统保持健康，而不是仅仅集中在产业界内部。”

RH：Mira，我想问你……显然我有很多问题想问你们两位；根据观众的提问情况，我可能还会问更多。但在转向观众提问之前，我想先问你最后一个问题。

GPT-3和DALL-E所做的一件事，就是展示了一条真正增强人类创造力的路径。这不仅涉及安全等方面的潜在风险，也在某种意义上帮助我们更具人性、更具创造力。请谈谈从DALL-E和GPT-3中获得的启示，以及这种人类能力增强的潜力。

MM：确实如此。早在GPT-3时期，我们就惊讶地发现模型能够生成富有创意甚至动人的诗歌。我们曾给GPT-3一个提示词，要求它以巴勃罗·聂鲁达的风格创作一首关于麦克斯韦方程组的诗歌。我们很惊讶它既能把握麦克斯韦基础电磁方程组的要素，又能以聂鲁达爱情诗的风格呈现，这非常美妙。

许多人都在尝试GPT-3的诗歌创作和创意功能，而我认为DALL-E在这方面表现得更为突出。或许因为DALL-E生成的是图像，而且我们通过DALL-E实验室提供的交互形式让每个人都乐在其中。即使在OpenAI内部，我们也会花数小时生成DALL-E图像。这真正展示了像DALL-E这样的技术如何能够 democratize 高质量图像和创意的生成，并将其推向更远。

我们经常被问到：“这是否在某种程度上稀释了人类的原创创作？未来会怎样？”人类几乎本能地会保护自己的原创作品。回顾历史，我认为这与16、17世纪的情况并无太大不同：当时能负担得起绘画作品的人不多，因此情况相当二元化——你要么是伦勃朗，要么什么都不是，中间缺乏细腻的鉴赏层次。作品要么是杰作，要么不是。

因此我认为，随着我们拥有DALL-E或GPT-3这样的工具，或许人们对这种协同创作会有更细腻的鉴赏，对人类原创作品也会有不同层面的珍视。例如，今天一位受过艺术院校训练的优秀艺术家可以尝试临摹《夜巡》；在我这个外行看来，临摹品可能很棒，但我们仍然对真正的伦勃朗原作赋予不同的价值。

你可能会说：“好吧，这是一种精英主义观点。那么更广泛的影响呢？”我认为这与全球化的影响并无太大不同，因为它本质上是思想和文化的交流。当然，全球化有其负面效应，但总体而言，它确实创造了更多样性和更繁荣的景象。我们在历史上的西欧看到过这一点，19世纪因文化交流的自由化而成为最具活力和多样性的时期之一；相比之下，罗马帝国崩溃后的情况则截然相反。

实际上有一本关于这个主题的有趣著作叫《创造性干扰》，讨论了这种对人类才能可能被稀释的担忧。但长远来看，从全球效应观察，这种效应实际上是多元化的——最终我们会拥有更多的思想、更繁荣的景象，并将在艺术、科学和社会语境中持续创造更多信息。

“我认为随着我们拥有DALL-E或GPT-3这样的工具，或许人们对这种协同创作会有更细腻的鉴赏，对人类原创作品也会有不同层面的珍视。”

RH：我想我们还有时间回答一个观众提问。我之所以问最后那个问题，是因为如果你们还没机会体验DALL-E并制作自己的打印作品的话——OpenAI为我们提供了这个体验机会，这是个有趣的实验。

观众：大家好，针对刚才提到的最后一点：首先，你们在预判和减轻这些技术发布可能带来的影响方面所展现的深思熟虑和全面性，非常令人鼓舞。

我想知道，有些影响只有在技术实际部署到现实世界且达到一定规模后才可能显现。你们如何考虑大规模社会效应层面的风险和问题，以及潜在的二阶效应？科技公司在这个过程中可以扮演什么角色？可能与此相关的是……Fei-Fei，你提到了治理委员会的监督作用，我想知道你们如何平衡流程引入与创新速度之间的张力？如果你还记得的话，我们曾在CloudAI共事。我们内部设立过监督委员会，结果导致进程有所放缓……很想听听你们的看法，你们如何看待这个问题。

RH：Mira，不如你先开始回答？

MM：你说得对。在部署方面，我们才刚刚起步。我们将这些模型推向市场，观察人们如何使用它们，识别风险并构建缓解工具；但更重要的是，将反馈带回模型开发阶段，以构建更稳健的模型。我认为对齐技术是其中的核心。

但更广泛地说，我们共同思考人工智能的治理体系至关重要，因为显然这些模型的开发者需要来自多元化的群体，但即便如此仍不足够。我们必须像看待电力一样看待人工智能。因此，我们必须在全球背景下更广泛地思考系统的部署和治理，并吸纳来自不同领域的意见。

FFL：是的。关于创新与监管或防护措施的问题也非常重要。我想我们都经常面对这个问题，而思考它总是很有意义的。

首先，我并不认为它们相互排斥。它们之间存在矛盾。但我相信，正如生活中的其他事物一样，我们必须找到恰当的平衡点。如果我们走向极端，认为应该完全不加约束，那么创新可能会失败并给我们所有人带来伤害，这未必是我们想要生活的世界；当然，走向另一个极端也同样不可取。

在人工智能领域，我认为现在是时候让每个组织——从独立研究者到政府机构——展开对话，共同思考如何实现这种平衡。如果处理得当，合理的约束实际上能促进良性的创新。

我举一个智能摄像头在医疗环境中进行机器学习的例子，这是我实验室工作的一部分，其中我们重点探讨了安全问题。我们使用摄像头帮助医生监测患者的安全状况。初衷是好的，但随即我们就面临隐私问题和"监控软件"争议：你在监控谁？监控什么？这促使我们将其作为一种重要的社会人类反馈，推动我们进行更多创新。

我的学生团队原本只专注于深度学习算法，现在开始运用差分隐私算法来推进隐私保护的机器学习研究。随后他们发现差分隐私算法速度过慢，无法应用于我们使用摄像头的视频场景。正是这种约束性要求、监管压力，实际上激励了创新。

我们正在发表论文，推动差分隐私机器学习算法处理大规模数据。这个完美例证说明：当我们重视这些约束时，它们反而会激励我们创造出更好的技术。我认为人工智能领域有很多这样的案例，而且这样做会让所有人受益。

RH：由此可见，米拉和李飞飞都是让我持续学习的榜样，让我们为她们鼓掌。

FFL：谢谢，谢谢大家。

里德作为企业家和投资者，通过构建网络来培育标志性的全球企业。

> 本文由AI自动翻译，原文链接：[AI's Human Factor | Greylock](https://greylock.com/reid-hoffman/ais-human-factor/)
> 
> 翻译时间：2026-01-12 04:42
