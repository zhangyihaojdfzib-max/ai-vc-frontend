---
title: Jupyter Agent：训练LLM通过代码笔记本进行数据科学推理
title_original: 'Jupyter Agents: training LLMs to reason with notebooks'
date: '2025-09-10'
source: Hugging Face Blog
source_url: https://huggingface.co/blog/jupyter-agent-2
author: ''
summary: 本文介绍了Jupyter Agent项目，旨在通过为大型语言模型提供代码执行能力，使其能够在Jupyter笔记本环境中自主完成数据科学任务。作者团队与Qwen-3
  Coder合作构建演示，并专注于提升较小模型在智能体任务上的表现。文章详细阐述了使用DABStep基准进行评估、通过简化脚手架提升性能、以及利用Kaggle笔记本构建训练数据集的流程，目标是微调模型以更好地处理数据分析问题。
categories:
- AI研究
tags:
- Jupyter Agent
- LLM智能体
- 代码执行
- 数据科学
- 模型微调
draft: false
translated_at: '2026-02-13T04:24:38.319909'
---

# Jupyter Agents：训练LLM通过笔记本进行推理

过去一年，我们一直致力于为LLM提供更多工具和自主性，以解决更复杂和开放式的任务。Jupyter Agent的目标是为模型提供终极工具：代码执行。

展示多步骤代码执行与推理的自然方式是在Jupyter Notebook中，它由代码和Markdown单元格组成。因此，我们构建了Jupyter Agent，使其能够作为一个智能体，直接在Jupyter笔记本内执行代码，并利用此环境解决数据分析和数据科学任务。可以把它想象成Cursor，但原生存在于您的数据科学工作流程中。我们与Qwen-3 Coder（目前最强大的编码模型之一）合作，构建了这个愿景的演示。这是我们早期jupyter-agent (v1)工作的后续。

虽然大型模型开始展现出有用的行为，但关键问题是我们如何能持续改进它们。为此，我们专注于增强较小模型在智能体数据科学任务上的表现，因为它们目前难以与大型模型竞争。

本项目的目标是构建一个流程：首先生成高质量的训练数据，然后对现有小模型进行微调，最后评估模型在相关基准测试上的性能是否有所提升。

让我们从最后一步开始：为评估模型在数据科学任务上的表现选择一个强大的基准。

## 🏁 入门：DABStep基准测试

为了了解我们是否在朝着构建更好的数据科学智能体取得进展，我们需要一个基准来衡量这些能力。去年，我们与Adyen合作，推出了DABStep基准测试：一种评估数据科学智能体在真实任务上表现的方法。设置很简单：为LLM提供数据集，并要求它回答非平凡的数据问题。

示例任务：

这个基准对当今的LLM来说仍然具有挑战性——例如，表现最好的开箱即用模型是Claude 4 Sonnet，其在困难任务上的准确率甚至不到20%。您可以在此处探索实时排行榜。

## 🎯 第一个基线

既然我们确定了一个好的基准，我们就可以尝试攀登它！我们着手构建一个用于微调的数据集，使得即使是一个小型数据智能体模型也能在DABStep上表现良好。

我们的首选是Qwen3-4B-Thinking-2507：体积极小（迭代速度快，易于运行），但又足够强大以在智能体场景中发挥作用。

基线结果：

- 简单任务：44.4%
- 困难任务：2.1%

不算太好——但这是一个有希望的起点，因为它留下了很大的改进空间。让我们看看如何改进它！

## 🔧 脚手架入门

智能体区别于纯聊天模型的一个核心方面是围绕模型构建的、用于引导其行为的脚手架。例如，DABStep中的评估脚本使用smolagents来执行代码。Smolagents带有预定义的行为、提示结构和预期格式。

我们还研究了Qwen-Agent代码库，作者在那里为模型定制了脚手架。这是有道理的：例如，Claude Code与Claude Sonnet配合得非常好，因为它们的脚手架是对齐的。

因此，我们重构了我们的脚手架：

- 将其精简到约200行代码。
- 无外部依赖。
- 灵感来源于tiny-agents的精神。

👉 在此查看：utils.py。

结果：准确率从44.4%跃升至59.7%（简单任务部分）。🚀

我们的循环：

- 一个包含两个工具的While循环：code execution用于运行代码，final_answer用于返回最终答案。
- 我们与Qwen-Agent的不同之处在于，明确添加了一个final_answer工具——在我们的测试中，这提高了性能。
- 与smolagents相比，我们通过移除大量提示和工具简化了脚手架。Smolagents还通过使用ReACT框架将许多假设硬编码到模型中。

## 🏃‍♂️ 训练流程

简化脚手架就位后，我们专注于为数据科学智能体任务微调Qwen3-4B。

## ⚙️ 数据集流程

改进模型在特定任务或行为上的秘诀是，在尽可能接近任务的数据上对其进行训练。一个自然的起点是查看真实的Jupyter Notebook，并找到与我们计划处理的任务（即数据分析）密切相关的笔记本。

Kaggle笔记本提供了大量高质量的数据分析笔记本，并由Kaggle提供：

数据集：

- Kaggle笔记本数据集：约2TB的笔记本。
- Kaggle数据集：5TB的Kaggle数据集，我们手动下载并将其链接到笔记本。
- 每个笔记本的丰富元数据（作者、使用的数据集等）。

既然我们在基础模型上取得了不错的结果，是时候构建一个数据集来帮助我们进一步改进它了。我们设计了一个多阶段流程，使用Datatrove大规模清理和准备Kaggle笔记本。

![Jupyter Agent Dataset Pipeline](/images/posts/f2cac725aa26.png)

以下是每个步骤的工作方式：

### 1. 大规模去重

我们从约2TB的Kaggle笔记本开始，通过复用我们在BigCode项目中的工作，将其减少到约250GB。作为StarCoder2训练数据处理的一部分，这些笔记本（不含输出单元格）已经进行了去重。
大多数Kaggle笔记本是微小变化或近乎相同的副本，因此这一步至关重要。关键洞察：约90%的原始笔记本是重复的，如果不进行过滤，会扭曲训练。

### 2. 下载链接的数据集

大多数Kaggle笔记本通过Kaggle元数据引用外部数据集。为了确保笔记本内的代码能够实际运行，我们构建了一个自动获取这些链接数据集的流程。这一步至关重要，因为否则许多笔记本将不完整或无法执行。

使用kagglehub包，我们下载了数千个数据集——总计约5TB。为了保持可管理性和相关性：

- 我们过滤掉了包含模型检查点、大型多模态语料库或LLM相关文件的数据集。
- 我们还排除了无法放入我们用于执行的虚拟E2B沙箱的非常大的数据集（10GB+）。

最终，我们拥有了一组丰富的可执行笔记本及其配对的数据集，为在真实、可运行的环境中训练智能体奠定了基础。

### 3. 教育质量评分

我们使用Qwen3-32B根据教育质量对笔记本进行评分。我们发现使用整个笔记本并不理想，因为许多包含琐碎或损坏的代码。我们的教育评分方法详见edu_scoring.py。

TL;DR：我们根据清晰度、完整性和教育价值为每个笔记本分配1-5的分数，并只保留高于选定阈值的笔记本。此过滤移除了约70%的笔记本。

这与BeyondWeb论文的见解类似，该论文表明使用高质量数据对于合成数据生成更好——这是我们用于QA（问答）生成所依赖的步骤。
这有助于模型从“高质量”笔记本中学习，而不是从嘈杂的笔记本中学习。

### 4. 过滤不相关的笔记本

我们排除了关于训练LLM或与数据分析无关的笔记本。
我们还通过使用Qwen3-32B的基于LLM的自动过滤流程，移除了实际上未使用数据集的笔记本。过滤的实现可以在extract_packages_and_files.py中找到。

TL;DR：我们提示Qwen3-32B识别并移除那些（1）与数据分析无关，或（2）实际上未使用数据集的笔记本。这一步移除了约20%的笔记本。

这确保了我们只在相关的数据科学任务上进行训练。

### 5. QA生成

使用清理后的笔记本，我们使用Qwen3-32B生成了问答对。问题和答案基于真实的笔记本执行轨迹，因此QA对基于真实的代码执行结果。提示设计：我们要求LLM生成可能针对该数据集真实提出的自然问题，然后验证笔记本是否提供了正确答案。

**挑战：** 我们必须尝试许多提示词才能生成更高难度的问题，因为大语言模型倾向于生成琐碎的问题，例如"数据集的大小是多少"。
**洞察：** 我们将此过程分解为两个步骤，因为大语言模型倾向于产生幻觉答案：

1.  生成问题和答案。
2.  让另一个大语言模型（可以访问笔记本）来检查答案是否正确。

完整的提示策略和实现可在 `generate_qa.py` 中找到。

### 6. 轨迹生成

最后，我们希望生成简洁的代码执行轨迹，因为即使是处理后的原始笔记本也常常是开放式的、冗长的，包含许多不相关的部分。然而，我们希望我们的 Jupyter Agent 能够高效地得到结果。为了生成更简洁的笔记本轨迹用于训练，我们基于原始笔记本合成了轨迹。我们提示 `Qwen-3-Coder-480B` 模型生成一个 Jupyter 笔记本代码，以回答先前生成的合成问答对中的问题。
轨迹捕获了逐步的代码执行过程，包括中间输出，这对于 Agent 训练至关重要。

我们使用 `E2B` 让我们的 Agent 解决合成问答对，这需要获取 Kaggle 数据集，以便代码可以通过 E2B 实际运行。

**挑战 1：** 许多数据集不可用。
**技巧：** 由于大语言模型擅长代码且拥有不错的世界模型，当数据集缺失时，我们提示它们**扮演代码解释器的角色**。

提示词开头：

```
You are a stateful Python code interpreter that executes code in a persistent environment. Your role is to execute Python code while maintaining state across multiple code cells, similar to a Jupyter notebook environment.
[REST OF THE PROMPT]

```

**挑战 2：** `Qwen3-Coder-480B-A35B` 模型不支持思维模式——我们如何提取代码注释？默认情况下，它通常只输出简短的注释，然后是几个步骤的代码执行。然而，我们希望在每个单元格之间有一些推理或注释。
**技巧：** 当我们从 `Qwen3-32B` 切换到 `Qwen3-Coder-480B-A35B` 时，我们注意到输出消息内容经常为空。这原来是 Qwen3-Coder 模型一个已知的特性：在使用工具调用时，模型不会返回空的助手响应。我们通过工具强制执行一些文本注释，方法是在代码执行工具调用中将 'comment' 作为必填字段传递。这样，当使用非推理模型生成代码单元格时，它将默认以第一人称视角输出对其操作的一些描述，从而模拟思维轨迹结构。

**注意：** 笔记本中生成的最终答案可能与问答对中指定的答案不同。这是因为 Agent 模型可能使用与原始 Kaggle 笔记本不同的数据预处理方法和步骤，而合成问题通常不会指定这些。这种差异是正常的，并为一个新的令人兴奋的研究方向奠定了基础：语言模型如何处理数据分析，以及它们是否与人类处理方式不同。为了完全透明，我们同时保留了大语言模型生成的最终答案和来自真实 Kaggle 笔记本的原始答案，作为模型性能的信号。我们鼓励社区尝试不同的数据集组合，看看如何能进一步提升性能。

### 7. 最终筛选

我们截断了过长的输出，并过滤掉了琐碎的轨迹，以防止内容长度问题，只保留高质量的轨迹。我们保留了与 DABStep 风格任务一致的、非琐碎的多轮对话轨迹。最终得到的 **Jupyter Agent 数据集** 成为对 Qwen3-4B 模型进行监督微调的基础，包含 51k 个合成笔记本和近 2 亿个 Token。

有了这个数据集，自然的下一步就是看看它是否真的能帮助我们的模型成为一个更强大的数据科学 Agent。让我们继续讨论训练流程并评估其影响！

## 🏃‍♂️ 训练流程

有了精心筛选的数据集，我们转向关键问题：**这些数据是否真的能帮助模型更好地解决数据分析任务？** 为了找到答案，我们建立了一个简单的微调流程，并进行了实验来衡量训练对我们合成笔记本的影响。

一些训练步骤特别有趣，并给了我们有用的见解：

*   对于轨迹生成，我们使用大语言模型生成问答对，这为我们提供了一个**可验证的环境**。
*   最后，我们使用 `TRL` 对 `Qwen3-4B` 进行了微调。使用了 `assistant_loss_only=True` → 带来小幅性能提升。在全参数多轮训练中添加了 neftune 噪声 → 避免过拟合。
    *   使用了 `assistant_loss_only=True` → 带来小幅性能提升。
    *   在全参数多轮训练中添加了 neftune 噪声 → 避免过拟合。

**挑战：**

*   提示模型进行工具调用很棘手：并非所有提示都能带来相同的性能（参考 Qwen 文档）。
*   我们必须手动测试每一个提示，以找到效果最好的。
*   工具调用的响应格式没有标准化，使得在不同模型之间切换变得困难。
*   原生 Qwen 的生成提示词不适应 TRL 中 `assistant_loss_only=True` 的训练模式，该模式默认需要生成 Token。因此，我们通过将助手响应部分包裹在生成标签中来调整原始的聊天模板。
*   在简短的推理文本上训练思维模型可能会破坏模型能力 → 在这种情况下，与 PEFT 相比，全参数训练效果更好。

我们完整的训练实现，包括超参数配置和模板调整，可在我们仓库的 `finetuning directory` 中找到。

## 📊 结果

首先，我们使用 `Qwen3-Coder-480B-A35B` 生成了最终数据集，该数据集包含高质量的代码和简短的类推理轨迹。之后，我们开始了训练，并尝试了各种配置，如 PEFT/适配器与全参数调优、学习率、轮数、添加噪声等。我们发现，全参数微调使模型能够更好地学习和复制 `Qwen3-Coder-480B-A35B` 的行为响应质量，其支持性注释更简短，更贴合数据分析任务，没有不必要的冗长推理。

我们对训练轮数的影响进行了一个小的消融研究：

我们观察到，对于监督微调，使用较低的学习率和较高的 neftune 噪声 (7)，比通常多训练几轮是有益的。最后，我们将训练好的模型与已实现的脚手架进行比较，以确定我们训练数据集的纯粹影响。总之，我们可以看到，与基础模型/带脚手架的模型相比，DABStep 简单任务的得分提升了高达 36%/22%：

![DABstep Easy Score](/images/posts/386350fec6a4.png)

我们还可以看到，即使我们的数据集专注于较简单的问题，困难任务的得分也能提高：

![DABstep Hard Score](/images/posts/fd6df97cd34e.png)

从上面的图表中，可以注意到新的脚手架和基于我们合成笔记本的调优都产生了显著影响。这使得 Qwen-4B（结合我们的流程 + 脚手架）成为 DABStep 上最先进的小模型 Agent。

在实践中，该模型现在能够通过一致的执行来解决广泛的、现实的 Kaggle 风格数据分析任务。对于最困难的查询，它还不够强大，但我们已经证明，即使是小模型，在搭配正确的数据和脚手架时，也能成为强大的 Agent。

## 亲自尝试 Jupyter Agent

这些结果表明，即使是小模型，采用正确的训练方法也能成为强大的数据科学 Agent。准备好亲自尝试了吗？我们已经公开了所有内容，以便你可以试验我们微调的模型和数据集。

我们公开发布了调优后的 Qwen3-4B-Instruct-2507 和 Qwen3-4B-Thinking-2507 的最佳性能检查点以及训练数据集，你可以尝试和实验：

*   Jupyter Agent 数据集
*   Jupyter-Agent-Qwen3-4B-Instruct
*   Jupyter-Agent-Qwen3-4B-Thinking

你可以使用以下代码仅用几行代码加载 Jupyter Agent 数据集：

```python
from datasets import load_dataset
```

ds = load_dataset("jupyter-agent/jupyter-agent-dataset", split="non-thinking")

tokenizer.apply_chat_template(ds[0]["text"])

```

你也可以使用以下代码，通过 E2B 代码执行直接使用来源 Kaggle 的数据集：

```python
import kagglehub
import e2b_code_interpreter as e2b
from datasets import load_dataset


ds = load_dataset("jupyter-agent/jupyter-agent-dataset", split="thinking")

dataset_name = ds[0]["kaggle_dataset_name"]

path = kagglehub.dataset_download(dataset_name)
print(path) 

sandbox_init = e2b.Sandbox(timeout=240)

file_name = ds[0]["files_used"][0]
file_name = file_name.split('/')[-1] if '/' in file_name else file_name
with open(f"{path}/{file_name}", "rb") as file:
    sandbox_init.files.write(f"/home/user/input/{file_name}", file)

execution = sandbox_init.run_code("<some code>")

```

你可以按照 Qwen 文档代码使用基于 Qwen 调优的 Jupyter Agent 模型：

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "jupyter-agent/jupyter-agent-qwen3-4b-instruct"


tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)


prompt = "Give me a short introduction to large language model."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)


generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=16384
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

content = tokenizer.decode(output_ids, skip_special_tokens=True)

print("content:", content)

```

对于 Thinking 模型，你可以使用以下代码解码思考响应和内容：

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "jupyter-agent/jupyter-agent-qwen3-4b-thinking"


try:
    
    index = len(output_ids) - output_ids[::-1].index(151668)
except ValueError:
    index = 0

thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")
content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")

```

## 🔮 后续步骤

- 更困难的任务：生成更具挑战性、多步骤的问题，以更好地反映真实世界的分析。
- 扩大规模：在更大规模的精选轨迹上进行训练，以突破当前在困难划分上 3.4% 的性能。
- 知识蒸馏：研究知识蒸馏，该方法在改进小模型方面已显示出强大的效果。
- 强化学习：构建一个强化学习环境，该方法在智能体任务上已被证明能达到最先进的性能。由于我们的问答设置已经提供了一个可验证的环境，我们可以直接利用它进行强化学习训练。

也许这将引领我们走向……Jupyter-Agent 3.😉

我们希望我们的发现能激励他人在开发更强大的笔记本编码智能体方面继续取得进展，我们很期待看到社区接下来会构建出什么。请深入探索我们在 🤗 Hub 上的 `jupyter-agent` 数据集，并通过 `https://github.com/huggingface/jupyter-agent` 探索代码库，开始你在 Jupyter 笔记本智能体上的实验。

---

> 本文由AI自动翻译，原文链接：[Jupyter Agents: training LLMs to reason with notebooks](https://huggingface.co/blog/jupyter-agent-2)
> 
> 翻译时间：2026-02-13 04:24
