---
title: 强化学习的现实应用：构建持久护城河
title_original: 'RL in Real Life: Durable Moats | Greylock'
date: '2025-12-11'
source: Greylock Partners
source_url: https://greylock.com/greymatter/rl-in-real-life-durable-moats/
author: ''
summary: 本文探讨了强化学习（RL）从实验室研究到实际应用的差距。尽管RL在AI实验室中表现出色，但在企业环境中仍处于实验阶段。文章指出，仅依赖改进的基础模型无法解决企业定制化的个性化和自动化挑战，真正的竞争优势（护城河）来自于将RL方法与独特的业务背景、专有数据及工作流程深度结合。文中分析了RL训练的关键要素，如智能体、环境和奖励函数，并强调高质量环境（具备保真度、精确性和可扩展性）是有效RL的核心。最后，文章提出了RL的两个主要应用方向：构建自我改进的产品体验和实现真正的企业自动化。
categories:
- AI研究
tags:
- 强化学习
- AI应用
- 企业自动化
- 技术商业化
- 智能体
draft: false
translated_at: '2026-01-06T17:13:07.092Z'
---

![RL in Real Life: Durable Moats](/images/posts/dd97eb56bfb6.jpg)

## 过去一年，强化学习（RL）已从通往通用智能的一条有前途的路径，变成了旧金山晚餐桌上的一个禁忌词。

尽管有时被批评为过度炒作，但强化学习在AI实验室内部已被证明非常有效，它能帮助模型在特定任务上“爬山”，实现更好的性能。这种针对明确目标进行可靠优化的能力，导致了RL驱动型初创公司的激增。然而，在实验室之外，RL的实际应用案例却出人意料地不成熟。

我们着手调查现实世界的应用与炒作之间的差距，并随后与许多顶级的Agent（智能体）和企业公司进行了交流。到目前为止，他们的大部分RL工作在很大程度上仍处于实验阶段，尚未触及前沿实验室所使用的复杂技术的皮毛。与此同时，我们也感受到了这些公司对于如何利用RL方法的热情与好奇，主要集中在：

1.  构建自我改进的产品体验，以及
2.  解锁真正的企业自动化

与前沿实验室的对话则描绘了一幅完全不同的图景：随着旗舰模型的改进，几乎任何任务都可以解决，对许多独特应用的需求也随之消失。这引出了一个核心问题：如果底层模型会持续改进，为什么还要投资于专门的RL用例？

现实情况是，仅靠更好的基础模型无法解决公司定制化的个性化和自动化挑战。这些问题依赖于对独特业务背景、专有数据的深刻理解，以及如何将这些知识应用于特定工作流程。随着模型的改进，我们相信真正的护城河将来自于这些要素与有效RL方法的共生结合。

在本文中，我们将深入探讨构建有效RL方法的复杂性，评估当前的应用案例，并讨论创始人在商业化基于RL的系统时应考虑的因素。

但首先，让我们简要了解一下RL在实践中的真实面貌。

RL并非新技术。它长期以来一直是后训练流程的一部分，旨在使模型更安全、更符合用户期望。这通常包括两个步骤：

-   **监督微调（SFT）** – 教导模型遵循特定任务的指令
-   **RL** – 教导模型通过学习哪些输出更好来提升任务表现

RL之所以成为一种有效的学习技术，是因为在适当条件下，它已被证明**泛化能力更强**、**数据效率更高**，并且能最优地保留先验知识。

RL训练循环的主要组成部分是：
**Agent（智能体）** – 包括策略模型和RL学习算法。

**环境** – 场景上下文的高保真副本，包含供Agent（智能体）执行的示例任务。此环境包括：

-   **状态** – Agent（智能体）执行动作所依据的上下文。
-   **动作** – 触发动作（从而引发状态变化）的工具或选择。
-   **奖励函数** – 用于定义成功动作并强化积极行为的评判标准。

一个很好的例子是编码Agent（智能体），它在代码库中解决任务、运行测试，并根据结果获得奖励。这里的环境是代码库，奖励则校准Agent（智能体）关于如何最好地解决每个任务的知识。

![Fig. 1 - Environment Agent Interaction](/images/posts/1a0b2905a417.jpg)

## 创建高质量环境

在许多方面，有效RL的关键在于环境创建。正如我们将在下文看到的，这本质上是一个难题，可归结为两个关键原则：**质量**和**可扩展性**。

1.  **质量**意味着环境是真实且有影响力的。这包括：
    *   **保真度**：环境必须真实，以便Agent（智能体）能在现实世界中准确执行任务。这可能意味着为工程任务寻找复杂的代码库，或为计算机使用创建应用程序的精确副本。
    *   **精确性**：任务定义和评分标准必须精确，以防止Agent（智能体）进行奖励黑客攻击或利用指令中的漏洞。
    *   **环境类型**：最有效的环境将是独特且可验证的。它们能深化模型当前能力之外的知识，同时仍能清晰地检查正确性。编码和数学任务是此类领域的良好示例。
2.  **可扩展性**证明模型的知识可以超越手工制作的任务。这里有两个因素很重要：
    *   **泛化性**：理想情况下，Agent（智能体）可以从多个场景中学习，并将学习成果泛化到它未曾见过的类似问题上。课程学习或逐步提升环境/任务难度等技术可以提高这种能力。
    *   **环境生成**：为了更有效地扩展学习，找到自动化环境和任务创建的方法，而非手动设计，至关重要。

现在我们已经了解了这项技术，让我们来看看在与构建者和购买者的对话中，始终突出的两个用例。

## 用例 #1：产品自我改进

随着模型的不断改进，围绕它们构建产品的应用公司面临着一个持续的张力：平衡自身产品的深度与模型能力的快速提升。同时，构建软件现在比以往任何时候都更容易，大多数领域的竞争都非常激烈。为了提高用户留存率，在软件之外构建产品深度变得势在必行。在实践中，这意味着更好的个性化——根据特定客户偏好微调产品，以此增加产品粘性。

RL是实现这种个性化的强大工具，因为它能够基于用户反馈和信号强化特定行为。一个能够持续从用户交互中学习、接收输入并适应客户需求的产品，将成为最强大的自我强化产品护城河。

尽管对这一概念的探索尚处早期，但许多AI公司已经开始认识到**为顾客量身定制产品**的重要性。例如，**Cursor**和**Cognition**是在线RL的早期采用者，能够根据用户对标签的接受度实时调整其产品中的代码建议。如今，公司主要通过三种方式处理这种微调问题：

1.  内部构建：在内部构建在线/离线强化学习基础设施以支持特定用例，并利用真实客户数据创建内部环境，以实现真实的学习飞轮效应。
2.  多用途平台：使用像Tinker或OpenAI RFT这样的训练后解决方案进行强化学习，并引入您自己的环境、评估标准和数据（真实或合成）。
3.  定制化服务方案：以高接触度的方式与强化学习服务公司合作，共同设计环境、奖励机制并训练您的模型。根据隐私限制，这些环境可以使用真实或合成数据。

根据产品复杂性、领域和成熟度的不同，企业在实现产品自我改进时会遇到不同的挑战。

1.  **明确强化学习的适用条件**：大多数智能体产品都是具有多个故障点的复杂系统，包括用户界面、工具、提示词、上下文以及最终的模型本身。因此，直接采用强化学习可能并非最优选择。企业应首先尝试更好的基础模型、评估或监督微调等技术。当这些基础工作就绪后，如果满足以下两个条件，强化学习可以填补剩余的空白：产品具有许多清晰且可验证的奖励信号，以及可以创建真实的模拟环境。这就是为什么代码生成产品非常适合强化学习：它们拥有快速的用户反馈循环和明确的成功标准（例如代码被采纳）。
2.  **应对稀疏奖励信号**：智能体产品依赖模型轨迹和用户遥测数据进行微调，但这些数据可能模糊不清，甚至过于稀疏而难以解读。例如，客户服务产品中的工单重新打开可能被视为负面奖励的代理信号，但如果用户自己错误地关闭了工单呢？在这种情况下，我们看到企业采用多种方法来获取更准确的用户信号：手动或基于LLM审查遥测数据以区分用户意图；使用评估来改进用户体验并创建更多产品触点以收集用户信号；添加部分奖励或使用奖励塑形来增加中间步骤的信号。
3.  **自建与采购的决策**：根据我们与采购方的交流，他们对尝试强化学习有具体的兴趣，但也对其有效性持有合理的怀疑态度。早期阶段，大多数人更倾向于与外部供应商或服务商合作进行实验，以验证方法的可行性。一旦得到验证，一些企业希望将强化学习引入内部，以获得更好的准确性、安全性和成本效益。无论哪种情况，我们都注意到两个主要的摩擦点：安全与数据治理：大多数公司无法与供应商共享真实的客户数据。这导致了一个痛苦的过程，即创建可能无法准确代表客户账户的合成数据和环境，从而产生次优结果。人才稀缺：对于大多数初创公司来说，招聘强化学习基础设施专家是一项艰巨的任务。这是因为很少有具备将强化学习方法应用于实际产品经验的人才，而且这类人才本身非常昂贵。

**要点**：尽管产品自我改进是一个吸引人的概念，但其效果仍有待观察。两个悬而未决的问题是：

1.  企业如何针对不同类型的产品构建最具可扩展性和最有效的方法？
2.  企业应如何平衡内部强化学习开发与外部服务？

## 用例 #2：企业自动化

尽管人工智能能力不断增强，但企业在代码生成和搜索之外，远未实现有意义的自动化。模型的推理能力有所提高，但它们仍然难以在需要上下文和细微差别的定制化企业环境中进行泛化。这造成了持续存在的"最后一公里"差距，阻碍了企业看到价值。像保险理赔处理或信贷审批这样的工作流程很好地说明了这个问题：这些流程在不同组织间差异很大，一刀切的方法无法应对所有情况。

企业工作流程往往具有以下特点：

*   专有性
*   依赖于深厚的领域专业知识
*   由混乱且通常不完整的数据塑造
*   在不同组织间高度可变

更好的模型可以提高性能，但无法解决工作流程中这种程度的细微差别。强化学习可以通过模拟组织的独特环境，并根据其版本的成功标准定制奖励，来帮助弥合这一差距。

基于强化学习的企业自动化需要：

1.  能够复制客户环境上下文和条件的模拟环境。
2.  从关键客户利益相关者或领域专家那里精确获取的任务。
3.  奖励函数，其激励足够细粒度的准确轨迹，以防止偏差。

![图 3 - 强化学习企业自动化](/images/posts/8e10d51c5196.jpg)

尽管我们仍处于企业自动化强化学习用例的早期阶段，但以下一些关键问题将决定该领域业务的可行性。

1.  **哪些工作流程适合强化学习？** 最佳候选者是基础模型通常难以处理的工作流程——那些难以泛化、高风险、组织特有且需要深厚领域专业知识的工作流程。这些通常出现在金融、法律和医疗保健等垂直领域，在这些领域，专有工作流程可能因公司而异。在这里，由于通用模型无法把握工作流程之间的细微差别，"最后一公里"的差距被放大了。
2.  **最终产品应如何交付？** 显然，人工智能自动化是企业采购方的首要任务，但他们的采用速度却严重滞后。之前的微调产品过于开放，并假设企业会知道 1) 如何使用它们 2) 用它们来做什么。这种缺乏教育和与最终用户沟通的情况导致了严重的采用摩擦。最佳的解决方案将针对特定用例量身定制，并引导用户朝着正确的方向前进。由于这些重要的工作流程具有高风险性，企业重视对这些流程的可见性和参与度。这使得在任何强化学习基础设施之上都必须有一个详细的软件层——一个将可解释性集成到用户体验中，并确保其所有操作都具有可审计性的软件层。

**要点**：真正的自动化机会在于专有的、高风险的工作流程，这些是通用模型无法胜任的。成功的公司将把可扩展的强化学习基础设施与深入的客户协作和工作流程专业知识相结合，以弥合"最后一公里"的差距。

强化学习是人工智能能力下一个前沿领域的重要技术，我们Greylock很高兴看到它在前沿实验室之外得到扩散。企业中仍有许多个性化和自动化的挑战尚未解决。随着企业内部进行实验并开始认识到这些机会的价值，最好的供应商将根据企业的现状提供支持，并为可扩展的联合开发树立先例。

除了本文讨论的应用场景外，构建成功产品的关键要素之一是组建合适的团队。最有可能将这一愿景变为现实的团队，应具备以下特质：在尖端后训练方法方面有深厚背景、拥有高质量的工程实践经验，并对商业及企业需求具备实际理解。

如果您正在该领域进行构思或构建，我们期待与您交流。您可以通过shreya@greylock.com联系我。

感谢Tyler Griggs、Gautam Mittal、Rohan Pandey、Amy Deng、Simon Guo、Kshitij Alwadhi、Jason Risch、Jerry Chen和Sophia Luo的见解与讨论，他们的思想启发了本文的创作。

Shreya专注于与在人工智能、网络安全、基础设施及开发者工具领域进行早期创业的创始人合作。


> 本文由AI自动翻译，原文链接：[RL in Real Life: Durable Moats | Greylock](https://greylock.com/greymatter/rl-in-real-life-durable-moats/)
> 
> 翻译时间：2026-01-06 17:13
