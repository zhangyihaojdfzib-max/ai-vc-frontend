---
title: 全身条件化自我中心视频预测：基于人体动作的未来帧生成
title_original: Whole-Body Conditioned Egocentric Video Prediction
date: '2025-07-01'
source: Berkeley AI Research (BAIR)
source_url: http://bair.berkeley.edu/blog/2025/07/01/peva/
author: Ritwik Gupta
summary: 本文提出了一种基于人体动作的自我中心视频预测模型（PEVA），旨在为具身智能体构建世界模型。该模型以高维、结构化的全身运动姿态轨迹为条件，通过自回归条件扩散Transformer架构，从第一人称视角预测未来视频帧。研究在真实世界数据集上训练模型，能够生成原子动作视频、模拟反事实场景并进行长时程推演，解决了动作与视觉高度依赖、人体控制高维复杂、自我中心视角隐藏身体及感知滞后等挑战，为建模真实环境中的具身智能体行为提供了新思路。
categories:
- AI研究
tags:
- 视频预测
- 具身智能
- 世界模型
- 扩散模型
- 自我中心视觉
draft: false
translated_at: '2026-01-05T16:35:07.385Z'
---

**基于人体动作的自我中心视频预测（PEVA）**。给定过去的视频帧和指定3D姿态预期变化的动作，PEVA预测下一视频帧。我们的结果表明，给定第一帧和一系列动作，我们的模型能够生成原子动作的视频（a）、模拟反事实场景（b），并支持生成长视频（c）。

近年来，世界模型领域取得了显著进展，这些模型通过学习模拟未来结果以用于规划和控制。从直觉物理到多步视频预测，这些模型变得越来越强大和富有表现力。但很少有模型是为真正的具身智能体设计的。为了创建具身智能体的世界模型，我们需要一个在现实世界中行动的真实具身智能体。真实的具身智能体拥有物理上接地的复杂动作空间，而非抽象的控制信号。它们还必须在多样化的现实场景中行动，并具有自我中心视角，而非美学场景和固定摄像机视角。

ð¡ 提示：点击任意图片可查看全分辨率版本。

**为何困难**
- 动作和视觉高度依赖于上下文。相同的视角可能导致不同的运动，反之亦然。这是因为人类在复杂、具身、目标导向的环境中行动。
- 人类控制是高维且结构化的。全身运动涉及48个以上的自由度，具有分层、时间依赖的动力学特性。
- 自我中心视角揭示了意图但隐藏了身体。第一人称视觉反映了目标，而非运动执行，模型必须从不可见的物理动作中推断后果。
- 感知滞后于行动。视觉反馈通常延迟数秒才出现，这需要长时程预测和时间推理。

为了开发具身智能体的世界模型，我们必须将方法建立在满足这些标准的智能体之上。人类通常先看后动——眼睛锁定目标，大脑对结果进行简短的视觉"模拟"，然后身体才开始移动。在每一刻，我们的自我中心视角既作为来自环境的输入，也反映了下一个动作背后的意图/目标。当我们考虑身体运动时，应同时考虑脚的动作（移动和导航）和手的动作（操控），或者更广义地说，全身控制。

**我们做了什么？**
我们训练了一个模型，用于**基于人体动作的自我中心视频预测（PEVA）**，以实现**全身条件化的自我中心视频预测**。PEVA以身体关节层次结构组织的运动学姿态轨迹为条件，学习从第一人称视角模拟物理人体动作如何塑造环境。我们在Nymeria数据集上训练了一个自回归条件扩散Transformer，该数据集将现实世界的自我中心视频与身体姿态捕捉数据配对。我们的分层评估协议测试了日益具有挑战性的任务，对模型的具身预测和控制能力进行了全面分析。这项工作代表了通过人类视角视频预测来建模复杂现实世界环境和具身智能体行为的初步尝试。

**方法**
**基于运动的结构化动作表示**
为了桥接人体运动和自我中心视觉，我们将每个动作表示为一个丰富的高维向量，捕捉全身动力学和详细的关节运动。我们摒弃简化的控制信号，基于身体的运动学树编码全局平移和相对关节旋转。运动在3D空间中表示，根部平移有3个自由度，上半身有15个关节。使用欧拉角表示相对关节旋转，得到一个48维的动作空间（3 + 15 × 3 = 48）。运动捕捉数据通过时间戳与视频对齐，然后从全局坐标转换为以骨盆为中心的局部坐标系，以实现位置和方向不变性。所有位置和旋转都经过归一化以确保稳定学习。每个动作捕捉帧间的运动变化，使模型能够将物理运动与随时间推移的视觉后果联系起来。

**PEVA的设计：自回归条件扩散Transformer**
虽然导航世界模型中的条件扩散Transformer（CDiT）使用速度和旋转等简单控制信号，但建模全身人体运动提出了更大的挑战。人体动作是高维、时间上持续且受物理约束的。为了应对这些挑战，我们从三个方面扩展了CDiT方法：
- **随机时间跳跃**：使模型能够学习短期运动动力学和长期活动模式。
- **序列级训练**：通过对每个帧前缀应用损失来建模整个运动序列。
- **动作嵌入**：将时间t的所有动作连接成一个一维张量，作为每个AdaLN层的条件，以处理高维全身运动。

**采样与推演策略**
在测试时，我们通过以一组过去上下文帧为条件来生成未来帧。我们将这些帧编码为潜在状态，并向目标帧添加噪声，然后使用我们的扩散模型逐步去噪。为了加速推理，我们限制了注意力机制：图像内注意力仅应用于目标帧，而上下文交叉注意力仅应用于最后一帧。对于动作条件预测，我们使用自回归推演策略。从上下文帧开始，我们使用VAE编码器对其进行编码，并附加当前动作。然后模型预测下一帧，该帧被添加到上下文中，同时丢弃最旧的帧，并对序列中的每个动作重复此过程。最后，我们使用VAE解码器将预测的潜在表示解码到像素空间。

**原子动作**
我们将复杂的人体运动分解为原子动作——例如手部动作（上、下、左、右）和全身动作（前进、旋转）——以测试模型对特定关节级运动如何影响自我中心视角的理解。此处包含一些示例：
身体移动动作
左手动作
右手动作

**长时推演**
此处可以看到模型在扩展的预测时间范围内保持视觉和语义一致性的能力。我们展示了PEVA基于全身运动生成连贯的16秒推演的一些示例。此处包含一些视频样本和图像样本供仔细查看：

**规划**
PEVA可用于规划，通过模拟多个候选动作，并根据它们与目标的感知相似性（通过LPIPS度量）进行评分。
在此示例中，它排除了通向水槽或户外的路径，找到了打开冰箱的正确路径。
在此示例中，它排除了导致抓取附近植物和前往厨房的路径，同时找到了通向货架的合理动作序列。

**实现视觉规划能力**
我们将规划表述为一个能量最小化问题，并遵循导航世界模型[arXiv:2412.03572]中介绍的方法，使用交叉熵方法（CEM）进行动作优化。具体来说，我们在固定身体其他部位的同时，优化左臂或右臂的动作序列。结果规划的代表性示例如下：
在此案例中，我们能够预测一系列动作，使我们的右臂抬起到搅拌棒。我们看到了方法的局限性，因为我们只预测右臂，所以没有预测相应地移动左臂向下。
在此案例中，我们能够预测一系列动作，伸向水壶，但未能像目标那样完全抓住它。
在此案例中，我们能够预测一系列动作，将我们的左臂向内拉，与目标类似。

**定量结果**
我们通过多个指标评估PEVA，以证明其在从全身动作生成高质量自我中心视频方面的有效性。

我们的模型在感知质量上持续超越基线，在长时间跨度内保持连贯性，并展现出随模型规模提升的强劲扩展性。

**基线感知指标**
不同模型间的基线感知指标对比。

**原子动作表现**
各模型在生成原子动作视频方面的性能比较。

**FID对比**
不同模型及时间跨度下的FID对比。

**扩展性**
PEVA具有良好的扩展能力。更大的模型带来更优的性能。

**未来方向**
我们的模型在根据全身运动预测第一人称视频方面展现出有前景的结果，但这仅是迈向具身规划的早期一步。当前规划仅限于模拟候选手臂动作，缺乏长时程规划与完整轨迹优化。将PEVA扩展至闭环控制或交互环境是关键的下一步。模型目前缺乏对任务意图或语义目标的显式条件约束。我们的评估使用图像相似度作为代理目标。未来工作可探索将PEVA与高层目标条件设定相结合，并整合以物体为中心的表示方法。

**致谢**
作者感谢Rithwik Nukala在原子动作标注方面的帮助。感谢Katerina Fragkiadaki、Philipp Krähenbühl、Bharath Hariharan、Guanya Shi、Shubham Tulsiani和Deva Ramanan为改进论文提出的宝贵建议与反馈；感谢Jianbo Shi关于控制理论的讨论；感谢Yilun Du在扩散驱动方面的支持；感谢Brent Yi在人体运动相关工作上的帮助；感谢Alexei Efros关于世界模型的讨论与辩论。本研究部分由ONR MURI N00014-21-1-2801项目支持。

更多详情，请阅读完整论文或访问项目网站。


> 本文由AI自动翻译，原文链接：[Whole-Body Conditioned Egocentric Video Prediction](http://bair.berkeley.edu/blog/2025/07/01/peva/)
> 
> 翻译时间：2026-01-05 13:17
