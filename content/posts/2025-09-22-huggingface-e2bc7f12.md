---
title: Gaia2与ARE：开源框架赋能智能体社区评估
title_original: 'Gaia2 and ARE: Empowering the community to study agents'
date: '2025-09-22'
source: Hugging Face Blog
source_url: https://huggingface.co/blog/gaia2
author: ''
summary: 本文介绍了Meta推出的Gaia2智能体基准及其配套开源研究环境ARE。Gaia2作为GAIA的升级版，专注于评估智能体在读写交互、模糊指令处理、时间敏感任务、噪声容忍及多智能体协作等复杂现实场景中的能力。ARE框架模拟真实世界条件，支持智能体的运行、调试与评估。文章展示了多个主流模型在Gaia2上的测试结果，指出当前模型在时间推理、模糊性处理和适应性方面仍面临挑战，并强调开源工具对社区研究的重要推动作用。
categories:
- AI研究
tags:
- 智能体评估
- 基准测试
- 开源框架
- AI研究
- Meta
draft: false
translated_at: '2026-02-05T04:15:03.901605'
---

# Gaia2与ARE：赋能社区评估智能体

在理想世界中，AI智能体应是可靠的助手。当接收到查询时，它们能轻松处理指令中的模糊性，构建分步计划，准确识别必要资源，执行计划而不偏离方向，并适应意外事件，同时保持准确性并避免幻觉。
然而，开发智能体并测试这些行为绝非易事：如果您曾尝试调试自己的智能体，很可能已经体会到这过程是多么繁琐和令人沮丧。现有的评估环境与所评估的任务紧密耦合，缺乏现实世界的灵活性，且未能反映开放世界智能体面临的混乱现实：模拟页面永远不会加载失败，事件不会自发出现，异步混乱也无从体现。

正因如此，我们非常高兴推出Gaia2——这是智能体基准GAIA的后续版本，能够分析更为复杂的行为。Gaia2与开源的Meta智能体研究环境（ARE）框架一同发布，用于运行、调试和评估智能体。ARE模拟复杂的类现实世界条件，并可定制以进一步研究智能体行为。Gaia2数据集采用CC by 4.0许可发布，ARE采用MIT许可。

## Gaia2：现实生活助手任务的智能体评估

GAIA是2023年发布的智能体基准，包含3个级别的信息检索问题，需要借助工具、网络浏览和推理来解决。两年间，最简单的级别对模型而言已变得过于简单，社区也即将解决最困难的问题，因此是时候推出一个全新且更难的智能体基准了！

于是Gaia2应运而生，作为GAIA的后续版本，在所研究的能力范围上远超前者！

GAIA是只读基准，而Gaia2现在是一个读写基准，专注于交互行为和复杂性管理。智能体现在不仅要在搜索和检索方面接受评估，还要在受控故障的嘈杂环境中，针对模糊或时间敏感的查询进行指令遵循评估——这比任何其他模拟环境都更贴近真实世界条件。我们希望测试智能体如何管理有时会失效的工具或API，在非常具体的时间范围内规划连续动作，并适应新事件——这带来了全新的复杂性维度！

为此，我们使用了以下任务组（基于1000个全新的人工创建场景）：

- 执行：多步骤指令遵循和工具使用（例如，联系人更新）
- 搜索：跨源信息收集（例如，从WhatsApp获取朋友所在城市）
- 模糊性处理：澄清冲突请求（例如，日程安排冲突）
- 适应性：对模拟中变化的响应（例如，使用后续信息更新邮件）
- 时间/时序推理：时间敏感动作（例如，3分钟延迟后的出租车预订）
- 智能体间协作：无直接API访问权限的智能体间通信
- 噪声容忍度：对API故障和环境不稳定的鲁棒性

秉承GAIA的精神，场景不需要专业知识：原则上人类应能获得100%的分数，这便于模型开发者进行调试。

想探索这个基准吗？查看我们的数据集，您可以在我们的演示中更好地展示它。

### Gaia2如何运行？

Gaia2在ARE（一个执行环境）中运行，您选择的智能体可以访问一组应用程序及相关的预填充数据。

对于Gaia2，我们创建了一个智能手机模拟环境，模拟人类日常使用的场景。它包含现实世界的应用程序，如消息传递（邮件）、实用工具（日历、联系人、购物、文件系统等），以及与智能体对话的聊天界面。所有应用程序也通过工具调用对智能体开放。最后但同样重要的是，演示还包含一个模拟角色的对话历史和应用交互记录。

所有智能体交互在执行过程中都会自动记录为结构化轨迹，用于深入分析和研究：包括工具调用、API响应、模型思考、时间指标（例如，响应延迟）、用户交互等——并且都可以导出为JSON格式。

### 结果

作为参考，我们比较了一系列大型开源和闭源模型：Llama 3.3-70B Instruct、Llama-4-Maverick、GPT-4o、Qwen3-235B-MoE、Grok-4、Kimi K2、Gemini 2.5 Pro、Claude 4 Sonnet以及所有推理模式下的GPT-5。

所有模型均使用相同设置进行评估（为保持一致性使用统一的ReAct循环，温度为0.5，生成限制为16K个Token），结合模型作为评判者（Llama 3.3 Instruct 70B）和根据特定任务进行的精确匹配评估。所有101个工具（及通用环境描述）均在系统提示词中提供。

在评估的模型中，截至2025年9月，总体得分最高的模型是启用高推理模式的GPT-5，最佳开源模型是Kimi K2。

某些能力似乎已被最佳模型接近解决：简单工具调用和指令遵循的执行，以及整体搜索（从当前GAIA的结果中我们可能已经猜到）。目前，模糊性、适应性和噪声分割对所有模型来说仍然具有挑战性，有趣的是，在那些被认为是复杂智能体任务（指令遵循和搜索）上的表现，并不能很好地代表在更接近现实世界任务上的表现。最后但同样重要的是，目前对所有模型来说最困难的分割是时间分割：目前模型很难正确处理时间敏感的动作（尽管使用专用工具和更好的时序推理可能会缓解这一问题）。这些结果的详细分析可在论文中找到。

然而，我们认为重要的是超越原始分数进行报告：如果模型正确但花费了数千个Token才达到正确解决方案，或运行了数小时，那么它就不如以数量级更快速度成功的模型“好”。因此，我们还根据成本对分数进行了归一化，成本量化为LLM调用和输出Token的平均数量（这两者共同定义了成本-性能帕累托前沿）。在论文中，您将找到分数与货币成本及时间的对比。

### 与您喜爱的模型进行比较！在Gaia2上评估

如果您想在Gaia2上评估您的模型，可以按照以下步骤操作：

首先，在您选择的Python环境（uv、conda、virtualenv等）中安装Meta的智能体研究环境：

```bash
pip install meta-agents-research-environments

```

然后，为所有配置运行基准测试：执行、搜索、适应性、时间和模糊性。别忘了使用hf_upload参数将所有结果上传到中心！

```bash
are-benchmark run --hf meta-agents-research-environments/Gaia2     --split validation --config CONFIGURATION     --model YOUR_MODEL --model_provider YOUR_PROVIDER     --agent default     --max_concurrent_scenarios 2     --scenario_timeout 300     --output_dir ./monitored_test_results     --hf_upload YOUR_HUB_DATASET_TO_SAVE_RESULTS 

```

运行预言机以获取您的聚合分数文件：

```bash
are-benchmark judge --hf meta-agents-research-environments/Gaia2     --split validation --config CONFIGURATION     --agent default     --max_concurrent_scenarios 2     --scenario_timeout 300     --output_dir ./monitored_test_results --hf_upload YOUR_HUB_DATASET_TO_SAVE_RESULTS 

```

最后，在README中添加有关您模型的所有相关信息，并在排行榜上分享，将Gaia2轨迹集中在此处！

## 超越Gaia2：使用ARE研究您的智能体

除了基准测试场景外，您还可以在ARE中使用Gaia2应用和内容，来检验模型是否能正确解决那些较难验证的任务，例如加载邮件、撰写跟进邮件、将事件添加到日历或预订会议——总之，这为通过交互评估您的AI助手提供了完美的设置！

您也可以轻松自定义环境，通过：1）连接您的工具（通过MCP或直接连接）来测试您的Agent；2）实现您自己的场景，包括定义触发或定时事件（例如：2分钟后，邮件应用将收到来自联系人的新邮件），以观察Agent如何适应不断变化的环境。

（由于Agent默认是json agent，它们不会干扰您的机器，除非您将它们连接到具有不安全权限的外部应用。因此，在添加自己的应用或使用不受信任的MCP时，请谨慎操作。）

以下是我们使用ARE的几个用例：

- 在真实或模拟数据上对任何Agent进行氛围检查，研究具有各自规则、工具、内容和验证的多样化设置
- 使用本地应用或MCP工具测试Agent的工具调用和编排能力
- 生成您自己的工具调用轨迹，以微调工具调用模型
- 在统一框架中轻松收集和复现现有的Agent基准测试
- 在用户界面中实时调试和研究Agent间的交互
- 研究在嘈杂环境（伴有API超时和模糊性）中的模型局限性

我们录制了3个视频，以便您查看其中部分用例（当然，我们希望社区能创造性地使用ARE :hugging_face:）。在这些视频中，我们使用了上述默认演示，其中模拟了机器学习博士生Linda Renne的生活。

### 1) 在简单任务上测试Agent：活动组织

为了测试默认模型在活动组织方面的能力，我们来策划一个生日派对！

我们首先要求Agent给Renne家族的所有成员发短信，通知他们11月7日用户30岁生日派对的消息。默认环境中有21个联系人，包括5位Renne家族成员——模拟“主人”Linda、她的父母George和Stephie、她的妹妹Anna以及她的祖父Morgan。Agent成功遍历联系人列表，找到四位家庭成员，并给他们发送了短信。

接下来，我们要求Agent创建一个日历邀请并将他们添加为受邀者。Agent记住了上述上下文！它在正确的日期创建了日历邀请，并正确地将家庭成员添加了进去。

### 2) 理解Agent：深入探究轨迹

ARE还允许我们查看Agent所采取行动背后的轨迹。
打开左侧的Agent日志工具，我们可以看到系统提示词、思维链、使用所调用工具采取的多步骤行动，以及整理有序的日志结果。如果您需要离线查阅，所有内容都可以导出为json格式！

### 3) 探索和扩展演示：将Agent连接到您自己的MCP

在最后一个示例中，我们通过MCP将ARE连接到远程机械臂，使其能向我们做手势，然后要求Agent通过挥动机器臂来回答我们的“是”或“否”问题！效果如下所示。

但这些示例只是非常简单的起点，我们真正期待的是您将构建的内容！（对于高级用户，您甚至可以在此处直接安装和编辑Meta-ARE代码。）

## 结论

Gaia2和ARE是新的研究工具，我们希望通过它们赋能任何人轻松构建更可靠、适应性更强的AI Agent——通过便捷的实验，让真实世界评估对所有人触手可及，并通过透明、可复现的基准测试和可调试的轨迹来提高信任度。

我们非常期待看到您用这个项目创造出什么！

---

> 本文由AI自动翻译，原文链接：[Gaia2 and ARE: Empowering the community to study agents](https://huggingface.co/blog/gaia2)
> 
> 翻译时间：2026-02-05 04:15
