---
title: Daggr：用代码串联AI应用，可视化调试工作流
title_original: 'Introducing Daggr: Chain apps programmatically, inspect visually'
date: '2026-01-29'
source: Hugging Face Blog
source_url: https://huggingface.co/blog/daggr
author: ''
summary: 本文介绍了由Gradio团队开发的开源Python库Daggr，它旨在解决构建复杂AI工作流时的调试与管理难题。Daggr采用代码优先的方式，允许开发者用Python定义连接Gradio应用、模型和自定义函数的工作流，并自动生成可视化画布。用户可实时检视中间结果、单独重运行出错步骤、替换备用节点，并享受与Gradio
  Spaces的无缝集成及状态持久化功能，从而显著提升AI应用的原型开发与调试效率。
categories:
- AI基础设施
tags:
- 工作流编排
- Gradio
- Python库
- 可视化调试
- AI开发工具
draft: false
translated_at: '2026-01-30T04:04:38.913262'
---

# 介绍 Daggr：以编程方式串联应用，可视化检视

TL;DR：Daggr 是一个全新的开源 Python 库，用于构建连接 Gradio 应用、机器学习模型和自定义函数的 AI 工作流。它能自动生成一个可视化画布，让你可以检视中间输出、重新运行单个步骤、管理复杂流程的状态，所有这些只需几行 Python 代码！

1.  背景
2.  快速开始
3.  分享你的工作流
4.  包含不同节点的端到端示例
5.  后续步骤

## 背景

如果你构建过需要组合多个模型或处理步骤的 AI 应用，你肯定体会过其中的痛苦：串联 API 调用、调试流程、以及丢失对中间结果的追踪。当一个 10 步工作流的第 5 步出错时，你通常不得不重新运行所有步骤，只是为了看看发生了什么。

大多数开发者要么构建难以调试的脆弱脚本，要么转向为生产流程设计的重型编排平台——这并不适合快速实验。

我们开发 Daggr 是为了解决在构建 AI 演示和工作流时反复遇到的问题：

**可视化你的代码流程**：与基于节点的图形界面编辑器（你需要在其中拖拽和连接节点）不同，Daggr 采用代码优先的方法。你用 Python 定义工作流，可视化画布会自动生成。这意味着你同时获得了两个世界的最佳体验：可版本控制的代码以及对中间输出的可视化检视。

**检视和重新运行任何步骤**：可视化画布不只是摆设。你可以检视任何节点的输出、修改输入、并重新运行单个步骤，而无需执行整个流程。当你调试一个 10 步工作流而只有第 7 步出问题时，这非常宝贵。你甚至可以提供“备用节点”——用另一个模型或 Space 替换一个——来构建健壮的工作流。

**一流的 Gradio 集成**：由于 Daggr 由 Gradio 团队构建，它能与 Gradio Spaces 无缝协作。指向任何公开（或私有）的 Space，你就可以将其用作工作流中的一个节点。无需适配器，无需包装器——只需引用 Space 名称和 API 端点。

**状态持久化**：Daggr 会自动保存你的工作流状态、输入值、缓存结果、画布位置——这样你就可以从上次中断的地方继续。使用“工作表”可以在同一个应用中维护多个工作空间。

## 快速开始

使用 pip 或 uv 安装 daggr，它只需要 Python 3.10 或更高版本：

```shell
pip install daggr
uv pip install daggr
```

这是一个生成图像并移除其背景的简单示例。从 Space 底部查看[此 Space 的 API 参考](https://huggingface.co/spaces/hf-applications/Z-Image-Turbo)，以了解它接受哪些输入并产生哪些输出。在此示例中，Space 返回原始图像和编辑后的图像，因此我们只返回编辑后的图像。

```py
import random
import gradio as gr
from daggr import GradioNode, Graph


image_gen = GradioNode(
    "hf-applications/Z-Image-Turbo",
    api_name="/generate_image",
    inputs={
        "prompt": gr.Textbox(
            label="Prompt",
            value="A cheetah sprints across the grassy savanna.",
            lines=3,
        ),
        "height": 1024,
        "width": 1024,
        "seed": random.random,
    },
    outputs={
        "image": gr.Image(label="Generated Image"),
    },
)


bg_remover = GradioNode(
    "hf-applications/background-removal",
    api_name="/image",
    inputs={
        "image": image_gen.image,  
    },
    outputs={
        "original_image": None,  
        "final_image": gr.Image(label="Final Image"),
    },
)

graph = Graph(
    name="Transparent Background Generator", 
    nodes=[image_gen, bg_remover]
)
graph.launch()
```

就是这样。运行此脚本，你将在端口 7860 上获得一个自动启动的可视化画布，以及一个可分享的实时链接，显示两个已连接的节点，你可以在每个步骤修改输入并检视输出。

![App](/images/posts/e9a2c3966135.png)

### 节点类型

Daggr 支持三种类型的节点：

**GradioNode** 调用 Gradio Space API 端点或本地运行的 Gradio 应用。传递 `run_locally=True`，Daggr 会自动克隆 Space，创建一个隔离的虚拟环境，并启动应用。如果本地执行失败，它会优雅地回退到远程 API。

```py
node = GradioNode(
    "username/space-name",
    api_name="/predict",
    inputs={"text": gr.Textbox(label="Input")},
    outputs={"result": gr.Textbox(label="Output")},
)


node = GradioNode(
    "hf-applications/background-removal",
    api_name="/image",
    run_locally=True,
    inputs={"image": gr.Image(label="Input")},
    outputs={"final_image": gr.Image(label="Output")},

```

**FnNode** — 运行自定义 Python 函数：

```py
def process(text: str) -> str:
    return text.upper()

node = FnNode(
    fn=process,
    inputs={"text": gr.Textbox(label="Input")},
    outputs={"result": gr.Textbox(label="Output")},
)

```

**InferenceNode** — 通过 Hugging Face Inference Providers 调用模型：

```py
node = InferenceNode(
    model="moonshotai/Kimi-K2.5:novita",
    inputs={"prompt": gr.Textbox(label="Prompt")},
    outputs={"response": gr.Textbox(label="Response")},
)

```

### 分享你的工作流

使用 Gradio 的隧道功能生成公共 URL：

```py
graph.launch(share=True)
```

要进行永久托管，请使用 Gradio SDK 将其部署在 Hugging Face Spaces 上——只需将 `daggr` 添加到你的 `requirements.txt` 文件中。

## 包含不同节点的端到端示例

我们现在将开发一个接收图像并生成 3D 资源的应用。此演示可以在 daggr 0.4.3 上运行。步骤如下：

1.  获取图像，移除背景：为此，我们将克隆 [BiRefNet Space](https://huggingface.co/spaces/merve/BiRefNet) 并在本地运行它。
2.  为提升效率，缩小图像：我们将为此使用 FnNode 编写一个简单的函数。
3.  生成 3D 资源风格的图像以获得更好效果：我们将使用 InferenceNode 配合 Inference Providers 上的 [Flux.2-klein-4B 模型](https://huggingface.co/spaces/merve/Flux.2-klein-4B)。
4.  将输出图像传递给 3D 生成器：我们将把输出图像发送到托管在 Spaces 上的 [Trellis.2 Space](https://huggingface.co/spaces/merve/Trellis.2)。

本地运行的 Space 可能会在应用文件中将模型加载到 CUDA（使用 `to.("cuda")`）或 ZeroGPU。要禁用此行为以在 CPU 上运行模型（如果你没有 NVIDIA GPU 的设备，这很有用），请复制你想要使用的 Space 并克隆它。

生成的图如下所示。

![App](/images/posts/a0ba95c58cd9.png)

让我们编写第一步，即背景移除器。我们将在本地克隆并运行[此 Space](https://huggingface.co/spaces/merve/background-removal)。此 Space 在 CPU 上运行，大约需要 13 秒。如果你有 NVIDIA GPU，可以换成[此应用](https://huggingface.co/spaces/merve/BiRefNet)。

```py
from daggr import FnNode, GradioNode, InferenceNode, Graph

background_remover = GradioNode(
   "merve/background-removal",
   api_name="/image",
   run_locally=True, 
   inputs={
       "image": gr.Image(),
   },
   outputs={
       "original_image": None,
       "final_image": gr.Image(
           label="Final Image"
       ),
   },
)
```

对于第二步，我们需要编写一个辅助函数来缩小图像并将其传递给 `FnNode`。

```py
from PIL import Image
from daggr.state import get_daggr_files_dir


def downscale_image_to_file(image: Any, scale: float = 0.25) -> str | None:
   pil_img = Image.open(image)
   scale_f = max(0.05, min(1.0, float(scale)))
   w, h = pil_img.size
   new_w = max(1, int(w * scale_f))
   new_h = max(1, int(h * scale_f))
   resized = pil_img.resize((new_w, new_h), resample=Image.LANCZOS)
   out_path = get_daggr_files_dir() / f"{uuid.uuid4()}.png"

   resized.save(out_path)
   return str(out_path)
```

我们现在可以将函数传递给 `FnNode` 进行初始化。

```py
downscaler = FnNode(
   downscale_image_to_file,
   name="Downscale image for Inference",
   inputs={
       "image": background_remover.final_image,
       "scale": gr.Slider(
           label="Downscale factor",
           minimum=0.25,
           maximum=0.75,
           step=0.05,
           value=0.25,
       ),
   },
   outputs={
       "image": gr.Image(label="Downscaled Image", type="filepath"),
   },
)
```

我们现在将使用Flux模型编写`InferenceNode`。

```py
flux_enhancer = InferenceNode(
   model="black-forest-labs/FLUX.2-klein-4B:fal-ai",
   inputs={
       "image": downscaler.image,
       "prompt": gr.Textbox(
           label="prompt",
           value=("Transform this into a clean 3D asset render"),
           lines=3,
       ),
   },
   outputs={
       "image": gr.Image(label="3D-Ready Enhanced Image"),
   },
)

```

当将使用`InferenceNode`的应用部署到Hugging Face Spaces时，请使用一个细粒度的Hugging Face访问令牌，并仅勾选"Make calls to Inference Providers"选项。

最后一个节点是通过查询Hugging Face上的Trellis.2 Space进行3D生成。

```py
trellis_3d = GradioNode(
   "microsoft/TRELLIS.2",
   api_name="/image_to_3d",
   inputs={
       "image": flux_enhancer.image,
       "ss_guidance_strength": 7.5,   
       "ss_sampling_steps": 12,     
   },
   outputs={
       "glb": gr.HTML(label="3D Asset (GLB preview)"),
   },
)

```

将它们链接在一起并启动应用非常简单，如下所示。

```py
graph = Graph(
   name="Image to 3D Asset Pipeline",
   nodes=[background_remover, downscaler, flux_enhancer, trellis_3d],
)

if __name__ == "__main__":
   graph.launch()

```

你可以在[此Space](https://huggingface.co/spaces/daggr/Image-to-3D-Asset-Pipeline)中找到正在运行的完整示例。要在本地运行，你只需要获取app.py文件，安装依赖项并登录到Hugging Face Hub。

## 后续步骤

Daggr目前处于测试阶段，并且有意保持轻量级。API可能会在版本之间发生变化，虽然我们在本地持久化工作流状态，但在更新期间仍可能出现数据丢失。如果你有功能请求或发现错误，请在此处提交问题。我们期待你的反馈！在社交媒体上分享你的daggr工作流并@Gradio，将有机会被展示。在此处查看所有精选作品。

---

> 本文由AI自动翻译，原文链接：[Introducing Daggr: Chain apps programmatically, inspect visually](https://huggingface.co/blog/daggr)
> 
> 翻译时间：2026-01-30 04:04
