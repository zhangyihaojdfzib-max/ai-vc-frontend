---
title: 与美英AI安全机构合作，强化AI系统防护
title_original: Strengthening our safeguards through collaboration with US CAISI and
  UK AISI
date: '2025-09-12'
source: Anthropic
source_url: https://www.anthropic.com/news/strengthening-our-safeguards-through-collaboration-with-us-caisi-and-uk-aisi
author: ''
summary: 本文介绍了Anthropic与美国CAISI及英国AISI在AI安全领域的合作成果。通过提供模型深度访问权限和开展迭代测试，双方共同识别并修复了宪法分类器中的多项漏洞，包括提示词注入、混淆攻击等。合作经验表明，政府机构的专业红队测试与公开漏洞赏金计划形成互补，能更有效地提升AI系统的整体安全性。
categories:
- 政策监管
tags:
- AI安全
- 红队测试
- 政府合作
- 漏洞防护
- 宪法分类器
draft: false
translated_at: '2026-02-11T04:29:20.089664'
---

# 通过与美国CAISI和英国AISI合作加强我们的安全防护措施

过去一年，我们与美国人工智能标准与创新中心（CAISI）和英国人工智能安全研究所（AISI）开展了合作。这两个政府机构旨在评估和提升人工智能系统的安全性。我们的自愿合作始于初步磋商，随后逐步发展为持续伙伴关系，CAISI和AISI团队得以在模型开发的不同阶段接触我们的系统，实现对系统的持续测试。

政府机构为此项工作带来了独特能力，特别是在网络安全、情报分析和威胁建模等国家安全领域拥有深厚专业知识。结合其机器学习专长，他们能够评估特定攻击向量和防御机制。他们的反馈帮助我们改进安全措施，使我们的系统能够抵御一些最复杂的滥用企图。

与独立外部专家合作识别人工智能系统漏洞，是Anthropic安全防护方法的核心组成部分，对于防止模型滥用造成现实世界危害至关重要。

## 发现并解决漏洞

此次合作已取得关键发现，帮助我们加强了用于防止模型恶意使用的工具。根据我们与CAISI和AISI分别达成的协议，每个组织在部署前对我们用于发现和防止越狱的防御系统——宪法分类器——在Claude Opus 4和4.1等模型上的多个迭代版本进行了评估，以帮助识别漏洞并建立强大的防护措施。

**宪法分类器测试**。我们向CAISI和AISI提供了宪法分类器的多个早期版本，并在改进过程中持续提供最新系统的访问权限。我们共同对这些分类器进行了压力测试：政府红队人员在部署前后识别了一系列漏洞，我们的技术团队则利用这些发现来加强防护措施。这些漏洞示例包括：

- **发现提示词注入漏洞**。政府红队人员通过提示词注入攻击发现了我们早期分类器的弱点。此类攻击使用隐藏指令诱使模型产生系统设计者未预期的行为。测试人员发现，特定注释（例如虚假声称已进行人工审核）可能完全绕过分类器检测。我们已修补这些漏洞。
- **压力测试防护架构**。他们开发了一种复杂的通用越狱方法，将有害交互编码成能规避我们标准检测方式的形式。这一发现促使我们从根本上重构了防护架构，以解决底层漏洞类别，而非仅仅修补单个利用方式。
- **识别基于密码的攻击**。使用密码、字符替换和其他混淆技术对有害请求进行编码，以规避我们的分类器。这些发现推动了检测系统的改进，使其能够识别并阻止伪装的有害内容，无论采用何种编码方法。
- **输入输出混淆攻击**。发现了使用针对我们特定防御措施定制的复杂混淆方法的通用越狱技术，例如将有害字符串分割成更广泛上下文中看似良性的组成部分。识别这些盲点使我们能够有针对性地改进过滤机制。
- **自动化攻击优化**。建立了新的自动化系统，逐步优化攻击策略。他们最近使用该系统，从一个效果较差的越狱方法迭代产生了一个有效的通用越狱方法，我们正利用此方法来改进防护措施。

**评估与风险方法论**。除了识别具体漏洞，CAISI和AISI团队还帮助我们加强了更广泛的安全方法。他们从外部视角对证据要求、部署监控和快速响应能力提出的见解，对于压力测试我们的假设以及识别可能需要额外证据来支持威胁模型的领域具有宝贵价值。

## 有效合作的关键经验

我们的经验为我们提供了关于如何与政府研究和标准机构有效合作以提升模型安全性的重要经验。

**全面的模型访问提升红队测试效果**。我们的经验表明，给予政府红队人员对我们系统更深层的访问权限，能够实现更复杂的漏洞发现。我们提供了几项关键资源：

- **部署前的防护原型**。测试人员可以在防护系统上线前进行评估和迭代，在防护措施部署前识别弱点。
- **多种系统配置**。我们提供了涵盖整个防护范围的模型，从完全无保护的版本到具备完整防护措施的模型。这种方法使测试人员能够首先针对基础模型开发攻击，然后逐步完善技术以绕过日益复杂的防御。仅提供帮助的模型变体也支持精确的有害输出评分和能力基准测试。
- **详尽的文档和内部资源**。我们向受信任的政府红队人员提供了防护架构细节、已记录的漏洞、防护报告以及细粒度的内容政策信息（包括具体的禁止请求和评估标准）。这种透明度帮助团队瞄准高价值的测试领域，而非盲目寻找弱点。
- **实时防护数据加速漏洞发现**。我们给予政府红队人员直接访问分类器评分的权限。这使得测试人员能够优化攻击策略并进行更有针对性的探索性研究。

**迭代测试促进复杂漏洞发现**。虽然单次评估具有价值，但持续合作使外部团队能够深入了解系统专业知识并发现更复杂的漏洞。在关键阶段，我们与合作伙伴保持了每日沟通渠道和频繁的技术深度交流。

**互补方法提供更强大的安全性**。CAISI和AISI的评估与我们更广泛的生态系统协同作用。公开的漏洞赏金计划从广泛的人才库中产生大量、多样化的漏洞报告，而专业专家团队则有助于发现需要深厚技术知识才能识别的复杂、微妙的攻击向量。这种多层策略有助于确保我们捕获常见利用方式和复杂的边缘情况。

## 持续合作

使强大的人工智能模型安全且有益，不仅需要技术创新，还需要产业与政府之间新的合作形式。我们的经验表明，当技术团队紧密合作以识别和应对风险时，公私合作伙伴关系最为有效。

随着人工智能能力的发展，对缓解措施的独立评估作用日益重要。我们欣慰地看到其他人工智能开发者也在与这些政府机构合作，并鼓励更多公司这样做，并更广泛地分享他们自己的经验。

我们向美国CAISI和英国AISI的技术团队表示衷心感谢，感谢他们严格的测试、深思熟虑的反馈以及持续的合作。他们的工作实质性地提升了我们系统的安全性，并推动了衡量人工智能防护有效性领域的发展。

## 相关内容

### 推出Claude Opus 4.6

我们正在升级我们最智能的模型。在智能体编码、计算机使用、工具使用、搜索和金融领域，Opus 4.6是一个行业领先的模型，通常优势显著。

### Claude是一个思考空间

我们做出了一个选择：Claude将保持无广告模式。我们将解释为何广告激励与真正有用的AI助手不相容，以及我们计划如何在扩展服务范围的同时不损害用户信任。

### Apple的Xcode现已支持Claude Agent SDK

---

> 本文由AI自动翻译，原文链接：[Strengthening our safeguards through collaboration with US CAISI and UK AISI](https://www.anthropic.com/news/strengthening-our-safeguards-through-collaboration-with-us-caisi-and-uk-aisi)
> 
> 翻译时间：2026-02-11 04:29
