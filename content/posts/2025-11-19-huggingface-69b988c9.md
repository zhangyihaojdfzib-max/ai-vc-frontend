---
title: Apriel-H1：蒸馏高效推理模型的非直观关键
title_original: 'Apriel-H1: The Surprising Key to Distilling Efficient Reasoning Models'
date: '2025-11-19'
source: Hugging Face Blog
source_url: https://huggingface.co/blog/ServiceNow-AI/apriel-h1
author: null
summary: 本文介绍了如何通过知识蒸馏将15B推理模型转换为Mamba混合架构，实现2.1倍吞吐量提升且质量损失极小。核心洞见在于蒸馏数据的选择：应使用教师模型SFT数据集中的高质量推理轨迹，而非预训练数据，以保留其复杂推理模式。文章还详细阐述了分阶段蒸馏方法，包括识别不重要层、渐进式转换和使用反向KL散度损失函数，成功克服了直觉误区并实现了高效模型转换。
categories:
- AI研究
tags:
- 模型蒸馏
- 推理模型
- Mamba架构
- 高效计算
- 知识迁移
draft: false
translated_at: '2026-01-06T01:06:27.631Z'
---

Apriel-H1：蒸馏高效推理模型的惊人关键
我们将15B推理模型转换为Mamba混合架构，实现了2.1倍的吞吐量提升，且质量损失极小。关键何在？一个关于蒸馏数据选择的非显而易见洞见，以及为何直觉在此处会失效。

当MiniMax在10月发布其M2事后分析，解释为何在230B规模放弃高效注意力机制时，舆论一度转向“高效注意力已死”。几天之内，Kimi Linear便证明了相反的情况。真正的教训是：这取决于你的约束条件。

我们的约束很简单：我们有一个强大的15B推理模型，需要在不从头开始的情况下使其高效。没有无限算力进行20T Token的预训练，也没有从第一天起就进行架构协同设计的奢侈条件。只有一个实际问题：能否通过蒸馏将效率改造到现有模型中？

剧透：可以，但前提是必须忽略关于使用什么数据的直觉。

**我们构建了什么**
Apriel-H1系列：包含七个检查点，涵盖25至40个Mamba层（总共50层），展示了完整的效率-质量边界。我们的旗舰模型Apriel-H1-15b-Thinker-SFT实现了2.1倍吞吐量，质量损失极小：MATH500和MTBench略有提升（分别从0.90到0.92和从8.30到8.58），而GSM8k（0.97 → 0.95）、GPQA（0.59 → 0.55）和AIME24（0.70 → 0.65）略有下降。总训练量：768亿Token。

Apriel-H1-15b-Thinker-SFT（绿色）对比全注意力教师模型（蓝色）。在各项基准测试中，推理质量几乎保持平稳，而吞吐量根据上下文长度提升了1.89-2.09倍。

完整细节请参阅我们的Apriel-H1论文。在此，我们重点介绍使其成功的关键洞见。

**非显而易见的洞见**
以下是我们最初认为可行的方法：仅在预训练数据上进行蒸馏，并用一些SFT数据补充。

推理似乎很可靠。我们正在插入从未见过数据的全新Mamba层。这些线性SSM需要从头学习通用的Token混合能力。除非它们接触到原始注意力层所见的相同广泛分布，否则如何能成为有效的混合器？

于是我们尝试了。然后我们尝试混合预训练和SFT数据。这没有奏效。蒸馏出的混合模型失去了推理质量，有时甚至非常严重。

真正有效的方法是：使用教师模型SFT数据集中的高质量推理轨迹。

蒸馏一个推理模型，并非转移通用的下一个Token预测能力。基础模型已经具备此能力，并且我们从一个强大的15B基础模型开始。我们要保留的是特定且脆弱的：教师模型的多步推理模式。

这些模式源于复杂的注意力机制。检索头从数千个Token之前提取上下文。归纳头识别并延续逻辑链。长程依赖将前提与多步后的结论连接起来。当你用Mamba的线性循环完全替代注意力时，这些计算机制就被打乱了。混合模型必须发现实现相同推理结果的新路径。

这种发现需要推理结构清晰可见且正确的明确示例：
- 多步数学证明，其中每个思路都从前一个推导而来
- 具有清晰逻辑依赖关系的编码任务
- 带有详细解释链的科学分析

另一方面，预训练数据过于嘈杂和分散。推理信号会丢失。你需要你所试图保留的特定能力的集中示例。

一旦我们理解了数据选择，我们的蒸馏方法也变得清晰。我们使用了反向KL散度（温度1）而非前向KL。反向KL始终胜出。为什么？我们在教师模型具有高置信度和清晰结构的问题上进行训练。反向KL的寻求模式行为鼓励学生模型致力于那些高置信度的预测。当你的教师模型自信且正确时，你希望你的学生模型也同样自信。

这个洞见是整个方法的关键：使你的蒸馏数据与你所要保留的能力相匹配，而非与你所要构建的能力相匹配。

**如何应用：分阶段蒸馏**
你不能简单地用Mamba替换40个注意力层并指望成功。我们艰难地学到了这一点，并最终开发了一种分阶段蒸馏程序来可靠地实现目标。

**阶段1：识别最不重要的层。** 我们在MMLU上使用了留一法分析：移除每一层，用恒等映射替换，然后测量性能下降。按重要性排序，将最不重要的25层替换为Mamba-in-Llama初始化的混合器。进行端到端蒸馏。这对我们的H-25检查点有效。

**阶段2：超过25层的渐进式转换。** 超过25层后，留一法失效，因为单独不重要的层在组合中变得关键。为了解决这个问题，我们开发了一种动态启发式方法，称之为MIL-Mamba-Replacement。对于每个剩余的注意力层，我们用MIL初始化一个Mamba混合器，运行100个训练步骤，并记录蒸馏损失。收敛到较低损失的层“更容易”被替换。这捕捉的是训练动态，而非静态重要性。

我们逐步推进：25 → 27 → 30 → 34 → 37 → 40个Mamba层，根据MMR分数对替换进行分组。每个检查点都从前一个蒸馏而来。

**阶段3：在SFT数据上进行端到端训练。** 达到目标Mamba层数后，我们进行了最终的SFT传递，直到推理性能稳定。经过559亿蒸馏Token和209亿SFT Token的训练，这产生了我们最终的Apriel-H1-15b-Thinker-SFT模型。

完整的效率边界。每个检查点显示累计训练Token数。我们的旗舰模型H-30-SFT（发布为Apriel-H1-15b-Thinker-SFT）总共使用了768亿Token，实现了2.1倍吞吐量，平均得分0.76。激进转换的H-40变体使用了1365亿Token，实现了3.4倍吞吐量。作为参考：英伟达的Nemotron-Nano-9B-v2实现了4.6倍吞吐量，得分0.77，但需要从头开始训练，且计算量高出数个数量级。

**使其可复现：Fast-LLM**
我们在Fast-LLM上构建了这一切，这是我们的开源训练框架。核心架构原则是：大语言模型Transformer应该是模块化的。注意力和Mamba是同一“混合”接口的不同实现，可以自由交换。

以下是Fast-LLM配置格式中的混合架构示例：
decoder:
type: "pattern"
blocks:
attention_block:
mixer:
type: "attention"
heads: 32
head_groups: 8
head_size: 128
mlp:
type: "gated"
activation: "silu"
mamba_block:
mixer:
type: "mamba"
d_inner: 4096
state_size: 16
dt_rank: 16
mlp:
type: "gated"
activation: "silu"
num_blocks: 50
pattern: ["attention_block", "attention_block", "mamba_block", ...]

`pattern`字段指定了层顺序。对于Apriel-H1-15b-Thinker-SFT：30个`mamba_block`，20个`attention_block`，按重要性放置。仅此而已。

蒸馏也是配置的一部分：
model:
base_model:
head:
distillation_model: teacher
distillation_loss_implementation: reverse_kl
reference_models:
teacher:
pretrained:
format: mistral
path: path/to/Apriel-Nemotron-15b-Thinker

Fast-LLM处理梯度累积、分布式训练、张量并行、检查点保存等大规模实验所需的一切。它是开源的，采用Apache 2.0许可证。你可以复现这项工作，因为我们设计的基础设施就是为了使其可复现。

**常见问题**
**为什么发布所有检查点？** 因为最优选择取决于你的约束条件。H-30提供了最佳平衡。H-40为延迟敏感型工作负载最大化吞吐量。中间检查点让你可以精确选择权衡点。

**为什么在不同上下文长度下获得不同的加速比？** Mamba的线性复杂度优势随序列长度增长而增加，而注意力的复杂度呈二次方增长。

**为什么只尝试Mamba？** 我们使用Mamba-1有三个原因：它拥有经过验证的蒸馏记录，表现出强大的实证性能，并且易于在我们的框架中实现。

这让我们得以首先聚焦数据问题。

**Mamba的超参数是什么？** 状态大小16，DT秩16，内部维度4096。在Apriel中，为了适配我们的GQA设置，我们扩展了B（输入投影）和x（状态）以匹配M1之后的总注意力头数。

**为什么没有尝试更先进的转换方法？** 我们使用了Mamba-in-Llama初始化和知识蒸馏，而非MOHAWK的多阶段流程，因为后者在初步实验中并未显示出显著优势。

**为什么只对H-30模型进行了SFT？** 我们仅对H-30应用了SFT，是为了验证经过蒸馏的混合模型能否通过标准的后训练得到提升。其他检查点是纯蒸馏的结果，但同样可以进行类似的微调。

**为什么没有探索RL？** 这是一个范围界定上的决定，旨在将蒸馏问题独立出来：能否仅通过知识蒸馏来传递推理能力？答案是：可以。但RL应能进一步缩小剩余的质量差距。我们正在为未来的迭代探索RL。

**你们真的证明了Apriel-H1在相似的计算预算下能媲美全注意力推理吗？** 我们并未在采用全注意力的Apriel与从预训练开始就采用相同方式训练的混合模型之间进行完全对等的比较。那将需要以Apriel-H1架构重复教师模型的所有中期训练和后训练，这超出了我们的计算预算。然而，我们可以断言的是，通过蒸馏来改造效率是切实可行且有效的，并且所得的混合模型可以通过微调来匹配甚至超越教师模型的推理质量。

**生产现实**
我们已在Hugging Face Transformers和vLLM中实现了Apriel-H1。Transformers的集成是直接的。我们提供了一个新的模型类，其中包含可互换的注意力和Mamba层。vLLM的集成利用了其近期的Mamba缓存操作，以实现连续批处理、前缀缓存和分块预填充。vLLM插件已准备就绪。我们目前正在等待最终的法律批准以将其开源。

**诚实的评估：** 如今部署混合模型意味着要面对不完善的工具。相关工具链正在快速成熟，但尚未达到开箱即用的程度。你将需要编写自定义代码，仔细验证数值行为，并设法绕过框架的限制。对于能够承担此成本的团队，吞吐量的提升是值得的。对于那些不能的团队，等待或许是更明智的选择。

**要点**
大多数团队没有无限的计算资源来进行20T-token的预训练。如果你已经投资了一个强大的基础模型并需要提升效率，这项工作展示了一条可行的路径：使用与你希望保留的能力相匹配的高质量、特定任务的数据，将其蒸馏到混合模型中。

那个令人惊讶的发现——使用推理数据来蒸馏推理能力——事后看来似乎显而易见，但却与最初的直觉相悖。我们验证了它，解释了其工作原理，并构建了使其可复现的基础设施。

**试试看**
模型：HuggingFace上的Apriel-H1系列
训练框架：GitHub上的Fast-LLM
教师模型：Apriel-Nemotron-15B-Thinker
论文：Apriel-H1: Towards Efficient Enterprise Reasoning Models
发现错误？请提交问题。发现了更好的层放置启发式方法？请告诉我们。基于Apriel-H1构建了有趣的东西？我们很乐意看到。

**引用：**
@article{apriel-h1-2025,
title={Apriel-H1: Towards Efficient Enterprise Reasoning Models},
author={SLAM Lab, ServiceNow},
journal={arXiv preprint arXiv:2511.02651},
year={2025}
}

**核心贡献者：** Oleksiy Ostapenko, Luke Kumar, Raymond Li, Denis Kocetkov, Joel Lamy-Poirier, Torsten Scholak
**贡献者：** Shruthan Radhakrishna, Soham Parikh, Shambhavi Mishra
**技术联合负责人：** Torsten Scholak, Sathwik Tejaswi Madhusudhan.

---

> 本文由AI自动翻译，原文链接：[Apriel-H1: The Surprising Key to Distilling Efficient Reasoning Models](https://huggingface.co/blog/ServiceNow-AI/apriel-h1)
> 
> 翻译时间：2026-01-06 01:06
