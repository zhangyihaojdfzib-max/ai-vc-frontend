---
title: FACTS Grounding：评估大语言模型事实性的新基准
title_original: 'FACTS Grounding: A new benchmark for evaluating the factuality of
  large language models'
date: '2024-12-17'
source: Google DeepMind
source_url: https://deepmind.google/blog/facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models/
author: ''
summary: 本文介绍了FACTS Grounding基准测试，这是一个用于评估大语言模型事实准确性和避免幻觉能力的新工具。该基准包含1719个精心设计的示例，要求模型基于提供的上下文文档生成长篇回答。评估采用Gemini
  1.5 Pro、GPT-4o和Claude 3.5 Sonnet三个前沿LLM作为自动评判员，通过资格审核和事实准确性两阶段评分。团队同时发布了公开数据集和Kaggle排行榜，旨在推动行业在事实性方面的进步，并计划持续更新以应对技术发展。
categories:
- AI研究
tags:
- 大语言模型
- 事实性评估
- 基准测试
- 幻觉检测
- AI评测
draft: false
translated_at: '2026-02-08T04:36:06.043963'
---

# FACTS Grounding：评估大语言模型事实性的新基准

FACTS 团队

![](/images/posts/4f7ee17e592f.jpg)

我们全面的基准测试和在线排行榜提供了一个急需的衡量标准，用于评估 LLM（大语言模型）在多大程度上能将其回答基于所提供的源材料并避免幻觉。

大语言模型正在改变我们获取信息的方式，但它们对事实准确性的把握仍不完美。它们可能会"幻觉"出虚假信息，尤其是在处理复杂输入时。这反过来会削弱人们对 LLM 的信任，并限制其在现实世界中的应用。

今天，我们推出 **FACTS Grounding**，这是一个全面的基准测试，用于评估 LLM 生成回答的能力，这些回答不仅要相对于给定输入在事实上准确，而且要足够详细，以提供令用户满意的答案。

我们希望我们的基准测试能推动全行业在事实性和基于事实方面取得进展。为了追踪进展，我们还在 Kaggle 上推出了 **FACTS 排行榜**。我们已经使用 FACTS Grounding 测试了领先的 LLM，并用它们的基于事实得分填充了初始排行榜。随着该领域的发展，我们将维护和更新排行榜。

![当前排行榜排名](/images/posts/eb8cc0f13bd4.jpg)

当前排行榜排名

## FACTS Grounding 数据集

为了准确评估任何给定 LLM 的事实性和基于事实的能力，FACTS Grounding 数据集包含 1,719 个示例，每个示例都经过精心设计，要求生成基于所提供上下文文档的长篇回答。每个示例包含一个文档、一个要求 LLM 仅参考所提供文档的系统指令，以及一个伴随的用户请求。

![FACTS Grounding 数据集中的一个示例](/images/posts/1c95d61ff833.jpg)

FACTS Grounding 数据集中的一个示例

所有示例被分为一个"公开"集（860 个）和一个"私有"保留集（859 个）。我们今天**发布公开集**，以便任何人都可以使用它来评估 LLM。当然，我们知道防止基准污染和排行榜作弊问题很重要，因此遵循行业标准做法，我们将保留私有评估集。FACTS 排行榜的得分是公开集和私有集上的平均性能。

为确保输入多样性，FACTS Grounding 示例包含各种长度的文档，最多可达 32,000 个 Token（约 20,000 词），涵盖金融、技术、零售、医学和法律等领域。用户请求同样范围广泛，包括摘要、问答生成和改写任务。我们没有包含任何可能需要创造力、数学或复杂推理能力的示例——这些能力可能要求模型在基于事实之外应用更高级的推理。

![提示词分布](/images/posts/c83c8f98ff63.jpg)

提示词分布

## 由领先 LLM 进行的集体评判

要在给定示例上取得成功，LLM 必须综合文档中的复杂信息，并生成长篇回答，该回答既要全面回应用户请求，又要完全归因于该文档。

FACTS Grounding 使用三个前沿的 LLM 评判员自动评估模型回答——即 Gemini 1.5 Pro、GPT-4o 和 Claude 3.5 Sonnet。我们选择组合不同的评判员，以减轻评判员可能给其自身模型家族成员生成的回答打更高分的任何潜在偏见。我们针对一个保留的测试集对自动评判模型进行了全面评估，以找到性能最佳的评判提示词模板，并验证其与人类评分者的一致性。

每个 FACTS Grounding 示例的评判分为两个阶段。首先，评估回答的资格，如果回答未能充分回应用户请求，则被取消资格。其次，如果回答完全基于所提供文档中包含的信息，没有幻觉，则被评判为事实准确。

在多个 AI 评判模型分别评估了给定 LLM 回答的资格和基于事实的准确性后，将结果汇总以确定 LLM 是否成功处理了该示例。整个基于事实任务的总分是所有评判模型在所有示例上得分的平均值。更多关于我们 FACTS Grounding 评估方法的细节，请参阅**我们的论文**。

![一个事实正确但未能恰当回应用户请求的回答，未能通过基准测试示例。这里我们看到三个模型回答的实例，自动 LLM 评判员认为它们不符合资格](/images/posts/f9e029eb2c4b.jpg)

一个事实正确但未能恰当回应用户请求的回答，未能通过基准测试示例。这里我们看到三个模型回答的实例，自动 LLM 评判员认为它们不符合资格

## FACTS Grounding 将持续演进

我们注意到基准测试可能很快被进展所超越，因此我们推出 FACTS Grounding 基准测试和排行榜只是一个开始。事实性和基于事实是塑造 LLM 及更广泛 AI 系统未来成功和有用性的关键因素之一，我们的目标是随着该领域的发展而增长和迭代 FACTS Grounding，不断提高标准。

我们鼓励 AI 社区**参与 FACTS Grounding**，在公开的示例集上评估他们的模型，或提交他们的模型进行评估。我们相信，全面的基准测试方法，加上持续的研究和开发，将继续改进 AI 系统。

致谢

FACTS 是 Google DeepMind 和 Google Research 的合作项目。FACTS Grounding 由以下人员领导：Alon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Dipanjan Das, Jon Lipovetz, Kate Olszewska, Lukas Haas, Michelle Liu, 和 Nate Keating。

我们也非常感谢以下人员的贡献：Adam Bloniarz, Carl Saroufim, Corey Fry, Dror Marcus, Doron Kukliansky, Gaurav Singh Tomar, James Swirhun, Jinwei Xing, Lily Wang, Madhu Gurumurthy, Michael Aaron, Moran Ambar, Rachana Fellinger, Rui Wang, Zizhao Zhang, 和 Sasha Goldshtein。

我们还要感谢 Avinatan Hassidim, D. Sculley, Fernando Pereira, Koray Kavukcuoglu, Slav Petrov, Ya Xu, 和 Yossi Matias 的持续支持。

---

> 本文由AI自动翻译，原文链接：[FACTS Grounding: A new benchmark for evaluating the factuality of large language models](https://deepmind.google/blog/facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models/)
> 
> 翻译时间：2026-02-08 04:36
