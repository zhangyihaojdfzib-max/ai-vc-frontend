---
title: Exphormer：利用扩展图实现图Transformer的高效扩展
title_original: 'Exphormer: Scaling transformers for graph-structured data'
date: '2024-01-23'
source: Google AI Blog
source_url: http://blog.research.google/2024/01/exphormer-scaling-transformers-for.html
author: ''
summary: 本文介绍了Google Research提出的Exphormer框架，旨在解决图Transformer模型在处理图结构数据时因完全注意力机制导致的二次方计算瓶颈。Exphormer通过构建稀疏的交互图，结合输入图的局部边、常数度扩展图的扩展边以及连接虚拟节点的全局边，在保持模型表达能力的同时显著提升了计算效率。该方法利用了扩展图的谱近似和快速混合特性，使远距离节点能在多层注意力中有效通信，为大规模图数据上的Transformer应用提供了可行的解决方案。
categories:
- AI研究
tags:
- 图神经网络
- Transformer
- 可扩展性
- 稀疏注意力
- 扩展图
draft: false
translated_at: '2026-01-06T18:15:23.219Z'
---

![](/images/posts/065f2879f517.gif)

1. 首页
2. 博客

# Exphormer：为图结构数据扩展Transformer

作者：Google Research 研究科学家 Ameya Velingker，Google 软件工程师 Balaji Venkatachalam

- 分享复制链接×

- 复制链接×

![](/images/posts/ae27af00e5ca.gif)

图，其中对象及其关系被表示为节点（或顶点）和节点对之间的边（或链接），在计算和机器学习（ML）中无处不在。例如，社交网络、道路网络以及分子结构和相互作用，这些领域的底层数据集都具有天然的图结构。ML 可用于学习节点、边或整个图的属性。

在图数据上进行学习的一种常见方法是**图神经网络**（GNNs），它通过对节点、边和全局属性应用可优化的变换来操作图数据。最典型的 GNN 类别通过**消息传递**框架运行，其中每一层聚合一个节点与其直接邻居的表示。

最近，**图Transformer模型**已成为消息传递 GNN 的一种流行替代方案。这些模型建立在 Transformer 架构在自然语言处理（NLP）领域成功的基础上，并将其适应于图结构数据。图Transformer中的注意力机制可以通过一个交互图来建模，其中边代表相互关注（attend）的节点对。与消息传递架构不同，图Transformer的交互图与输入图是分开的。典型的交互图是一个完全图，这代表一个**完全注意力机制**，它建模所有节点对之间的直接交互。然而，这造成了二次方的计算和内存瓶颈，限制了图Transformer仅适用于最多包含几千个节点的小型图数据集。使图Transformer可扩展一直被认为是该领域最重要的研究方向之一（参见此处的第一个开放性问题）。

一个自然的补救措施是使用边数更少的**稀疏**交互图。已经提出了**许多稀疏且高效的Transformer**来消除序列的二次方瓶颈，然而，它们通常无法以原则性的方式扩展到图数据。

在 ICML 2023 上发表的论文《Exphormer: Sparse Transformers for Graphs》中，我们通过引入一个专门为图数据设计的Transformer稀疏注意力框架，来解决可扩展性挑战。Exphormer 框架利用了**扩展图**这一来自**谱图论**的强大工具，并能够在各种数据集上取得强大的实证结果。我们的 Exphormer 实现现已可在 GitHub 上获取。

Exphormer 的核心关键思想是使用**扩展图**，这是一种稀疏但连接良好的图，具有一些有用的特性——1）图的矩阵表示具有与完全图相似的线性代数特性；2）它们表现出随机游走的快速混合性，即从任何起始节点出发的随机游走只需少量步数就足以确保收敛到图节点上的“稳定”分布。扩展图已在算法、伪随机性、复杂性理论和纠错码等多个领域得到应用。

一类常见的扩展图是 **d-正则扩展图**，其中每个节点都有 d 条边（即每个节点的度为 d）。扩展图的质量由其**谱隙**来衡量，这是其**邻接矩阵**（一种图的矩阵表示，行和列由节点索引，条目表示节点对是否通过边连接）的代数性质。那些最大化谱隙的图被称为**拉马努金图**——它们实现了 d - 2*√(d-1) 的谱隙，这基本上是 d-正则图中可能的最佳值。多年来，针对不同的 d 值，已经提出了许多拉马努金图的确定性和随机性构造。我们使用了 Friedman 的**随机扩展图构造**，它产生接近拉马努金的图。

![](/images/posts/575ef8a96513.gif)

Exphormer 用一个稀疏的 d-正则扩展图的边，取代了标准Transformer中密集、完全连接的交互图。直观地说，扩展图的谱近似和混合特性允许远距离节点在图Transformer架构中堆叠多个注意力层后相互通信，即使这些节点可能不直接相互关注。此外，通过确保 d 是常数（与节点数量无关），我们在得到的交互图中获得了线性数量的边。

## Exphormer：构建稀疏交互图

Exphormer 将扩展图的边与输入图和虚拟节点相结合。更具体地说，Exphormer 的稀疏注意力机制构建了一个包含三种类型边的交互图：

- 来自输入图的边（局部注意力）
- 来自常数度扩展图的边（扩展注意力）
- 从每个节点到一小部分虚拟节点的边（全局注意力）

![](/images/posts/0f07e6562492.gif)

每个组件都有特定的目的：来自输入图的边保留了输入图结构的归纳偏差（这通常在完全连接的注意力模块中丢失）。同时，扩展边提供了良好的全局连接性和随机游走混合特性（在谱上以少得多的边近似完全图）。最后，虚拟节点充当全局“记忆池”，可以直接与每个节点通信。虽然这导致了来自每个虚拟节点的额外边数等于输入图中的节点数，但得到的图仍然是稀疏的。扩展图的度和虚拟节点的数量是可调整的超参数，用于改进质量指标。

此外，由于我们使用一个常数度的扩展图和少量常数个虚拟节点进行全局注意力，得到的稀疏注意力机制相对于原始输入图的大小是线性的，即，它建模的直接交互数量与节点和边的总数同阶。

我们还证明了 Exphormer 与密集Transformer具有相同的表达能力，并遵循通用近似性质。特别是，当 Exphormer 的稀疏注意力图增加了自循环（连接节点自身的边）时，它可以通用地近似连续函数 [1,2]。

### 与序列稀疏Transformer的关系

将 Exphormer 与序列的稀疏注意力方法进行比较是很有趣的。也许在概念上与我们方法最相似的架构是 **BigBird**，它通过组合不同的组件来构建交互图。BigBird 也使用虚拟节点，但与 Exphormer 不同的是，它使用窗口注意力和来自 **Erdős-Rényi** 随机图模型的随机注意力作为其余组件。

BigBird 中的窗口注意力关注序列中一个Token周围的Token——Exphormer 中的局部邻域注意力可以看作是窗口注意力在图上的推广。

在 n 个节点上的 Erdős-Rényi 图 G(n, p)，以概率 p 独立地连接每一对节点，在 p 足够高时也能起到扩展图的作用。然而，需要超线性数量的边（Ω(n log n)）才能确保 Erdős-Rényi 图是连通的，更不用说是一个好的扩展图了。另一方面，Exphormer 中使用的扩展图只有**线性**数量的边。

先前的研究已经展示了基于完整图Transformer的模型在节点数高达5,000的图数据集上的应用。为了评估Exphormer的性能，我们基于著名的GraphGPS框架[3]进行构建，该框架结合了消息传递和图Transformer，并在多个数据集上实现了最先进的性能。我们证明，在GraphGPS框架中，用Exphormer替换图注意力组件中的密集注意力机制，可以获得性能相当甚至更优的模型，且通常可训练参数更少。

此外，Exphormer显著地使图Transformer架构能够很好地扩展到超出上述通常图规模限制的范围。Exphormer可以扩展到包含10,000多个节点的图数据集，例如Coauthor数据集，甚至扩展到更大的图，例如著名的ogbn-arxiv数据集（一个包含17万个节点和110万条边的引文网络）。

![](/images/posts/9728dada8239.png)

最后，我们观察到，通过扩展图创建小直径覆盖图的Exphormer，展现出有效学习长程依赖关系的能力。Long Range Graph Benchmark是一套包含五个图学习数据集的基准测试，旨在衡量模型捕捉长程交互的能力。结果显示，基于Exphormer的模型优于标准的GraphGPS模型（后者在发布时曾是其中四个数据集上的最先进模型）。

图Transformer已成为机器学习的一个重要架构，它将NLP中非常成功的基于序列的Transformer适配到图结构数据上。然而，可扩展性已被证明是在大型图数据集上使用图Transformer的一个主要挑战。在本文中，我们介绍了Exphormer，这是一个利用扩展图来提高图Transformer可扩展性的稀疏注意力框架。Exphormer被证明具有重要的理论特性，并展现出强大的实证性能，尤其是在学习长程依赖关系至关重要的数据集上。欲了解更多信息，我们建议读者观看ICML 2023上的简短介绍视频。

我们感谢我们的研究合作者：来自不列颠哥伦比亚大学的Hamed Shirzad和Danica J. Sutherland，以及来自谷歌研究院的Ali Kemal Sinop。特别感谢Tom Small为本文制作了动画。

- 算法与理论
- 机器智能

- 分享复制链接×

- 复制链接×
