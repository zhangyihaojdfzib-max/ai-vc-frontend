---
title: Smol2Operator：训练轻量级视觉语言模型成为GUI智能体
title_original: 'Smol2Operator: Post-Training GUI Agents for Computer Use'
date: '2025-09-23'
source: Hugging Face Blog
source_url: https://huggingface.co/blog/smol2operator
author: ''
summary: 本文介绍了一种通过多阶段训练策略，将轻量级视觉语言模型（如SmolVLM2-2.2B-Instruct）转化为具备图形用户界面（GUI）交互能力的智能体的完整方法。核心工作包括：将多个异构GUI数据集的复杂动作格式统一为标准动作空间，并通过“从零到感知”和“从感知到认知”两个训练阶段，逐步赋予模型界面理解、元素定位及任务推理能力。研究开源了全部训练方案、数据处理工具、模型及数据集，旨在推动GUI自动化领域的可复现研究。
categories:
- AI研究
tags:
- GUI自动化
- 视觉语言模型
- 智能体
- 多阶段训练
- 动作空间统一
draft: false
translated_at: '2026-02-05T04:15:14.103691'
---

太长不看：本工作展示了轻量级视觉-语言模型如何习得基于图形用户界面（GUI）的技能，并演化为具备智能体能力的GUI编码器。我们发布了所有训练方案、数据处理工具、最终模型、演示及数据集，以确保完全可复现性并推动进一步研究 🫡。完整资源请在此处查看。

- 引言
- 1. 数据转换与统一动作空间
    - 动作空间不一致的挑战
    - 我们的统一方法
    - 数据转换示例
    - 使用动作空间转换器进行自定义动作空间适配
    - 关键特性
    - 使用示例
    - 已转换并发布的数据集
- 2. 第一阶段：从零到感知
    - 训练数据
    - 优化实验
    - 图像分辨率与坐标系分析
    - 关键发现
    - 第一阶段结果
- 3. 第二阶段：从感知到认知
    - 训练数据
    - 第二阶段结果
- 4. 开源即所需
- 5. 结论
- 下一步计划？

## 引言

图形用户界面（GUI）自动化是计算机视觉领域最具挑战性的前沿之一。开发能够观察并与用户界面交互的模型，将使AI智能体能够在移动端、桌面端和网页平台中导航。这将重塑数字交互的未来。

在这篇博客文章中，我们提出了一种通过多阶段训练策略来训练用于GUI自动化的视觉-语言模型的综合方法。我们展示了如何将一个不具备任何基础能力的模型，转变为能够理解并与图形界面交互的智能编码器。

我们的目标并非打造一个SOTA模型，而是旨在展示从数据处理到模型训练的完整流程，并在此过程中揭示如何在视觉-语言模型中解锁基于GUI的基础能力。

![GUI能力结合了对界面的理解和精确的元素定位。这些能力使模型能够将高级任务转化为低级的GUI操作，如点击、输入……](/images/posts/bb3129716c02.png)

GUI能力结合了对界面的理解和精确的元素定位。这些能力使模型能够将高级任务转化为低级的GUI操作，如点击、输入……

我们的方法以**SmolVLM2-2.2B-Instruct**作为基线模型，这是一个小巧而强大的视觉-语言模型，最初不具备处理GUI任务的基础能力。这使其成为展示我们训练方法有效性的理想候选者。通过我们的两阶段训练过程，我们首先在模型中灌输基础能力，然后使用监督微调（SFT）增强其智能推理能力。

我们在一个已建立的感知基准测试**ScreenSpot-v2**上评估了我们的方法，该测试评估模型理解和定位屏幕截图中元素的能力。我们的流程灵感来源于**AGUVIS**论文，并利用其精心策划的数据集，在其基础工作上进一步构建。

![基础模型 SmolVLM2-2.2B-Instruct 训练阶段 ScreenSpot-v2 性能的演变。](/images/posts/9bd3cef90cac.png)

基础模型 **SmolVLM2-2.2B-Instruct** 训练阶段 ScreenSpot-v2 性能的演变。

## 1. 数据转换与统一动作空间

本节解释了我们如何**将来自多个数据集的异构GUI动作格式转换为单一的统一格式**。通过标准化函数名称、签名和参数，我们创建了一致、高质量的数据，为有效的模型训练奠定了基础。

### 动作空间不一致的挑战

处理多个GUI自动化数据集时，一个主要挑战是动作表示缺乏标准化。不同的数据集使用不同的函数签名、参数命名约定和动作分类法，这使得在不同数据源上训练统一模型变得困难。

### 我们的统一方法

我们获取了最初由**AGUVIS**使用的开源数据集（xlangai/aguvis-stage1, xlangai/aguvis-stage2），并实施了一个全面的数据转换流程来创建统一的动作空间。我们的方法包括：

1.  **函数解析与归一化**：我们开发了一个函数解析器（见 `utils/function_parser.py`），可以从所有数据集的各种格式中提取和解析函数调用。该解析器支持任何函数签名格式，处理复杂的参数结构，并能以正确的参数顺序重建函数调用。
2.  **动作空间统一**：我们实现了一个全面的动作转换系统（见 `preprocessing/action_conversion.py`），将所有原始动作表示转换为标准化的函数命名和参数结构。这个过程凸显了不同数据集间函数签名存在的显著不一致性，并使我们能够：
    - 移除不需要或冗余的动作
    - 标准化参数命名约定
    - 创建连贯的动作词汇表
3.  **（额外）灵活的适配框架**：我们的转换流程包含实用工具，允许用户：
    - 使用 `utils/action_space_converter.py` 工具将整个数据集适配到他们自己的动作空间命名约定
    - 提取并分析当前的动作空间结构

### 示例数据转换

以下是我们动作转换系统（`preprocessing/action_conversion.py`）中的真实示例，展示了我们如何将异构的动作表示转换为我们的统一格式（基础坐标归一化到 [0,1]）：

转换前（原始动作数据集格式）：

```python
mobile.home()
mobile.open_app(app_name='drupe')
mobile.swipe(from_coord=[0.581, 0.898], to_coord=[0.601, 0.518])
mobile.long_press(x=0.799, y=0.911)
mobile.terminate(status='success')

pyautogui.click(x=0.8102, y=0.9463)
pyautogui.doubleClick(x=0.8102, y=0.9463)
pyautogui.hotkey(keys=['ctrl', 'c'])
pyautogui.scroll(page=-0.1)
pyautogui.write(message='bread buns')
pyautogui.dragTo(from_coord=[0.87, 0.423], to_coord=[0.8102, 0.9463])
```

转换后（统一动作数据集格式）：

```python
navigate_home()
open_app(app_name='drupe')
swipe(from_coord=[0.581, 0.898], to_coord=[0.601, 0.518])
long_press(x=0.799, y=0.911)
final_answer('success')

click(x=0.8102, y=0.9463)
double_click(x=0.8102, y=0.9463)
press(keys=['ctrl', 'c'])
scroll(direction='up', amount=10)  
type(text='bread buns')
drag(from_coord=[0.87, 0.423], to_coord=[0.8102, y=0.9463])
```

这个统一过程对于创建连贯的训练数据至关重要，使模型能够在不同的GUI环境中学习一致的动作模式。

### （额外）使用动作空间转换器进行自定义动作空间适配

为了最大限度地适应不同的用例，我们开发了**动作空间转换器**（`utils/action_space_converter.py`），这个工具允许用户轻松地将一个动作空间适配到他们自定义的动作词汇表和命名约定。

您可以使用此工具将一个动作签名（函数名称、参数名称、参数值更改等）转换为另一个：

转换前

```python
assistant_message: "Action: click(x=0.5, y=0.3)"
```

转换后

```python
assistant_message: "Action: touch(x_coord=200, y_coord=300)"
```

### 关键特性

动作空间转换器提供：

1. 可配置映射：在统一操作与您偏好的操作名称之间定义自定义映射
2. 参数转换：重命名参数、应用值转换并设置默认值
3. 灵活架构：支持简单的参数映射和复杂的自定义转换函数
4. 验证：内置验证功能，确保映射配置有效

### 使用示例

```python
from utils.action_space_converter import ActionSpaceConverter, ActionMapping, ParameterMapping
from utils.function_parser import parse_function_call


mappings = [
    ActionMapping(
        source_function="click",
        target_function="touch",
        parameter_mappings=[
            ParameterMapping(source_name="x", target_name="x_coord"),
            ParameterMapping(source_name="y", target_name="y_coord")
        ],
        description="Touch screen at coordinates"    ),
    ActionMapping(
        source_function="type", 
        target_function="write", 
        parameter_mappings=[
            ParameterMapping(source_name="text", target_name="content")
            
            
        ],
        description="Input text"    
    )
]

assistant_message = "I'll interact at those coordinates for you. click(x=0.5, y=0.3) Now I'll input the text. type(text='hello world')"


parsed_function_calls = parse_function_call(text)


converter = ActionSpaceConverter(mappings)


converted_actions = converter.convert_actions(parsed_function_calls)
for new_function_call, old_function_call in zip(converted_actions, parsed_function_calls):
    text = text.replace(old_function_call.to_string(), new_function_call.to_string())

print(text)


```

该工具使研究人员和从业者能够：

- 自定义训练数据：调整数据集以匹配其特定的操作词汇要求
- 领域适应：为不同平台（移动端 vs. 桌面端 vs. 网页端）转换操作
- 框架集成：轻松将训练数据与现有自动化框架对齐
- 快速实验：快速测试不同的操作空间配置
- 发布准备：使用一致的命名约定标准化操作空间，以便生产部署

操作空间转换器对于准备训练数据集尤其有价值，因为它确保了不同部署环境中操作词汇的一致性，同时保持了与现有自动化框架的兼容性。

### 转换并发布的数据集

通过此流程，我们将开源数据集 `xlangai/aguvis-stage1`、`xlangai/aguvis-stage2` 转换到我们的统一操作空间（参见此处）。此过程的输出作为两个新的完全格式化数据集发布：`smolagents/aguvis-stage-1` 和 `smolagents/aguvis-stage-2`。

## 2. 第一阶段：从零到感知

### 训练数据

第一阶段利用了 `smolagents/aguvis-stage-1` 数据集，该数据集通过将低级指令与多样化的可执行操作（以代码形式表示）配对，引入了 **GUI 基础**。例如，`smolagents/aguvis-stage-1` 中的用户/助手对话遵循以下结构：

```json
{
  "user": "click on more button",
  "assistant": "click(x=0.8875, y=0.2281)",
}

```

每个样本都将屏幕截图与多轮用户/助手交互联系起来，使模型能够学习跨对话轮次的细粒度操作基础。在微调期间，数据整理器在计算损失时会屏蔽除助手答案之外的所有内容。

### 优化实验

在进行全面第一阶段训练之前，我们进行了全面的消融研究，以确定最佳的训练配置。

### 图像分辨率与坐标系分析

我们测试了不同的图像大小和坐标表示系统，以确定 SmolVLM2 的最佳配置：

- 测试的图像尺寸：384px、768px、1152px
- 坐标系：像素坐标 vs. 归一化坐标（0-1 范围）
- 训练数据：来自 Aguvis 数据集的 40 万个样本

一些 SOTA GUI VLM（例如 Qwen-VL）似乎也使用了不同的归一化范围（0–1000），但本实验未测试此范围。

表 1：HuggingFaceTB/SmolVLM2-2.2B-Instruct 的基线（40 万个样本，aguvis-stage-1）。数值越高越好。

正如我们的基准测试结果所示，SmolVLM2-2.2B-Instruct 基础模型最初在 ScreenSpot-v2 等感知基准测试中取得了 0% 的性能。这种完全缺乏基础能力的情况为我们评估训练方法的有效性提供了一个干净的起点。

### 关键发现

根据我们的实验，我们确定：

- 图像尺寸：1152px
- 坐标系：归一化坐标（0-1 范围）对 SmolVLM2 最有效
- 注意：像素坐标和归一化坐标之间的最佳选择可能因基础模型的预训练方法而异

### 第一阶段结果

使用最佳配置（1152px 分辨率，归一化坐标），我们在 `smolagents/aguvis-stage-1` 数据集上训练了 2 个轮次。结果非常显著，**在 ScreenSpot-v2 上比基线提高了 +41%**。

这一巨大改进表明，我们的第一阶段训练成功地为模型注入了基本的基础能力，使其能够理解并定位屏幕截图中的视觉元素。

表 2：HuggingFaceTB/SmolVLM2-2.2B-Instruct 的基线（2 个轮次，aguvis-stage-1）。

## 3. 第二阶段：从感知到认知

第一阶段提供了基础能力，而第二阶段则针对 **智能体推理能力**，即在行动前进行思考和规划的能力。此阶段将模型从一个识别 GUI 元素的反应式系统转变为一个能够执行复杂、多步骤交互的主动智能体。

第二阶段使用 `smolagents/aguvis-stage-2` 数据集，该数据集引入了智能体场景：

- 对即将执行的操作进行**显式推理**
- 跨多个交互步骤的**上下文一致性**
- 高级指令需要多步骤、低级别的操作。

对即将执行的操作进行**显式推理**

跨多个交互步骤的**上下文一致性**

高级指令需要多步骤、低级别的操作。

例如，`smolagents/aguvis-stage-2` 的聊天消息如下所示：

```json
{
  "system": "You are a helpful GUI agent. ...",
  "user": "Please generate the next move according to the UI screenshot, instruction and previous actions.\n\nInstruction: What information does the site provide about Judith Lauand's career, works and exhibitions?\n\nPrevious actions:\nNone",
  "assistant": "<think>\nClick on the link labeled 'Judith Lauand: Brazilian 1922-2022' to explore more about her career and exhibitions.\n</think>\n<code>\nclick(x=0.41, y=0.178)\n</code>",
}

```

每个样本都将屏幕截图与系统/用户/助手对话联系起来。在微调期间，数据整理器在计算损失时会屏蔽除助手答案之外的所有内容。

### 第二阶段结果

从第一阶段检查点（1152 px 分辨率，归一化坐标）开始，我们在 `smolagents/aguvis-stage-2` 上对模型进行了两个轮次的微调。在 ScreenSpot-v2 上的准确率从 41% 提高到 61%，表明显式推理提高了 GUI 基础性能。

表 2：HuggingFaceTB/SmolVLM2-2.2B-Instruct 在第一阶段微调后的基线（2 个轮次，aguvis-stage-1）。

## 4. 开源即所需

所有训练代码、数据处理流程、数据集和模型都是开源的！

1. **训练方案 (recipe.ipynb)**：包含第一阶段和第二阶段的完整训练流程，涵盖数据集混合配置与训练编排。我们利用 TRL 库来训练模型。
2. **数据集 (smolagents/aguvis-stage-1, smolagents/aguvis-stage-2)**：所有使用的数据集均为开源。
3. **模型 (smolagents/SmolVLM2-2.2B-Instruct-Agentic-GUI)**：通过应用上述训练方案得到的模型。
4. **预处理工具**：
   - **函数解析器 (utils/function_parser.py)**：用于从多种数据集格式中解析、规范化和重构函数调用的工具。支持复杂参数结构、位置参数及多函数调用提取。
   - **动作转换系统 (preprocessing/action_conversion.py)**：核心统一引擎，将移动端和 PyAutoGUI 桌面端动作转换为标准化 API 格式。具备智能坐标处理、滚动动作方向检测以及全面的参数规范化功能。
   - **动作空间转换器 (utils/action_space_converter.py)**：灵活的工具，用于将统一动作空间适配到自定义词汇表和命名规范。通过可配置的参数映射实现特定领域的定制。

## 5. 结论

我们的实验表明，即使对于小型视觉语言模型（VLM），仅通过监督微调（SFT），高质量、面向推理的数据也能显著提升图形用户界面（GUI）的落地能力。除了原始性能的提升，这些结果还表明 GUI 落地能力很大程度上取决于数据质量。精心构建的数据集能够教会模型理解用户界面的结构和语义，为准确的动作预测提供必要的落地基础。

为了支持 GUI 智能体的发展，我们开源了所有内容：完整的流程、数据集以及训练好的模型。您可以复现我们的结果，尝试不同的模型和架构，或将我们的方法适配到新领域。智能体人工智能的未来，正依赖于像您这样的研究者不断突破边界！

## 下一步是什么？

虽然 SFT 在监督任务上表现出色，但强化学习（RL）或直接偏好优化（DPO）等新兴方法有助于发展更强的推理能力，并实现实时适应。这些进展预示着新一代 GUI 智能体的诞生，它们将通过交互进行学习和改进，而不仅仅依赖于静态数据集。

让我们携手共建 GUI 智能体的未来 🤗

---

> 本文由AI自动翻译，原文链接：[Smol2Operator: Post-Training GUI Agents for Computer Use](https://huggingface.co/blog/smol2operator)
> 
> 翻译时间：2026-02-05 04:15
