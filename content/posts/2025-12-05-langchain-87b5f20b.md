---
title: DeepAgents CLI在Terminal Bench 2.0上的评估表现
title_original: Evaluating DeepAgents CLI on Terminal Bench 2.0
date: '2025-12-05'
source: LangChain Blog
source_url: https://blog.langchain.com/evaluating-deepagents-cli-on-terminal-bench-2-0/
author: LangChain Accounts
summary: 本文介绍了DeepAgents CLI编码智能体在Terminal Bench 2.0基准测试中的评估情况。该智能体基于Deep Agents SDK构建，提供交互式终端界面，具备shell执行、文件系统操作和记忆功能。文章详细说明了如何使用Harbor框架在隔离的沙盒环境中进行大规模评估，包括Docker、Modal等多种执行环境支持。DeepAgents
  CLI（由Sonnet 4.5驱动）在89项跨领域任务中获得了约42.5%的分数，与Claude Code表现相当。
categories:
- AI产品
tags:
- 智能体评估
- 终端工具
- 基准测试
- 沙盒环境
- AI编程助手
draft: false
translated_at: '2026-01-06T01:09:58.943Z'
---

作者：Vivek Trivedy 与 Eugene Yurtsev
DeepAgents CLI 是一个构建在 Deep Agents SDK 之上的编码 Agent（智能体），它提供了一个交互式终端界面，具备 shell 执行、文件系统工具和记忆功能。

DeepAgents CLI 在实际任务中表现如何？
在本文中，我们将展示如何在 Terminal Bench 2.0 上评估 DeepAgents CLI。这是一个衡量 Agent（智能体）在软件工程、生物学、安全、游戏等领域的 89 项任务中能力的基准测试。
DeepAgents CLI（由 Sonnet 4.5 驱动）在 Terminal Bench 上获得了约 42.5% 的分数，与 Claude Code 本身的表现相当。

**什么是 DeepAgents CLI**
DeepAgents CLI 是一个由终端驱动的编码 Agent（智能体）。它是开源、用 Python 编写且模型无关的。
DeepAgents CLI 是一个由终端驱动的编码 Agent（智能体），它是开源、用 Python 编写且模型无关的。它内置了多种功能，包括文件操作、shell 命令执行、网络搜索、通过待办事项进行任务规划，以及跨会话的持久化记忆存储。

**快速开始：**
```
export ANTHROPIC_API_KEY="your-api-key"
uvx deepagents-cli
```
该 Agent（智能体）会在修改文件前，通过差异对比的方式提出更改建议供您批准。
观看演示视频以了解其实际运行情况。

**挑战：运行隔离评估**
在我们评估任何东西之前，需要解决一个基本问题：我们如何每次都在一个干净、隔离的环境中运行我们的 Agent（智能体）？
DeepAgents 最近添加了一个沙盒抽象层，使其能够与不同的执行环境协同工作。编码 Agent（智能体）会修改文件、安装包和运行命令——每次测试都可能留下影响后续测试的残留物。我们需要隔离，以便每次测试都从一个干净的状态开始，同时能够并行运行多个测试，并确保 Agent（智能体）无法影响您的本地机器。

**Harbor：沙盒化 Agent（智能体）执行**
这就是 Harbor 的用武之地。Harbor 是一个用于在容器化环境中大规模评估 Agent（智能体）的框架，支持 Docker、Modal、Daytona、E2B 和 Runloop 作为沙盒提供者。它负责处理：
- 在基准测试任务上自动执行测试
- 自动奖励评分以验证任务完成情况
- 预构建评估数据集（如 Terminal Bench）的注册
Harbor 处理了在隔离环境中运行 Agent（智能体）的所有基础设施复杂性，让您可以专注于改进您的 Agent（智能体）。

我们构建了 deepagents-harbor 以使评估变得简单直接：
```
git clone <https://github.com/langchain-ai/deepagents.git>
cd libs/harbor
uv sync
# 使用 API 密钥配置 .env
cp .env.example .env
# 通过 Docker 运行
uv run harbor run --agent-import-path deepagents_harbor:DeepAgentsWrapper \\
--dataset terminal-bench@2.0 -n 1 --jobs-dir jobs/terminal-bench --env docker
# 通过 Daytona 大规模运行（需要 DAYTONA_API_KEY）
--dataset terminal-bench@2.0 -n 10 --jobs-dir jobs/terminal-bench --env daytona
```
我们发现 Daytona 对于大规模运行评估特别有帮助，它允许我们同时运行 40 个试验，并显著加快了迭代周期。

Harbor 提供了一个具备 shell 执行能力的沙盒环境。我们构建了一个 HarborSandbox 后端，它封装了这个环境，并在 shell 命令之上实现了文件系统工具（例如，`edit_file`、`read_file`、`write_file`、`ls`）。

```
class DeepAgentHarbor(BaseAgent):
    async def run(
        self,
        instruction: str,
        environment: BaseEnvironment,
        context: AgentContext,
    ) -> None:
        # 创建一个封装 Harbor 环境并提供文件系统工具的 DeepAgents 后端
        backend = HarborSandbox(environment)
        # 使用 Harbor 后端初始化 DeepAgent CLI
        agent, _ = create_cli_agent(
            model=self._model,
            backend=backend,
            ...
        )
        # 运行 Agent（智能体）
        result = await agent.ainvoke(
            {"messages": [{"role": "user", "content": instruction}]},
        )
```

**Terminal Bench 测试内容**
Terminal Bench 2.0 包含了 89 项任务，涵盖软件工程、生物学、安全、游戏等领域。它通过终端来衡量 Agent（智能体）在计算机环境中的操作能力。

**示例任务：**
*   `path-tracing`：从渲染图像中逆向工程 C 程序
*   `chess-best-move`：使用国际象棋引擎寻找最优走法
*   `git-multibranch`：包含合并冲突的复杂 git 操作
*   `sqlite-with-gcov`：构建带代码覆盖率的 SQLite，分析报告

任务的难度范围很广——有些需要很多操作（例如，`cobol-modernization` 耗时近 10 分钟，涉及 100 多次工具调用），而较简单的任务几秒钟即可完成。

**自动验证：**
每项任务都包含 Harbor 自动运行的验证逻辑，根据 Agent（智能体）的解决方案是否满足任务要求，分配一个奖励分数（0 表示不正确，1 表示正确）。

**基线结果**
我们在 Terminal Bench 2.0 上使用 `claude-sonnet-4-5` 运行了 DeepAgents CLI，进行了 2 次试验，分别获得了 44.9% 和 40.4% 的分数（平均：42.65%）。这个基线水平与使用相同模型的其他实现相当。
虽然不同运行之间存在相当大的采样方差，但这个基线验证了 DeepAgents 提供了一个有竞争力的基础。

**后续步骤**
通过在 Terminal Bench 2 上运行 DeepAgents CLI，我们已经确立了 DeepAgents 作为一个坚实的起点。在接下来的文章中，我们将探讨如何系统地分析 Agent（智能体）运行轨迹，并确定具体的优化方案以提高性能。


> 本文由AI自动翻译，原文链接：[Evaluating DeepAgents CLI on Terminal Bench 2.0](https://blog.langchain.com/evaluating-deepagents-cli-on-terminal-bench-2-0/)
> 
> 翻译时间：2026-01-06 01:09
