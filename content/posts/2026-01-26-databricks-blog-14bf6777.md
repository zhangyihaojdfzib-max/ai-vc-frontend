---
title: 利用NVIDIA MPS扩展小型LLM性能：吞吐量提升超50%
title_original: Scaling Small LLMs with NVIDIA MPS
date: '2026-01-26'
source: Databricks Blog
source_url: https://www.databricks.com/blog/scaling-small-llms-nvidia-mps
author: ''
summary: 本文探讨了NVIDIA多进程服务（MPS）在提升小型语言模型（≤30亿参数）推理吞吐量方面的应用。通过实验发现，在模型规模小、上下文长度短（<2000
  token）且GPU利用率不足的场景下，MPS允许多个推理进程共享GPU资源，实现计算与内存操作的重叠，从而显著提升单GPU吞吐量，部分场景下提升幅度超过50%。文章分析了MPS的适用条件与收益来源，为企业在特定工作负载下优化GPU资源利用提供了实践指导。
categories:
- AI基础设施
tags:
- NVIDIA MPS
- 小型语言模型
- GPU优化
- 推理加速
- 性能扩展
draft: false
translated_at: '2026-01-27T00:58:25.235555'
---

![Scaling Small LLMs with NVIDIA MPS](/images/posts/6cb4ffb4f203.png)

小型模型正迅速变得更加强大，并适用于广泛的企业用例。与此同时，每一代新的GPU都显著提升了计算能力和内存带宽。结果如何呢？即使在高度并发的工作负载下，小型LLM也常常让GPU的计算和内存带宽资源大量闲置。

在代码补全、检索、语法纠正或专用模型等用例中，我们的企业客户在Databricks上部署了许多此类小型语言模型，我们不断将GPU的性能推向极限。NVIDIA的多进程服务（MPS）看起来是一个很有前景的工具：它允许多个推理进程共享单个GPU上下文，使它们的内存和计算操作能够重叠——从而有效地从相同的硬件中榨取更多的工作量。

我们着手严格测试MPS是否能在我们的生产环境中提供更高的单GPU吞吐量。我们发现，在以下场景中，MPS能带来显著的吞吐量提升：

-   参数非常少（≤30亿）且上下文长度短到中等（<2000个Token）的语言模型
-   仅执行预填充（prefill）工作负载的非常小的语言模型（<30亿参数）
-   具有显著CPU开销的推理引擎

根据我们的消融实验，关键解释有两方面：在GPU层面，当单个引擎未能充分利用计算或内存带宽时——尤其是在小模型的注意力主导阶段——MPS能够实现有意义的核函数重叠；同时，作为一个有益的副作用，它还可以通过将总批次在多个引擎间分片来缓解CPU瓶颈（如调度器开销或多模态工作负载中的图像处理开销），从而降低每个引擎的CPU负载。

## 什么是MPS？

NVIDIA的多进程服务（MPS）是一项功能，它通过将多个进程的CUDA内核复用到硬件上，使它们能够更高效地共享单个GPU。正如NVIDIA官方文档所述：

> 多进程服务（MPS）是CUDA应用程序编程接口（API）的一种替代性、二进制兼容的实现。MPS运行时架构旨在透明地支持协作式多进程CUDA应用程序。

简而言之，MPS在驱动程序中提供了一个二进制兼容的CUDA实现，允许多个进程（如推理引擎）更高效地共享GPU。进程无需串行化访问（并在轮换间隙让GPU闲置），当资源可用时，它们的核函数和内存操作会被MPS服务器复用和重叠执行。

## 扩展前景：MPS何时能提供帮助？

在给定的硬件设置上，有效利用率在很大程度上取决于模型大小、架构和上下文长度。由于近期的大语言模型倾向于采用相似的架构，我们使用Qwen2.5模型系列作为代表性示例，来探索模型大小和上下文长度的影响。

以下实验比较了在同一块NVIDIA H100 GPU上（启用MPS）运行的两个相同推理引擎与单实例基准线，使用了完全平衡的同构工作负载。

![Scaling Study for Qwen2.5 Series Model Family (Fixed Batch Size = 1024)](/images/posts/085a591cd9a9.png)

扩展研究的关键观察结果：

-   对于上下文较短的小型模型，MPS能带来超过50%的吞吐量提升。
-   对于相同大小的模型，随着上下文长度增加，收益呈对数线性下降。
-   即使上下文很短，随着模型规模增大，收益也迅速缩小。
-   对于70亿参数的模型或2k上下文长度，收益降至10%以下，最终甚至会导致性能下降。

![Prefill Scaling Study for Qwen2.5 Series Model Family (Fixed Batch Size = 256)](/images/posts/efe4be2532c5.png)

针对预填充密集型工作负载的扩展研究的关键观察结果：

-   小型模型（<30亿参数）：MPS始终能带来超过100%的吞吐量提升。
-   中型模型（约30亿参数）：随着上下文长度增加，收益逐渐减少，最终导致性能衰退。
-   大型模型（>30亿参数）：对于这些模型规模，MPS不提供任何性能优势。

上述扩展结果表明，MPS的收益在GPU利用率低、模型小且上下文短的场景中最为显著，这些条件有利于实现有效的重叠执行。

## 剖析收益：MPS的好处究竟来自哪里？

为了精确找出原因，我们沿着现代Transformer的两个核心构建模块来分解问题：MLP（多层感知器）层和注意力机制。通过隔离每个组件（并消除CPU开销等其他混杂因素），我们可以更精确地归因收益。

#### 所需的GPU资源

Transformer由具有不同扩展行为的注意力层和MLP层组成：

-   MLP：加载权重一次；独立处理每个Token -> 每个Token的内存带宽和计算量恒定。
-   注意力：加载KV缓存并与所有先前Token计算点积 -> 每个Token的内存带宽和计算量呈线性增长。

考虑到这一点，我们进行了有针对性的消融实验。

### 仅MLP模型（移除注意力）

对于小型模型，即使批次中的Token数量增加，MLP层也可能无法使计算饱和。我们通过从模型中移除注意力块来隔离MLP的影响。

![MLP only models MPS gain](/images/posts/006a76790de3.png)

如上图所示，收益有限且迅速消失。随着模型规模或上下文长度增加，单个引擎的计算已经饱和（更大的MLP中每个Token的FLOPs更多，更长的序列带来更多Token）。一旦引擎受限于计算能力，运行两个饱和的引擎几乎没有任何好处——1 + 1 <= 1。

### 仅注意力模型（移除MLP）

在观察到MLP带来的有限收益后，我们采用Qwen2.5-3B模型，并类似地测量了仅注意力设置下的情况。

![Attention vs MLP for decode heavy workload(Qwen2.5-3B)](/images/posts/4dd0310689d6.png)

![Attention vs MLP for Prefill heavy workload(Qwen2.5-3B)](/images/posts/7ab404ed0e58.png)

结果引人注目：

-   对于预填充和解码阶段，仅注意力工作负载显示的MPS收益都显著大于完整模型。
-   对于解码，收益随着上下文长度线性递减，这与我们的预期一致，因为在解码阶段，注意力所需的资源随上下文长度增长。
-   对于预填充，收益下降的速度比解码阶段更快。

MPS的收益是纯粹来自注意力部分的收益，还是存在某种注意力-MLP重叠效应？为了研究这一点，我们计算了“完整模型预期收益”，将其作为“仅注意力”和“仅MLP”收益的加权平均值，权重是它们对总运行时间的贡献。这个“完整模型预期收益”基本上只来自注意力-注意力和MLP-MLP的重叠，而不考虑注意力-MLP的重叠。

对于解码工作负载，“完整模型预期收益”略高于实际收益，这表明注意力-MLP重叠的影响有限。此外，对于预填充工作负载，从序列长度128开始，真实的完整模型收益远低于预期收益。一个可能的假设解释是，由于另一个引擎花费了大量时间在执行饱和的MLP计算上，导致未饱和的注意力内核被重叠执行的机会减少。因此，MPS的大部分收益来自于两个引擎的注意力部分都未饱和的情况。

### 额外收益：回收因CPU开销而损失的GPU时间

上述消融实验主要关注GPU受限的工作负载，但最严重的资源未充分利用情况发生在GPU闲置等待CPU工作时——例如调度器开销、Token化，或多模态模型中的图像预处理。

在单引擎设置中，这些CPU停顿直接浪费了GPU周期。而使用MPS时，每当第一个引擎因CPU而阻塞时，第二个引擎就可以接管GPU，将空闲时间转化为有效的计算。

为了隔离这种效应，我们特意选择了一个早期GPU层面增益已消失的配置：Gemma-4B（在这个模型大小和上下文长度下，注意力机制和MLP层已充分饱和，因此内核重叠的收益微乎其微）。

![MPS Gain for Gemma-4B on vLLM + Async Scheduling Enabled](/images/posts/c0a5b3e07d8f.png)

在8秒延迟下，基线单引擎（蓝线）受限于调度器的CPU开销。这种限制可以通过两种方式解除：一是在vLLM中启用异步调度（绿线，吞吐量提升33%），二是在不启用异步调度的情况下，使用MPS运行两个引擎（黄线，吞吐量提升35%）。这种近乎相同的增益证实了，在CPU受限的场景中，MPS能够回收与异步调度所消除的基本相同的GPU空闲时间。MPS之所以有用，是因为原版vLLM v1.0在调度器层仍存在CPU开销，而像异步调度这样的优化尚未完全可用。

## 一颗子弹，而非万能钥匙

根据我们的实验，MPS能在少数几种运行状态下为小模型推理带来显著增益：

*   具有显著CPU开销的引擎
*   上下文长度短到中等（<2k Token）的极小语言模型（≤30亿参数）
*   预填充密集型工作负载中的极小语言模型（<30亿参数）

在这些优势区间之外（例如，70亿参数以上的模型、长上下文（>8k）、或已经是计算受限的工作负载），GPU层面的收益很难通过MPS轻易获取。

另一方面，MPS也引入了运维复杂性：

*   额外的组件：MPS守护进程、客户端环境设置，以及用于在多个引擎间分配流量的路由器/负载均衡器
*   调试复杂性增加：引擎间缺乏隔离 → 一个引擎的内存泄漏或OOM（内存不足）可能破坏或杀死共享同一GPU的所有其他引擎
*   监控负担加重：我们现在必须监控守护进程健康状态、客户端连接状态、引擎间负载均衡等
*   脆弱的故障模式：由于所有引擎共享单一的CUDA上下文和MPS守护进程，单个行为异常的客户端就可能破坏或耗尽整个GPU，立即影响所有共置的引擎。

简而言之：MPS是一个锋利、专用的工具——在上述狭窄的特定场景下极其有效，但很少能带来普适性的胜利。我们非常享受探索GPU共享的极限，并找出真正的性能悬崖在哪里。在整个推理技术栈中，仍有大量未开发的性能和成本效益潜力。如果你对分布式服务系统感兴趣，或者想让LLM在生产环境中的运行成本降低10倍，我们正在招聘！

作者：Xiaotong Jiang

---

> 本文由AI自动翻译，原文链接：[Scaling Small LLMs with NVIDIA MPS](https://www.databricks.com/blog/scaling-small-llms-nvidia-mps)
> 
> 翻译时间：2026-01-27 00:58
